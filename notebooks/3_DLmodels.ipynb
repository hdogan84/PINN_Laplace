{"cells":[{"cell_type":"code","execution_count":164,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-21T08:21:29.078990Z","iopub.status.busy":"2024-03-21T08:21:29.078141Z","iopub.status.idle":"2024-03-21T08:21:29.100221Z","shell.execute_reply":"2024-03-21T08:21:29.099372Z","shell.execute_reply.started":"2024-03-21T08:21:29.078956Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra \n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.metrics import accuracy_score, classification_report\n","import torch\n","import matplotlib.pyplot as plt\n","import os"]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[],"source":["dataset_name = \"2D_Helm/\"\n","\n","#Configurpave to be dynamically adjusted\n","download_path = \"../data/\" #In the .gitignore list an\n","\n","#n the rest of the code.\n","path_to_datasets = download_path + \"/\" + dataset_name "]},{"cell_type":"code","execution_count":166,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:29.107643Z","iopub.status.busy":"2024-03-21T08:21:29.107375Z","iopub.status.idle":"2024-03-21T08:21:41.062102Z","shell.execute_reply":"2024-03-21T08:21:41.060979Z","shell.execute_reply.started":"2024-03-21T08:21:29.107621Z"},"trusted":true},"outputs":[],"source":["# This cell now makes use of the downloadfolder for the datasets.\n","df_train= pd.read_csv(path_to_datasets + \"/\" + 'helm_train.csv')\n","df_test=pd.read_csv(path_to_datasets + \"/\" +  'helm_test.csv')\n","#print(\"Dataframes MITBIH correctly read into workspace\")\n","\n","#split target and value\n","train_target=df_train['p(x,y)']\n","test_target=df_test['p(x,y)']\n","train=df_train.drop('p(x,y)',axis=1)\n","test=df_test.drop('p(x,y)',axis=1)"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>p(x,y)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>85</th>\n","      <td>0.888889</td>\n","      <td>0.555556</td>\n","      <td>0.939693</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           x         y    p(x,y)\n","85  0.888889  0.555556  0.939693"]},"execution_count":167,"metadata":{},"output_type":"execute_result"}],"source":["df_train.sample(1)"]},{"cell_type":"code","execution_count":168,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.065807Z","iopub.status.busy":"2024-03-21T08:21:41.065436Z","iopub.status.idle":"2024-03-21T08:21:41.072551Z","shell.execute_reply":"2024-03-21T08:21:41.071545Z","shell.execute_reply.started":"2024-03-21T08:21:41.065770Z"},"trusted":true},"outputs":[],"source":["#Switches to decide the dataset sampling method and which models should be run\n","class Config_Sampling:\n","    oversample = False #equals to B_SMOTE\n","    undersample = False\n","    sample_name = \"UNDEFINED_SAMPLE\"\n","    \n","Train_Simple_ANN = True #Trains the simple ANN\n","\n"," "]},{"cell_type":"markdown","metadata":{},"source":["## **Simple Artificial Neural Network**\n","ANN without convolutional layers. Only Dense layers are used. No Pooling, Flattening or Dropping out. Base model for later comparison."]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","metadata":{},"source":["Implement Torch Dataset object"]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[],"source":["class ECG_Dataset(Dataset):\n","    def __init__(self, csv_file, transform=None, target_transform=None):\n","        self.dataframe = csv_file.values\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","        #return self.dataframe.shape[0] # Alternative notation\n","\n","    def __getitem__(self, idx):\n","        inputs = torch.tensor(self.dataframe[idx,:-1], requires_grad=True).to(torch.float32)\n","        label = torch.tensor(self.dataframe[idx,-1]).to(torch.float32)\n","\n","        return inputs, label"]},{"cell_type":"markdown","metadata":{},"source":["Custom function for preprocessing (to elaborate later, currently just returns the input itself)"]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[],"source":["class Lambda(nn.Module):\n","    def __init__(self, func):\n","        super().__init__()\n","        self.func = func\n","\n","    def forward(self, x):\n","        return self.func(x)\n","\n","\n","def preprocess(x):\n","    return x * torch.Tensor([1.0])"]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[],"source":["# Define the ANN model\n","class SimpleANN(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super().__init__()\n","        self.fc0 = nn.Sequential(Lambda(preprocess))\n","        self.fc1 = nn.Linear(input_size, 24)\n","        self.fc2 = nn.Linear(24, 48)  \n","        self.fc3 = nn.Linear(48, 24) \n","        self.fc4 = nn.Linear(24, 12)  \n","        self.fc5 = nn.Linear(12, output_size)  # Hidden to output layer\n","        self.fc6 = nn.Linear(6, output_size)  # Hidden to output layer\n","        self.relu = nn.LeakyReLU(negative_slope=0.001)    # Activation function\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.sigmoid = nn.Sigmoid() \n","\n","    def forward(self, x):\n","        #x = self.fc0(x)\n","        x = self.fc1(torch.cos(x))\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.relu(x)\n","        x = self.fc3(x)\n","        x = self.relu(x)\n","        x = self.fc4(x)\n","        x = self.relu(x)\n","        x = self.fc5(x)\n","        return x"]},{"cell_type":"code","execution_count":173,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[],"source":["kappa = 4 * torch.pi"]},{"cell_type":"code","execution_count":175,"metadata":{},"outputs":[],"source":["def target_func(x,y, kappa): \n","    pres = torch.cos(kappa*x) + torch.cos(kappa*y)\n","    return pres"]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[],"source":["class DirichletBC(nn.Module):\n","    def __init__(self):\n","        super(DirichletBC, self).__init__()\n","\n","    def forward(self, inputs, outputs, kappa):\n","        \"\"\"\n","        Impose Dirichlet BC on the boundary.\n","        Args:\n","        Returns:\n","            torch.Tensor: Computed loss (scalar).\n","        \"\"\"\n","        # Determine \n","\n","        for i, (x,y) in enumerate(inputs):\n","            if x==0 or x==1 or y==0 or y==1:\n","                outputs[i] = target_func(x, y, kappa)\n","        \n","        return outputs"]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[],"source":["def get_data(train_ds, valid_ds, bs, shuffle):\n","    return (\n","        DataLoader(train_ds, batch_size=bs, shuffle=shuffle),\n","        DataLoader(valid_ds, batch_size=bs),\n","    )"]},{"cell_type":"code","execution_count":178,"metadata":{},"outputs":[],"source":["train_ds = ECG_Dataset(df_train)\n","test_ds = ECG_Dataset(df_test)\n","train_dl, test_dl = get_data(train_ds, test_ds, 64, shuffle=False)"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[],"source":["def batch_loss_train(outputs, labels, inputs, kappa, loss_fn, optimizer):\n","    loss = loss_fn(outputs, labels, inputs, kappa)\n","    \n","    loss = loss.sum()\n","\n","    with torch.no_grad():\n","        # We can still compute loss and gradients for model parameters\n","        loss.backward(retain_graph=True)  # Retain graph to keep gradients for model parameters\n","    \n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    return loss.item()"]},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[],"source":["def batch_loss_test(outputs, labels, loss_fn):\n","    loss = loss_fn(outputs, labels)    \n","    return loss.item()"]},{"cell_type":"code","execution_count":181,"metadata":{},"outputs":[],"source":["def test_loop(dataloader, model, loss_fn):\n","    # Set the model to evaluation mode - important for batch normalization and dropout layers\n","    model.eval()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, mse, mape = 0, 0, 0\n","    epsilon = 1e-8\n","\n","    # Evaluating the model with torch.no_grad()    \n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            # test_loss += loss_fn(pred, y, X,kappa) / len(X)\n","            mse += torch.mean((y - pred) ** 2).item() \n","            y = y + epsilon\n","            mape += torch.mean(torch.abs((y - pred) / y)) \n","\n","    \n","    print(f\"Test set => Accuracy: {(mape):>0.4f}, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[],"source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    model.train()\n","    optimizer.zero_grad()\n","    train_loss = 0.0\n","    #all_outputs = torch.empty(0, 1, requires_grad=True)\n","    Dirichlet = DirichletBC()\n","    \n","    for inputs, labels in dataloader:\n","        # forward pass. Better without shuffle to keep the coordinates sorted\n","        \n","        outputs = model(inputs)\n","        outputs = Dirichlet(inputs,outputs, kappa)\n","        train_loss += batch_loss_train(outputs,labels,inputs,kappa,loss_fn, optimizer)\n","    \n","    \n","    print(f'Train loss: {train_loss}')\n","    "]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[],"source":["class HelmholtzLoss(nn.Module):\n","    def __init__(self):\n","        super(HelmholtzLoss, self).__init__()\n","\n","    def forward(self, preds, targets, inputs, kappa):\n","        \"\"\"\n","        Compute the Calculate the helmholtz eq. for predictions.\n","        labels (targets) should satisfy zero anyway\n","        \"\"\"\n","        grad_xy = torch.autograd.grad(outputs=preds, inputs=inputs, grad_outputs=torch.ones_like(preds), create_graph=True, allow_unused=True)[0]\n","                              \n","        grad_x = grad_xy[:,0].unsqueeze(1)\n","        grad_y = grad_xy[:,1].unsqueeze(1)\n","\n","        laplace_x = torch.autograd.grad(outputs=grad_x, inputs=inputs, grad_outputs=torch.ones_like(grad_x), create_graph=True, allow_unused=True)[0]\n","        grad_xx = laplace_x[:,0].unsqueeze(1)\n","\n","        laplace_y = torch.autograd.grad(outputs=grad_y, inputs=inputs, grad_outputs=torch.ones_like(grad_y), create_graph=True, allow_unused=True)[0]\n","        grad_yy = laplace_y[:,1].unsqueeze(1)\n","\n","        #print(preds.shape, grad_xx.shape)\n","\n","        loss = grad_xx + grad_yy + kappa**2 * preds\n","\n","        #loss = grad_xx + grad_yy + kappa**2 * target_func(inputs[:,0],inputs[:,1],kappa)\n","        \n","        return loss\n"]},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[],"source":["def fit(epochs, model, loss_fn, opt, train_dl, valid_dl):\n","    for t in range(epochs):  \n","        print(f\"Epoch {t+1}   -------------------------------\")\n","        train_loop(train_dl, model, loss_fn, optimizer)\n","        test_loop(test_dl, model, loss_fn)\n"]},{"cell_type":"code","execution_count":185,"metadata":{},"outputs":[],"source":["# Define the model\n","torch.set_printoptions(precision=6)\n","\n","input_size = 2  # Number of input features\n","output_size = 1  # Output size (e.g., regression or binary classification)\n","model = SimpleANN(input_size, output_size)\n","\n","# Define loss and optimizer\n","criterion =  HelmholtzLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":186,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1   -------------------------------\n","Train loss: -991.8505249023438\n","Test set => Accuracy: 2.9293, Avg loss: 0.000000 \n","\n","Epoch 2   -------------------------------\n","Train loss: -1155.8004150390625\n","Test set => Accuracy: 3.0963, Avg loss: 0.000000 \n","\n","Epoch 3   -------------------------------\n","Train loss: -1310.9017028808594\n","Test set => Accuracy: 3.2871, Avg loss: 0.000000 \n","\n","Epoch 4   -------------------------------\n","Train loss: -1472.034423828125\n","Test set => Accuracy: 3.5093, Avg loss: 0.000000 \n","\n","Epoch 5   -------------------------------\n","Train loss: -1649.3028564453125\n","Test set => Accuracy: 3.7475, Avg loss: 0.000000 \n","\n","Epoch 6   -------------------------------\n","Train loss: -1841.5528564453125\n","Test set => Accuracy: 4.0108, Avg loss: 0.000000 \n","\n","Epoch 7   -------------------------------\n","Train loss: -2051.4385375976562\n","Test set => Accuracy: 4.2941, Avg loss: 0.000000 \n","\n","Epoch 8   -------------------------------\n","Train loss: -2273.4134521484375\n","Test set => Accuracy: 4.5945, Avg loss: 0.000000 \n","\n","Epoch 9   -------------------------------\n","Train loss: -2509.455810546875\n","Test set => Accuracy: 4.9421, Avg loss: 0.000000 \n","\n","Epoch 10   -------------------------------\n","Train loss: -2777.49267578125\n","Test set => Accuracy: 5.3441, Avg loss: 0.000000 \n","\n","Epoch 11   -------------------------------\n","Train loss: -3080.7454833984375\n","Test set => Accuracy: 5.7989, Avg loss: 0.000000 \n","\n","Epoch 12   -------------------------------\n","Train loss: -3421.0633544921875\n","Test set => Accuracy: 6.2953, Avg loss: 0.000000 \n","\n","Epoch 13   -------------------------------\n","Train loss: -3782.1146240234375\n","Test set => Accuracy: 6.7490, Avg loss: 0.000000 \n","\n","Epoch 14   -------------------------------\n","Train loss: -4109.383056640625\n","Test set => Accuracy: 7.2046, Avg loss: 0.000000 \n","\n","Epoch 15   -------------------------------\n","Train loss: -4443.44873046875\n","Test set => Accuracy: 7.7288, Avg loss: 0.000000 \n","\n","Epoch 16   -------------------------------\n","Train loss: -4836.8643798828125\n","Test set => Accuracy: 8.3708, Avg loss: 0.000000 \n","\n","Epoch 17   -------------------------------\n","Train loss: -5315.212646484375\n","Test set => Accuracy: 9.1234, Avg loss: 0.000000 \n","\n","Epoch 18   -------------------------------\n","Train loss: -5868.98388671875\n","Test set => Accuracy: 9.9808, Avg loss: 0.000000 \n","\n","Epoch 19   -------------------------------\n","Train loss: -6497.752685546875\n","Test set => Accuracy: 10.9584, Avg loss: 0.000000 \n","\n","Epoch 20   -------------------------------\n","Train loss: -7215.99560546875\n","Test set => Accuracy: 12.1149, Avg loss: 0.000000 \n","\n","Epoch 21   -------------------------------\n","Train loss: -8059.078857421875\n","Test set => Accuracy: 13.4589, Avg loss: 0.000000 \n","\n","Epoch 22   -------------------------------\n","Train loss: -9034.2060546875\n","Test set => Accuracy: 15.0126, Avg loss: 0.000000 \n","\n","Epoch 23   -------------------------------\n","Train loss: -10155.34912109375\n","Test set => Accuracy: 16.8047, Avg loss: 0.000000 \n","\n","Epoch 24   -------------------------------\n","Train loss: -11439.625\n","Test set => Accuracy: 18.8706, Avg loss: 0.000000 \n","\n","Epoch 25   -------------------------------\n","Train loss: -12907.47265625\n","Test set => Accuracy: 21.2452, Avg loss: 0.000000 \n","\n","Epoch 26   -------------------------------\n","Train loss: -14582.08837890625\n","Test set => Accuracy: 23.9663, Avg loss: 0.000000 \n","\n","Epoch 27   -------------------------------\n","Train loss: -16489.33154296875\n","Test set => Accuracy: 27.0690, Avg loss: 0.000000 \n","\n","Epoch 28   -------------------------------\n","Train loss: -18657.31591796875\n","Test set => Accuracy: 30.6011, Avg loss: 0.000000 \n","\n","Epoch 29   -------------------------------\n","Train loss: -21117.3388671875\n","Test set => Accuracy: 34.6092, Avg loss: 0.000000 \n","\n","Epoch 30   -------------------------------\n","Train loss: -23904.15087890625\n","Test set => Accuracy: 39.1428, Avg loss: 0.000000 \n","\n","Epoch 31   -------------------------------\n","Train loss: -27056.294921875\n","Test set => Accuracy: 44.2617, Avg loss: 0.000000 \n","\n","Epoch 32   -------------------------------\n","Train loss: -30614.7021484375\n","Test set => Accuracy: 50.0309, Avg loss: 0.000000 \n","\n","Epoch 33   -------------------------------\n","Train loss: -34624.310546875\n","Test set => Accuracy: 56.5213, Avg loss: 0.000000 \n","\n","Epoch 34   -------------------------------\n","Train loss: -39133.8291015625\n","Test set => Accuracy: 63.8096, Avg loss: 0.000000 \n","\n","Epoch 35   -------------------------------\n","Train loss: -44196.1953125\n","Test set => Accuracy: 71.9776, Avg loss: 0.000000 \n","\n","Epoch 36   -------------------------------\n","Train loss: -49868.23828125\n","Test set => Accuracy: 81.1138, Avg loss: 0.000000 \n","\n","Epoch 37   -------------------------------\n","Train loss: -56211.69921875\n","Test set => Accuracy: 91.3143, Avg loss: 0.000000 \n","\n","Epoch 38   -------------------------------\n","Train loss: -63292.76953125\n","Test set => Accuracy: 102.6879, Avg loss: 0.000000 \n","\n","Epoch 39   -------------------------------\n","Train loss: -71189.705078125\n","Test set => Accuracy: 115.3692, Avg loss: 0.000000 \n","\n","Epoch 40   -------------------------------\n","Train loss: -79991.388671875\n","Test set => Accuracy: 129.4728, Avg loss: 0.000000 \n","\n","Epoch 41   -------------------------------\n","Train loss: -89778.681640625\n","Test set => Accuracy: 145.1288, Avg loss: 0.000000 \n","\n","Epoch 42   -------------------------------\n","Train loss: -100641.0859375\n","Test set => Accuracy: 162.4776, Avg loss: 0.000000 \n","\n","Epoch 43   -------------------------------\n","Train loss: -112676.19140625\n","Test set => Accuracy: 181.6701, Avg loss: 0.000000 \n","\n","Epoch 44   -------------------------------\n","Train loss: -125987.41015625\n","Test set => Accuracy: 202.8668, Avg loss: 0.000000 \n","\n","Epoch 45   -------------------------------\n","Train loss: -140688.234375\n","Test set => Accuracy: 226.2388, Avg loss: 0.000000 \n","\n","Epoch 46   -------------------------------\n","Train loss: -156896.23828125\n","Test set => Accuracy: 251.9698, Avg loss: 0.000000 \n","\n","Epoch 47   -------------------------------\n","Train loss: -174736.3046875\n","Test set => Accuracy: 280.2524, Avg loss: 0.000000 \n","\n","Epoch 48   -------------------------------\n","Train loss: -194343.5703125\n","Test set => Accuracy: 311.2917, Avg loss: 0.000000 \n","\n","Epoch 49   -------------------------------\n","Train loss: -215861.0859375\n","Test set => Accuracy: 345.3051, Avg loss: 0.000000 \n","\n","Epoch 50   -------------------------------\n","Train loss: -239438.5859375\n","Test set => Accuracy: 382.5216, Avg loss: 0.000000 \n","\n","Epoch 51   -------------------------------\n","Train loss: -265236.3359375\n","Test set => Accuracy: 423.1906, Avg loss: 0.000000 \n","\n","Epoch 52   -------------------------------\n","Train loss: -293422.0\n","Test set => Accuracy: 467.5702, Avg loss: 0.000000 \n","\n","Epoch 53   -------------------------------\n","Train loss: -324174.53125\n","Test set => Accuracy: 515.9323, Avg loss: 0.000000 \n","\n","Epoch 54   -------------------------------\n","Train loss: -357684.0703125\n","Test set => Accuracy: 568.5648, Avg loss: 0.000000 \n","\n","Epoch 55   -------------------------------\n","Train loss: -394148.921875\n","Test set => Accuracy: 625.7747, Avg loss: 0.000000 \n","\n","Epoch 56   -------------------------------\n","Train loss: -433775.34375\n","Test set => Accuracy: 687.8778, Avg loss: 0.000000 \n","\n","Epoch 57   -------------------------------\n","Train loss: -476783.96875\n","Test set => Accuracy: 755.2159, Avg loss: 0.000000 \n","\n","Epoch 58   -------------------------------\n","Train loss: -523402.625\n","Test set => Accuracy: 828.1422, Avg loss: 0.000000 \n","\n","Epoch 59   -------------------------------\n","Train loss: -573872.78125\n","Test set => Accuracy: 907.0231, Avg loss: 0.000000 \n","\n","Epoch 60   -------------------------------\n","Train loss: -628447.578125\n","Test set => Accuracy: 992.2357, Avg loss: 0.000000 \n","\n","Epoch 61   -------------------------------\n","Train loss: -687392.3125\n","Test set => Accuracy: 1084.1819, Avg loss: 0.000000 \n","\n","Epoch 62   -------------------------------\n","Train loss: -750984.625\n","Test set => Accuracy: 1183.2838, Avg loss: 0.000000 \n","\n","Epoch 63   -------------------------------\n","Train loss: -819515.21875\n","Test set => Accuracy: 1289.9829, Avg loss: 0.000000 \n","\n","Epoch 64   -------------------------------\n","Train loss: -893288.0625\n","Test set => Accuracy: 1404.7402, Avg loss: 0.000000 \n","\n","Epoch 65   -------------------------------\n","Train loss: -972620.21875\n","Test set => Accuracy: 1528.0369, Avg loss: 0.000000 \n","\n","Epoch 66   -------------------------------\n","Train loss: -1057843.09375\n","Test set => Accuracy: 1660.3752, Avg loss: 0.000000 \n","\n","Epoch 67   -------------------------------\n","Train loss: -1149302.15625\n","Test set => Accuracy: 1802.2783, Avg loss: 0.000000 \n","\n","Epoch 68   -------------------------------\n","Train loss: -1247357.65625\n","Test set => Accuracy: 1954.2915, Avg loss: 0.000000 \n","\n","Epoch 69   -------------------------------\n","Train loss: -1352384.15625\n","Test set => Accuracy: 2116.9817, Avg loss: 0.000000 \n","\n","Epoch 70   -------------------------------\n","Train loss: -1464772.5\n","Test set => Accuracy: 2290.9399, Avg loss: 0.000000 \n","\n","Epoch 71   -------------------------------\n","Train loss: -1584928.4375\n","Test set => Accuracy: 2476.7783, Avg loss: 0.000000 \n","\n","Epoch 72   -------------------------------\n","Train loss: -1713274.0\n","Test set => Accuracy: 2675.1362, Avg loss: 0.000000 \n","\n","Epoch 73   -------------------------------\n","Train loss: -1850247.9375\n","Test set => Accuracy: 2886.6748, Avg loss: 0.000000 \n","\n","Epoch 74   -------------------------------\n","Train loss: -1996305.0625\n","Test set => Accuracy: 3112.0806, Avg loss: 0.000000 \n","\n","Epoch 75   -------------------------------\n","Train loss: -2151917.5\n","Test set => Accuracy: 3352.0659, Avg loss: 0.000000 \n","\n","Epoch 76   -------------------------------\n","Train loss: -2317575.5625\n","Test set => Accuracy: 3607.3691, Avg loss: 0.000000 \n","\n","Epoch 77   -------------------------------\n","Train loss: -2493785.9375\n","Test set => Accuracy: 3878.7554, Avg loss: 0.000000 \n","\n","Epoch 78   -------------------------------\n","Train loss: -2681075.125\n","Test set => Accuracy: 4167.0156, Avg loss: 0.000000 \n","\n","Epoch 79   -------------------------------\n","Train loss: -2879987.125\n","Test set => Accuracy: 4472.9697, Avg loss: 0.000000 \n","\n","Epoch 80   -------------------------------\n","Train loss: -3091084.8125\n","Test set => Accuracy: 4797.4653, Avg loss: 0.000000 \n","\n","Epoch 81   -------------------------------\n","Train loss: -3314950.625\n","Test set => Accuracy: 5141.3774, Avg loss: 0.000000 \n","\n","Epoch 82   -------------------------------\n","Train loss: -3552186.625\n","Test set => Accuracy: 5505.6123, Avg loss: 0.000000 \n","\n","Epoch 83   -------------------------------\n","Train loss: -3803415.0\n","Test set => Accuracy: 5891.1035, Avg loss: 0.000000 \n","\n","Epoch 84   -------------------------------\n","Train loss: -4069277.5\n","Test set => Accuracy: 6298.8174, Avg loss: 0.000000 \n","\n","Epoch 85   -------------------------------\n","Train loss: -4350437.875\n","Test set => Accuracy: 6729.7480, Avg loss: 0.000000 \n","\n","Epoch 86   -------------------------------\n","Train loss: -4647578.875\n","Test set => Accuracy: 7184.9229, Avg loss: 0.000000 \n","\n","Epoch 87   -------------------------------\n","Train loss: -4961405.5\n","Test set => Accuracy: 7665.3999, Avg loss: 0.000000 \n","\n","Epoch 88   -------------------------------\n","Train loss: -5292646.0\n","Test set => Accuracy: 8172.2690, Avg loss: 0.000000 \n","\n","Epoch 89   -------------------------------\n","Train loss: -5642049.125\n","Test set => Accuracy: 8706.6543, Avg loss: 0.000000 \n","\n","Epoch 90   -------------------------------\n","Train loss: -6010386.75\n","Test set => Accuracy: 9269.7148, Avg loss: 0.000000 \n","\n","Epoch 91   -------------------------------\n","Train loss: -6398451.5\n","Test set => Accuracy: 9862.6348, Avg loss: 0.000000 \n","\n","Epoch 92   -------------------------------\n","Train loss: -6807062.0\n","Test set => Accuracy: 10486.6436, Avg loss: 0.000000 \n","\n","Epoch 93   -------------------------------\n","Train loss: -7237058.25\n","Test set => Accuracy: 11142.9980, Avg loss: 0.000000 \n","\n","Epoch 94   -------------------------------\n","Train loss: -7689306.0\n","Test set => Accuracy: 11832.9941, Avg loss: 0.000000 \n","\n","Epoch 95   -------------------------------\n","Train loss: -8164692.0\n","Test set => Accuracy: 12557.9600, Avg loss: 0.000000 \n","\n","Epoch 96   -------------------------------\n","Train loss: -8664132.0\n","Test set => Accuracy: 13319.2637, Avg loss: 0.000000 \n","\n","Epoch 97   -------------------------------\n","Train loss: -9188561.75\n","Test set => Accuracy: 14118.3057, Avg loss: 0.000000 \n","\n","Epoch 98   -------------------------------\n","Train loss: -9738944.0\n","Test set => Accuracy: 14956.5312, Avg loss: 0.000000 \n","\n","Epoch 99   -------------------------------\n","Train loss: -10316271.25\n","Test set => Accuracy: 15835.4121, Avg loss: 0.000000 \n","\n","Epoch 100   -------------------------------\n","Train loss: -10921553.5\n","Test set => Accuracy: 16756.4688, Avg loss: 0.000000 \n","\n","Epoch 101   -------------------------------\n","Train loss: -11555832.75\n","Test set => Accuracy: 17721.2520, Avg loss: 0.000000 \n","\n","Epoch 102   -------------------------------\n","Train loss: -12220175.75\n","Test set => Accuracy: 18731.3613, Avg loss: 0.000000 \n","\n","Epoch 103   -------------------------------\n","Train loss: -12915677.5\n","Test set => Accuracy: 19788.4238, Avg loss: 0.000000 \n","\n","Epoch 104   -------------------------------\n","Train loss: -13643458.0\n","Test set => Accuracy: 20894.1133, Avg loss: 0.000000 \n","\n","Epoch 105   -------------------------------\n","Train loss: -14404666.0\n","Test set => Accuracy: 22050.1523, Avg loss: 0.000000 \n","\n","Epoch 106   -------------------------------\n","Train loss: -15200479.5\n","Test set => Accuracy: 23258.2871, Avg loss: 0.000000 \n","\n","Epoch 107   -------------------------------\n","Train loss: -16032097.0\n","Test set => Accuracy: 24520.3164, Avg loss: 0.000000 \n","\n","Epoch 108   -------------------------------\n","Train loss: -16900755.5\n","Test set => Accuracy: 25838.0781, Avg loss: 0.000000 \n","\n","Epoch 109   -------------------------------\n","Train loss: -17807716.5\n","Test set => Accuracy: 27213.4590, Avg loss: 0.000000 \n","\n","Epoch 110   -------------------------------\n","Train loss: -18754271.0\n","Test set => Accuracy: 28648.3750, Avg loss: 0.000000 \n","\n","Epoch 111   -------------------------------\n","Train loss: -19741737.5\n","Test set => Accuracy: 30144.8008, Avg loss: 0.000000 \n","\n","Epoch 112   -------------------------------\n","Train loss: -20771466.0\n","Test set => Accuracy: 31704.7383, Avg loss: 0.000000 \n","\n","Epoch 113   -------------------------------\n","Train loss: -21844835.5\n","Test set => Accuracy: 33330.2500, Avg loss: 0.000000 \n","\n","Epoch 114   -------------------------------\n","Train loss: -22963253.0\n","Test set => Accuracy: 35023.4297, Avg loss: 0.000000 \n","\n","Epoch 115   -------------------------------\n","Train loss: -24128164.5\n","Test set => Accuracy: 36786.4336, Avg loss: 0.000000 \n","\n","Epoch 116   -------------------------------\n","Train loss: -25341039.5\n","Test set => Accuracy: 38621.4453, Avg loss: 0.000000 \n","\n","Epoch 117   -------------------------------\n","Train loss: -26603377.5\n","Test set => Accuracy: 40530.6914, Avg loss: 0.000000 \n","\n","Epoch 118   -------------------------------\n","Train loss: -27916714.0\n","Test set => Accuracy: 42516.4688, Avg loss: 0.000000 \n","\n","Epoch 119   -------------------------------\n","Train loss: -29282616.0\n","Test set => Accuracy: 44581.1016, Avg loss: 0.000000 \n","\n","Epoch 120   -------------------------------\n","Train loss: -30702681.0\n","Test set => Accuracy: 46726.9688, Avg loss: 0.000000 \n","\n","Epoch 121   -------------------------------\n","Train loss: -32178538.0\n","Test set => Accuracy: 48956.5000, Avg loss: 0.000000 \n","\n","Epoch 122   -------------------------------\n","Train loss: -33711851.0\n","Test set => Accuracy: 51272.1562, Avg loss: 0.000000 \n","\n","Epoch 123   -------------------------------\n","Train loss: -35304315.0\n","Test set => Accuracy: 53676.4766, Avg loss: 0.000000 \n","\n","Epoch 124   -------------------------------\n","Train loss: -36957662.0\n","Test set => Accuracy: 56172.0000, Avg loss: 0.000000 \n","\n","Epoch 125   -------------------------------\n","Train loss: -38673647.0\n","Test set => Accuracy: 58761.3828, Avg loss: 0.000000 \n","\n","Epoch 126   -------------------------------\n","Train loss: -40454071.0\n","Test set => Accuracy: 61447.2773, Avg loss: 0.000000 \n","\n","Epoch 127   -------------------------------\n","Train loss: -42300769.0\n","Test set => Accuracy: 64232.4062, Avg loss: 0.000000 \n","\n","Epoch 128   -------------------------------\n","Train loss: -44215596.0\n","Test set => Accuracy: 67119.5312, Avg loss: 0.000000 \n","\n","Epoch 129   -------------------------------\n","Train loss: -46200462.0\n","Test set => Accuracy: 70111.4844, Avg loss: 0.000000 \n","\n","Epoch 130   -------------------------------\n","Train loss: -48257291.0\n","Test set => Accuracy: 73211.1406, Avg loss: 0.000000 \n","\n","Epoch 131   -------------------------------\n","Train loss: -50388058.0\n","Test set => Accuracy: 76421.4219, Avg loss: 0.000000 \n","\n","Epoch 132   -------------------------------\n","Train loss: -52594772.0\n","Test set => Accuracy: 79745.2969, Avg loss: 0.000000 \n","\n","Epoch 133   -------------------------------\n","Train loss: -54879462.0\n","Test set => Accuracy: 83185.7969, Avg loss: 0.000000 \n","\n","Epoch 134   -------------------------------\n","Train loss: -57244212.0\n","Test set => Accuracy: 86746.0156, Avg loss: 0.000000 \n","\n","Epoch 135   -------------------------------\n","Train loss: -59691142.0\n","Test set => Accuracy: 90429.0547, Avg loss: 0.000000 \n","\n","Epoch 136   -------------------------------\n","Train loss: -62222376.0\n","Test set => Accuracy: 94238.1406, Avg loss: 0.000000 \n","\n","Epoch 137   -------------------------------\n","Train loss: -64840130.0\n","Test set => Accuracy: 98176.5156, Avg loss: 0.000000 \n","\n","Epoch 138   -------------------------------\n","Train loss: -67546618.0\n","Test set => Accuracy: 102247.4609, Avg loss: 0.000000 \n","\n","Epoch 139   -------------------------------\n","Train loss: -70344084.0\n","Test set => Accuracy: 106454.3359, Avg loss: 0.000000 \n","\n","Epoch 140   -------------------------------\n","Train loss: -73234844.0\n","Test set => Accuracy: 110800.5469, Avg loss: 0.000000 \n","\n","Epoch 141   -------------------------------\n","Train loss: -76221226.0\n","Test set => Accuracy: 115289.5547, Avg loss: 0.000000 \n","\n","Epoch 142   -------------------------------\n","Train loss: -79305606.0\n","Test set => Accuracy: 119924.8750, Avg loss: 0.000000 \n","\n","Epoch 143   -------------------------------\n","Train loss: -82490392.0\n","Test set => Accuracy: 124710.0859, Avg loss: 0.000000 \n","\n","Epoch 144   -------------------------------\n","Train loss: -85778040.0\n","Test set => Accuracy: 129648.8281, Avg loss: 0.000000 \n","\n","Epoch 145   -------------------------------\n","Train loss: -89171030.0\n","Test set => Accuracy: 134744.7812, Avg loss: 0.000000 \n","\n","Epoch 146   -------------------------------\n","Train loss: -92671896.0\n","Test set => Accuracy: 140001.6875, Avg loss: 0.000000 \n","\n","Epoch 147   -------------------------------\n","Train loss: -96283196.0\n","Test set => Accuracy: 145423.3750, Avg loss: 0.000000 \n","\n","Epoch 148   -------------------------------\n","Train loss: -100007552.0\n","Test set => Accuracy: 151013.6719, Avg loss: 0.000000 \n","\n","Epoch 149   -------------------------------\n","Train loss: -103847600.0\n","Test set => Accuracy: 156776.5156, Avg loss: 0.000000 \n","\n","Epoch 150   -------------------------------\n","Train loss: -107806032.0\n","Test set => Accuracy: 162715.8750, Avg loss: 0.000000 \n","\n","Epoch 151   -------------------------------\n","Train loss: -111885564.0\n","Test set => Accuracy: 168835.7969, Avg loss: 0.000000 \n","\n","Epoch 152   -------------------------------\n","Train loss: -116088964.0\n","Test set => Accuracy: 175140.3750, Avg loss: 0.000000 \n","\n","Epoch 153   -------------------------------\n","Train loss: -120419040.0\n","Test set => Accuracy: 181633.7812, Avg loss: 0.000000 \n","\n","Epoch 154   -------------------------------\n","Train loss: -124878656.0\n","Test set => Accuracy: 188320.1875, Avg loss: 0.000000 \n","\n","Epoch 155   -------------------------------\n","Train loss: -129470684.0\n","Test set => Accuracy: 195203.9062, Avg loss: 0.000000 \n","\n","Epoch 156   -------------------------------\n","Train loss: -134198056.0\n","Test set => Accuracy: 202289.2812, Avg loss: 0.000000 \n","\n","Epoch 157   -------------------------------\n","Train loss: -139063728.0\n","Test set => Accuracy: 209580.6875, Avg loss: 0.000000 \n","\n","Epoch 158   -------------------------------\n","Train loss: -144070752.0\n","Test set => Accuracy: 217082.5938, Avg loss: 0.000000 \n","\n","Epoch 159   -------------------------------\n","Train loss: -149222144.0\n","Test set => Accuracy: 224799.5312, Avg loss: 0.000000 \n","\n","Epoch 160   -------------------------------\n","Train loss: -154521020.0\n","Test set => Accuracy: 232736.0469, Avg loss: 0.000000 \n","\n","Epoch 161   -------------------------------\n","Train loss: -159970528.0\n","Test set => Accuracy: 240896.8125, Avg loss: 0.000000 \n","\n","Epoch 162   -------------------------------\n","Train loss: -165573828.0\n","Test set => Accuracy: 249286.5312, Avg loss: 0.000000 \n","\n","Epoch 163   -------------------------------\n","Train loss: -171334144.0\n","Test set => Accuracy: 257909.9688, Avg loss: 0.000000 \n","\n","Epoch 164   -------------------------------\n","Train loss: -177254772.0\n","Test set => Accuracy: 266771.9688, Avg loss: 0.000000 \n","\n","Epoch 165   -------------------------------\n","Train loss: -183338984.0\n","Test set => Accuracy: 275877.4062, Avg loss: 0.000000 \n","\n","Epoch 166   -------------------------------\n","Train loss: -189590164.0\n","Test set => Accuracy: 285231.2500, Avg loss: 0.000000 \n","\n","Epoch 167   -------------------------------\n","Train loss: -196011676.0\n","Test set => Accuracy: 294838.5625, Avg loss: 0.000000 \n","\n","Epoch 168   -------------------------------\n","Train loss: -202607020.0\n","Test set => Accuracy: 304704.3438, Avg loss: 0.000000 \n","\n","Epoch 169   -------------------------------\n","Train loss: -209379616.0\n","Test set => Accuracy: 314833.8125, Avg loss: 0.000000 \n","\n","Epoch 170   -------------------------------\n","Train loss: -216333008.0\n","Test set => Accuracy: 325232.1562, Avg loss: 0.000000 \n","\n","Epoch 171   -------------------------------\n","Train loss: -223470792.0\n","Test set => Accuracy: 335904.6875, Avg loss: 0.000000 \n","\n","Epoch 172   -------------------------------\n","Train loss: -230796560.0\n","Test set => Accuracy: 346856.6875, Avg loss: 0.000000 \n","\n","Epoch 173   -------------------------------\n","Train loss: -238314016.0\n","Test set => Accuracy: 358093.6250, Avg loss: 0.000000 \n","\n","Epoch 174   -------------------------------\n","Train loss: -246026792.0\n","Test set => Accuracy: 369621.0625, Avg loss: 0.000000 \n","\n","Epoch 175   -------------------------------\n","Train loss: -253938712.0\n","Test set => Accuracy: 381444.3125, Avg loss: 0.000000 \n","\n","Epoch 176   -------------------------------\n","Train loss: -262053520.0\n","Test set => Accuracy: 393569.1875, Avg loss: 0.000000 \n","\n","Epoch 177   -------------------------------\n","Train loss: -270375136.0\n","Test set => Accuracy: 406001.2812, Avg loss: 0.000000 \n","\n","Epoch 178   -------------------------------\n","Train loss: -278907336.0\n","Test set => Accuracy: 418746.3125, Avg loss: 0.000000 \n","\n","Epoch 179   -------------------------------\n","Train loss: -287654136.0\n","Test set => Accuracy: 431810.1875, Avg loss: 0.000000 \n","\n","Epoch 180   -------------------------------\n","Train loss: -296619480.0\n","Test set => Accuracy: 445198.7188, Avg loss: 0.000000 \n","\n","Epoch 181   -------------------------------\n","Train loss: -305807408.0\n","Test set => Accuracy: 458917.8438, Avg loss: 0.000000 \n","\n","Epoch 182   -------------------------------\n","Train loss: -315222000.0\n","Test set => Accuracy: 472973.5625, Avg loss: 0.000000 \n","\n","Epoch 183   -------------------------------\n","Train loss: -324867312.0\n","Test set => Accuracy: 487372.0000, Avg loss: 0.000000 \n","\n","Epoch 184   -------------------------------\n","Train loss: -334747608.0\n","Test set => Accuracy: 502119.2500, Avg loss: 0.000000 \n","\n","Epoch 185   -------------------------------\n","Train loss: -344867016.0\n","Test set => Accuracy: 517221.5625, Avg loss: 0.000000 \n","\n","Epoch 186   -------------------------------\n","Train loss: -355229784.0\n","Test set => Accuracy: 532685.2500, Avg loss: 0.000000 \n","\n","Epoch 187   -------------------------------\n","Train loss: -365840344.0\n","Test set => Accuracy: 548516.6250, Avg loss: 0.000000 \n","\n","Epoch 188   -------------------------------\n","Train loss: -376702880.0\n","Test set => Accuracy: 564722.1875, Avg loss: 0.000000 \n","\n","Epoch 189   -------------------------------\n","Train loss: -387821888.0\n","Test set => Accuracy: 581308.3750, Avg loss: 0.000000 \n","\n","Epoch 190   -------------------------------\n","Train loss: -399201824.0\n","Test set => Accuracy: 598281.6875, Avg loss: 0.000000 \n","\n","Epoch 191   -------------------------------\n","Train loss: -410847112.0\n","Test set => Accuracy: 615648.8125, Avg loss: 0.000000 \n","\n","Epoch 192   -------------------------------\n","Train loss: -422762360.0\n","Test set => Accuracy: 633416.5625, Avg loss: 0.000000 \n","\n","Epoch 193   -------------------------------\n","Train loss: -434952128.0\n","Test set => Accuracy: 651591.5000, Avg loss: 0.000000 \n","\n","Epoch 194   -------------------------------\n","Train loss: -447421056.0\n","Test set => Accuracy: 670180.6250, Avg loss: 0.000000 \n","\n","Epoch 195   -------------------------------\n","Train loss: -460173824.0\n","Test set => Accuracy: 689190.7500, Avg loss: 0.000000 \n","\n","Epoch 196   -------------------------------\n","Train loss: -473215200.0\n","Test set => Accuracy: 708628.8750, Avg loss: 0.000000 \n","\n","Epoch 197   -------------------------------\n","Train loss: -486549920.0\n","Test set => Accuracy: 728502.1250, Avg loss: 0.000000 \n","\n","Epoch 198   -------------------------------\n","Train loss: -500182816.0\n","Test set => Accuracy: 748817.6250, Avg loss: 0.000000 \n","\n","Epoch 199   -------------------------------\n","Train loss: -514118784.0\n","Test set => Accuracy: 769582.5625, Avg loss: 0.000000 \n","\n","Epoch 200   -------------------------------\n","Train loss: -528362752.0\n","Test set => Accuracy: 790804.1250, Avg loss: 0.000000 \n","\n"]}],"source":["fit(30, model, criterion, optimizer, train_dl, test_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":29414,"sourceId":37484,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
