{"cells":[{"cell_type":"code","execution_count":98,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-21T08:21:29.078990Z","iopub.status.busy":"2024-03-21T08:21:29.078141Z","iopub.status.idle":"2024-03-21T08:21:29.100221Z","shell.execute_reply":"2024-03-21T08:21:29.099372Z","shell.execute_reply.started":"2024-03-21T08:21:29.078956Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra \n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.metrics import accuracy_score, classification_report\n","import torch\n","import matplotlib.pyplot as plt\n","import os"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[],"source":["dataset_name = \"2D_Helm/\"\n","\n","#Configurpave to be dynamically adjusted\n","download_path = \"../data/\" #In the .gitignore list an\n","\n","#n the rest of the code.\n","path_to_datasets = download_path + \"/\" + dataset_name "]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:29.107643Z","iopub.status.busy":"2024-03-21T08:21:29.107375Z","iopub.status.idle":"2024-03-21T08:21:41.062102Z","shell.execute_reply":"2024-03-21T08:21:41.060979Z","shell.execute_reply.started":"2024-03-21T08:21:29.107621Z"},"trusted":true},"outputs":[],"source":["# This cell now makes use of the downloadfolder for the datasets.\n","df_train= pd.read_csv(path_to_datasets + \"/\" + 'helm_train.csv')\n","df_test=pd.read_csv(path_to_datasets + \"/\" +  'helm_test.csv')\n","#print(\"Dataframes MITBIH correctly read into workspace\")\n","\n","#split target and value\n","train_target=df_train['p(x,y)']\n","test_target=df_test['p(x,y)']\n","train=df_train.drop('p(x,y)',axis=1)\n","test=df_test.drop('p(x,y)',axis=1)"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>p(x,y)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.8</td>\n","      <td>-0.64</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     x    y  p(x,y)\n","4  0.0  0.8   -0.64"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["df_train.sample(1)"]},{"cell_type":"code","execution_count":102,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.065807Z","iopub.status.busy":"2024-03-21T08:21:41.065436Z","iopub.status.idle":"2024-03-21T08:21:41.072551Z","shell.execute_reply":"2024-03-21T08:21:41.071545Z","shell.execute_reply.started":"2024-03-21T08:21:41.065770Z"},"trusted":true},"outputs":[],"source":["Train_Simple_ANN = True #Trains the simple ANN"]},{"cell_type":"markdown","metadata":{},"source":["The following functions should normally be imported from the other script"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["def grad_x(p, len_x): \n","    \"\"\"Calculate the gradient of the input p in x-direction\n","       It assumes uniformly distributed nodes in a domain D=[0,1] x [0,1].\n","       Hence, len_x = len_y.\n","       Adjust coordinates and geometry if needed, or alternatively transform variables.\n","    \"\"\"\n","    epsilon = 1e-8 #\n","    dX = 1.0/(len_x-1) #Spacing h is denoted with dX\n","    gradp_x=torch.zeros_like(p) + epsilon\n","\n","    for i in range(1,len_x-1):\n","        for j in range(0,len_x):\n","            gradp_x[i*len_x+j] = (p[(i+1)*len_x+j] - p[(i-1)*len_x+j]) * 0.5/dX\n","\n","    #One-sided gradient for @ x=0. It means i==0. \n","    for j in range(0,len_x):\n","        gradp_x[j] = ( -p[2*len_x+j] +4*p[len_x+j] - 3*p[j]) * 0.5/dX\n","\n","\n","    #One-sided gradient for @ x=1. It means i==lenX-1. Second-order accurate\n","    for j in range(0,len_x):\n","        gradp_x[(len_x-1)*len_x+j] =  (3*p[(len_x-1)*len_x+j] - 4*p[(len_x-2)*len_x+j] + p[(len_x-3)*len_x+j]) * 0.5/dX\n","\n","    return gradp_x"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["def grad_y(p, len_x): \n","    \"\"\"Calculate the gradient of the input p in y-direction\n","       It assumes uniformly distributed nodes in a domain D=[0,1] x [0,1].\n","       Hence, len_x = len_y.\n","       Adjust coordinates and geometry if needed, or alternatively transform variables.\n","    \"\"\"\n","    epsilon = 1e-8 #\n","    dX = 1.0/(len_x-1) #Spacing h is denoted with dX\n","    gradp_y=torch.zeros_like(p) + epsilon\n","\n","    for i in range(0,len_x):\n","        for j in range(1,len_x-1):\n","            gradp_y[i*len_x+j] = (p[i*len_x+j+1] - p[i*len_x+j-1]) * 0.5/dX\n","\n","    # One-sided @ y=0. Corresponds j=0.\n","    for i in range(0,len_x):\n","            gradp_y[i*len_x] = (-p[i*len_x+2] + 4*p[i*len_x+1] - 3*p[i*len_x]) * 0.5/dX\n","\n","    # One-sided @ y=1. Corresponds j=lenY-1.\n","    for i in range(0,len_x):\n","            gradp_y[(i+1)*len_x-1] = (-p[(i+1)*len_x-3] + 4*p[(i+1)*len_x-2] - 3*p[(i+1)*len_x-1]) * 0.5/dX\n","    \n","    return gradp_y"]},{"cell_type":"markdown","metadata":{},"source":["## **Simple Artificial Neural Network**\n","ANN without convolutional layers. Only Dense layers are used. No Pooling, Flattening or Dropping out. Base model for later comparison."]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","metadata":{},"source":["Implement Torch Dataset object"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["class ECG_Dataset(Dataset):\n","    def __init__(self, csv_file, transform=None, target_transform=None):\n","        self.dataframe = csv_file.values\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","        #return self.dataframe.shape[0] # Alternative notation\n","\n","    def __getitem__(self, idx):\n","        inputs = torch.tensor(self.dataframe[idx,:-1], requires_grad=True).to(torch.float32)\n","        label = torch.tensor(self.dataframe[idx,-1]).to(torch.float32)\n","\n","        return inputs, label"]},{"cell_type":"markdown","metadata":{},"source":["Custom function for preprocessing (to elaborate later, currently just returns the input itself)"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["class Lambda(nn.Module):\n","    def __init__(self, func):\n","        super().__init__()\n","        self.func = func\n","\n","    def forward(self, x):\n","        return self.func(x)\n","\n","\n","def preprocess(x):\n","    return x * torch.Tensor([1.0])"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["# Define the ANN model\n","class SimpleANN(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super().__init__()\n","        self.fc0 = nn.Sequential(Lambda(preprocess))\n","        self.fc1 = nn.Linear(input_size, 24)\n","        self.fc2 = nn.Linear(24, 48)  \n","        self.fc3 = nn.Linear(48, 24) \n","        self.fc4 = nn.Linear(24, 12)  \n","        self.fc5 = nn.Linear(12, output_size)  # Hidden to output layer\n","        self.fc6 = nn.Linear(6, output_size)  # Hidden to output layer\n","        self.relu = nn.LeakyReLU(negative_slope=0.001)    # Activation function\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.sigmoid = nn.Sigmoid() \n","\n","    def forward(self, x):\n","        #x = self.fc0(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.relu(x)\n","        x = self.fc3(x)\n","        x = self.relu(x)\n","        x = self.fc4(x)\n","        x = self.relu(x)\n","        x = self.fc5(x)\n","        return x"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[],"source":["kappa = 0 * torch.pi"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[],"source":["lenX = int(np.sqrt(len(df_train)))\n","lenY = lenX\n","batch_size = 36\n","lambdaDBC = 1 #Weight factor for the Dirichlet loss. "]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[],"source":["def target_func(x,y, kappa): \n","    pres = x*x - y*y\n","    return pres"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["class DirichletBC(nn.Module):\n","    def __init__(self):\n","        super(DirichletBC, self).__init__()\n","\n","    def forward(self, inputs, target_func, kappa):\n","        \"\"\"\n","        Impose Dirichlet BC on the boundary.\n","        Args:\n","        Returns:\n","            torch.Tensor: Computed loss (scalar).\n","        \"\"\"\n","        DirichletValues = []\n","        DirichletIndex = []\n","\n","        for i, (x,y) in enumerate(inputs):\n","            if x==0 or x==1 or y==0 or y==1:\n","                DirichletValues.append(target_func(inputs[i,0], inputs[i,1], kappa))\n","                DirichletIndex.append(i)\n","        \n","        DirichletValues = torch.tensor(DirichletValues,requires_grad=True)\n","        DirichletIndex = torch.tensor(DirichletIndex,requires_grad=False)\n","                   \n","        return DirichletIndex.unsqueeze(1), DirichletValues.unsqueeze(1)"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[],"source":["def get_data(train_ds, valid_ds, bs, shuffle):\n","    return (\n","        DataLoader(train_ds, batch_size=bs, shuffle=shuffle),\n","        DataLoader(valid_ds, batch_size=bs),\n","    )"]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[],"source":["train_ds = ECG_Dataset(df_train)\n","test_ds = ECG_Dataset(df_test)\n","train_dl, test_dl = get_data(train_ds, test_ds, batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["Apply Boundary Conditions"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[],"source":["bc = DirichletBC()\n","for inputs, labels in train_dl:\n","    # Watch out here. This calculated assuming one batch size=dataset_len\n","        \n","    DirichletIndx, DirichletVals = bc(inputs,target_func, kappa)"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[],"source":["def batch_loss_train(outputs, labels, inputs, kappa, loss_fn, optimizer):\n","    loss = loss_fn(outputs, labels, inputs, kappa)\n","    \n","    loss.backward()\n","    \n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    return loss.item()"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["def batch_loss_test(outputs, labels, loss_fn):\n","    loss = loss_fn(outputs, labels)    \n","    return loss.item()"]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[],"source":["def test_loop(dataloader, model, loss_fn):\n","    # Set the model to evaluation mode - important for batch normalization and dropout layers\n","    model.eval()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, mse, mape = 0, 0, 0\n","    epsilon = 1e-8\n","\n","    # Evaluating the model with torch.no_grad()    \n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            #print(pred.view(lenX,lenX))\n","            # test_loss += loss_fn(pred, y, X,kappa) / len(X)\n","            mse += torch.sum((y - pred) ** 2).item()\n","            #y = y + epsilon \n","            mape += torch.mean(torch.abs((y - pred))) \n","\n","    print(f\"Test set => Mean Squared error: {(mse/size):>0.4f}, Mean Abs error: {mape:>8f} \\n\")"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[],"source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    model.train()\n","    optimizer.zero_grad()\n","    train_loss = 0.0\n","    #all_outputs = torch.empty(0, 1, requires_grad=True)\n","    \n","    for inputs, labels in dataloader:\n","        # forward pass. Better without shuffle to keep the coordinates sorted\n","        \n","        outputs = model(inputs)\n","        #print(outputs.view(lenX,lenX))\n","\n","        train_loss += batch_loss_train(outputs,labels,lenX,lenY,loss_fn, optimizer)\n","    \n","    print(f'Train loss: {train_loss}')\n","    "]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[],"source":["mse_loss = nn.MSELoss()"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[],"source":["class LaplaceLoss(nn.Module):\n","    def __init__(self):\n","        super(LaplaceLoss, self).__init__()\n","\n","    def forward(self, preds, targets, len_x, len_y):\n","        \"\"\"\n","        Compute the Calculate the Laplace eq. for predictions.\n","        labels (targets) should satisfy zero anyway\n","        \"\"\"\n","                            \n","        gradp_x = grad_x(preds,len_x)\n","        gradp_y = grad_y(preds,len_y)\n","\n","        grad_p_xx = grad_x(gradp_x,len_x)\n","        grad_p_yy = grad_y(gradp_y,len_y)\n","        laplacian = grad_p_xx + grad_p_yy \n","        PDELoss = mse_loss(laplacian, torch.zeros_like(laplacian))\n","\n","        # Dirichlet BC loss\n","        DBCpreds = preds[DirichletIndx.squeeze(1)].clone()\n","        DBCLoss = mse_loss(DBCpreds, DirichletVals)\n","\n","        loss = PDELoss.sum() + lambdaDBC * DBCLoss.sum()\n","        \n","        return loss\n"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[],"source":["def fit(epochs, model, loss_fn, opt, train_dl, valid_dl):\n","    for t in range(epochs):  \n","        print(f\"Epoch {t+1}   -------------------------------\")\n","        train_loop(train_dl, model, loss_fn, optimizer)\n","        test_loop(train_dl, model, loss_fn)\n"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[],"source":["# Define the model\n","torch.set_printoptions(precision=6)\n","\n","input_size = 2  # Number of input features\n","output_size = 1  # Output size (e.g., regression or binary classification)\n","model = SimpleANN(input_size, output_size)\n","\n","# Define loss and optimizer\n","criterion =  LaplaceLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1   -------------------------------\n","Train loss: 0.37538135051727295\n","Test set => Mean Squared error: 9.2040, Mean Abs error: 0.397591 \n","\n","Epoch 2   -------------------------------\n","Train loss: 0.3737735450267792\n","Test set => Mean Squared error: 9.1941, Mean Abs error: 0.396945 \n","\n","Epoch 3   -------------------------------\n","Train loss: 0.37214282155036926\n","Test set => Mean Squared error: 9.1843, Mean Abs error: 0.396279 \n","\n","Epoch 4   -------------------------------\n","Train loss: 0.3707449734210968\n","Test set => Mean Squared error: 9.1750, Mean Abs error: 0.395649 \n","\n","Epoch 5   -------------------------------\n","Train loss: 0.3695260286331177\n","Test set => Mean Squared error: 9.1662, Mean Abs error: 0.395048 \n","\n","Epoch 6   -------------------------------\n","Train loss: 0.3684138357639313\n","Test set => Mean Squared error: 9.1582, Mean Abs error: 0.394473 \n","\n","Epoch 7   -------------------------------\n","Train loss: 0.3672342002391815\n","Test set => Mean Squared error: 9.1509, Mean Abs error: 0.393906 \n","\n","Epoch 8   -------------------------------\n","Train loss: 0.36598438024520874\n","Test set => Mean Squared error: 9.1447, Mean Abs error: 0.393354 \n","\n","Epoch 9   -------------------------------\n","Train loss: 0.36478957533836365\n","Test set => Mean Squared error: 9.1393, Mean Abs error: 0.392811 \n","\n","Epoch 10   -------------------------------\n","Train loss: 0.36360979080200195\n","Test set => Mean Squared error: 9.1348, Mean Abs error: 0.392273 \n","\n","Epoch 11   -------------------------------\n","Train loss: 0.3626353144645691\n","Test set => Mean Squared error: 9.1312, Mean Abs error: 0.391798 \n","\n","Epoch 12   -------------------------------\n","Train loss: 0.36163264513015747\n","Test set => Mean Squared error: 9.1285, Mean Abs error: 0.391425 \n","\n","Epoch 13   -------------------------------\n","Train loss: 0.36050474643707275\n","Test set => Mean Squared error: 9.1266, Mean Abs error: 0.391181 \n","\n","Epoch 14   -------------------------------\n","Train loss: 0.3594377338886261\n","Test set => Mean Squared error: 9.1257, Mean Abs error: 0.391081 \n","\n","Epoch 15   -------------------------------\n","Train loss: 0.35826122760772705\n","Test set => Mean Squared error: 9.1256, Mean Abs error: 0.391060 \n","\n","Epoch 16   -------------------------------\n","Train loss: 0.3570329248905182\n","Test set => Mean Squared error: 9.1262, Mean Abs error: 0.391138 \n","\n","Epoch 17   -------------------------------\n","Train loss: 0.35562893748283386\n","Test set => Mean Squared error: 9.1273, Mean Abs error: 0.391250 \n","\n","Epoch 18   -------------------------------\n","Train loss: 0.35434243083000183\n","Test set => Mean Squared error: 9.1287, Mean Abs error: 0.391397 \n","\n","Epoch 19   -------------------------------\n","Train loss: 0.3529340922832489\n","Test set => Mean Squared error: 9.1304, Mean Abs error: 0.391567 \n","\n","Epoch 20   -------------------------------\n","Train loss: 0.35153526067733765\n","Test set => Mean Squared error: 9.1322, Mean Abs error: 0.391752 \n","\n","Epoch 21   -------------------------------\n","Train loss: 0.35031160712242126\n","Test set => Mean Squared error: 9.1342, Mean Abs error: 0.391941 \n","\n","Epoch 22   -------------------------------\n","Train loss: 0.34898149967193604\n","Test set => Mean Squared error: 9.1363, Mean Abs error: 0.392129 \n","\n","Epoch 23   -------------------------------\n","Train loss: 0.3475921154022217\n","Test set => Mean Squared error: 9.1385, Mean Abs error: 0.392318 \n","\n","Epoch 24   -------------------------------\n","Train loss: 0.3461153209209442\n","Test set => Mean Squared error: 9.1408, Mean Abs error: 0.392524 \n","\n","Epoch 25   -------------------------------\n","Train loss: 0.34447741508483887\n","Test set => Mean Squared error: 9.1434, Mean Abs error: 0.392743 \n","\n","Epoch 26   -------------------------------\n","Train loss: 0.34285295009613037\n","Test set => Mean Squared error: 9.1464, Mean Abs error: 0.392981 \n","\n","Epoch 27   -------------------------------\n","Train loss: 0.34119996428489685\n","Test set => Mean Squared error: 9.1498, Mean Abs error: 0.393242 \n","\n","Epoch 28   -------------------------------\n","Train loss: 0.33944836258888245\n","Test set => Mean Squared error: 9.1538, Mean Abs error: 0.393539 \n","\n","Epoch 29   -------------------------------\n","Train loss: 0.3375653922557831\n","Test set => Mean Squared error: 9.1584, Mean Abs error: 0.393867 \n","\n","Epoch 30   -------------------------------\n","Train loss: 0.3355996608734131\n","Test set => Mean Squared error: 9.1635, Mean Abs error: 0.394215 \n","\n","Epoch 31   -------------------------------\n","Train loss: 0.33363303542137146\n","Test set => Mean Squared error: 9.1691, Mean Abs error: 0.394571 \n","\n","Epoch 32   -------------------------------\n","Train loss: 0.3313048183917999\n","Test set => Mean Squared error: 9.1754, Mean Abs error: 0.394943 \n","\n","Epoch 33   -------------------------------\n","Train loss: 0.32914113998413086\n","Test set => Mean Squared error: 9.1818, Mean Abs error: 0.395311 \n","\n","Epoch 34   -------------------------------\n","Train loss: 0.3264787197113037\n","Test set => Mean Squared error: 9.1886, Mean Abs error: 0.395687 \n","\n","Epoch 35   -------------------------------\n","Train loss: 0.32397133111953735\n","Test set => Mean Squared error: 9.1958, Mean Abs error: 0.396065 \n","\n","Epoch 36   -------------------------------\n","Train loss: 0.32174766063690186\n","Test set => Mean Squared error: 9.2025, Mean Abs error: 0.396398 \n","\n","Epoch 37   -------------------------------\n","Train loss: 0.3195043206214905\n","Test set => Mean Squared error: 9.2106, Mean Abs error: 0.396786 \n","\n","Epoch 38   -------------------------------\n","Train loss: 0.3170379102230072\n","Test set => Mean Squared error: 9.2192, Mean Abs error: 0.397192 \n","\n","Epoch 39   -------------------------------\n","Train loss: 0.3150639533996582\n","Test set => Mean Squared error: 9.2276, Mean Abs error: 0.397576 \n","\n","Epoch 40   -------------------------------\n","Train loss: 0.3127598166465759\n","Test set => Mean Squared error: 9.2368, Mean Abs error: 0.397974 \n","\n","Epoch 41   -------------------------------\n","Train loss: 0.31006646156311035\n","Test set => Mean Squared error: 9.2465, Mean Abs error: 0.398382 \n","\n","Epoch 42   -------------------------------\n","Train loss: 0.3072909712791443\n","Test set => Mean Squared error: 9.2577, Mean Abs error: 0.398846 \n","\n","Epoch 43   -------------------------------\n","Train loss: 0.30428773164749146\n","Test set => Mean Squared error: 9.2702, Mean Abs error: 0.399346 \n","\n","Epoch 44   -------------------------------\n","Train loss: 0.30154696106910706\n","Test set => Mean Squared error: 9.2829, Mean Abs error: 0.399848 \n","\n","Epoch 45   -------------------------------\n","Train loss: 0.29846879839897156\n","Test set => Mean Squared error: 9.2981, Mean Abs error: 0.400425 \n","\n","Epoch 46   -------------------------------\n","Train loss: 0.29543536901474\n","Test set => Mean Squared error: 9.3150, Mean Abs error: 0.401056 \n","\n","Epoch 47   -------------------------------\n","Train loss: 0.29285603761672974\n","Test set => Mean Squared error: 9.3297, Mean Abs error: 0.401578 \n","\n","Epoch 48   -------------------------------\n","Train loss: 0.28967222571372986\n","Test set => Mean Squared error: 9.3466, Mean Abs error: 0.402169 \n","\n","Epoch 49   -------------------------------\n","Train loss: 0.28649017214775085\n","Test set => Mean Squared error: 9.3666, Mean Abs error: 0.402838 \n","\n","Epoch 50   -------------------------------\n","Train loss: 0.2833869755268097\n","Test set => Mean Squared error: 9.3885, Mean Abs error: 0.403566 \n","\n","Epoch 51   -------------------------------\n","Train loss: 0.2798462510108948\n","Test set => Mean Squared error: 9.4121, Mean Abs error: 0.404347 \n","\n","Epoch 52   -------------------------------\n","Train loss: 0.2764732539653778\n","Test set => Mean Squared error: 9.4385, Mean Abs error: 0.405190 \n","\n","Epoch 53   -------------------------------\n","Train loss: 0.272824764251709\n","Test set => Mean Squared error: 9.4667, Mean Abs error: 0.406072 \n","\n","Epoch 54   -------------------------------\n","Train loss: 0.27039796113967896\n","Test set => Mean Squared error: 9.4866, Mean Abs error: 0.406684 \n","\n","Epoch 55   -------------------------------\n","Train loss: 0.26673927903175354\n","Test set => Mean Squared error: 9.5069, Mean Abs error: 0.407292 \n","\n","Epoch 56   -------------------------------\n","Train loss: 0.2633831799030304\n","Test set => Mean Squared error: 9.5322, Mean Abs error: 0.408031 \n","\n","Epoch 57   -------------------------------\n","Train loss: 0.26051896810531616\n","Test set => Mean Squared error: 9.5630, Mean Abs error: 0.408942 \n","\n","Epoch 58   -------------------------------\n","Train loss: 0.25699302554130554\n","Test set => Mean Squared error: 9.5990, Mean Abs error: 0.409984 \n","\n","Epoch 59   -------------------------------\n","Train loss: 0.25381314754486084\n","Test set => Mean Squared error: 9.6357, Mean Abs error: 0.411028 \n","\n","Epoch 60   -------------------------------\n","Train loss: 0.250064879655838\n","Test set => Mean Squared error: 9.6717, Mean Abs error: 0.412046 \n","\n","Epoch 61   -------------------------------\n","Train loss: 0.2463880479335785\n","Test set => Mean Squared error: 9.7102, Mean Abs error: 0.413121 \n","\n","Epoch 62   -------------------------------\n","Train loss: 0.2429932951927185\n","Test set => Mean Squared error: 9.7521, Mean Abs error: 0.414260 \n","\n","Epoch 63   -------------------------------\n","Train loss: 0.2390856146812439\n","Test set => Mean Squared error: 9.7979, Mean Abs error: 0.415472 \n","\n","Epoch 64   -------------------------------\n","Train loss: 0.23546504974365234\n","Test set => Mean Squared error: 9.8429, Mean Abs error: 0.416638 \n","\n","Epoch 65   -------------------------------\n","Train loss: 0.231797456741333\n","Test set => Mean Squared error: 9.8889, Mean Abs error: 0.417815 \n","\n","Epoch 66   -------------------------------\n","Train loss: 0.22852835059165955\n","Test set => Mean Squared error: 9.9444, Mean Abs error: 0.419189 \n","\n","Epoch 67   -------------------------------\n","Train loss: 0.22476635873317719\n","Test set => Mean Squared error: 10.0021, Mean Abs error: 0.420655 \n","\n","Epoch 68   -------------------------------\n","Train loss: 0.22081026434898376\n","Test set => Mean Squared error: 10.0662, Mean Abs error: 0.422210 \n","\n","Epoch 69   -------------------------------\n","Train loss: 0.21675480902194977\n","Test set => Mean Squared error: 10.1306, Mean Abs error: 0.423792 \n","\n","Epoch 70   -------------------------------\n","Train loss: 0.21293848752975464\n","Test set => Mean Squared error: 10.1941, Mean Abs error: 0.425344 \n","\n","Epoch 71   -------------------------------\n","Train loss: 0.20957303047180176\n","Test set => Mean Squared error: 10.2602, Mean Abs error: 0.426876 \n","\n","Epoch 72   -------------------------------\n","Train loss: 0.20565885305404663\n","Test set => Mean Squared error: 10.3327, Mean Abs error: 0.428589 \n","\n","Epoch 73   -------------------------------\n","Train loss: 0.20147496461868286\n","Test set => Mean Squared error: 10.4089, Mean Abs error: 0.430403 \n","\n","Epoch 74   -------------------------------\n","Train loss: 0.1975374072790146\n","Test set => Mean Squared error: 10.4831, Mean Abs error: 0.432130 \n","\n","Epoch 75   -------------------------------\n","Train loss: 0.1940384805202484\n","Test set => Mean Squared error: 10.5585, Mean Abs error: 0.433867 \n","\n","Epoch 76   -------------------------------\n","Train loss: 0.19026093184947968\n","Test set => Mean Squared error: 10.6472, Mean Abs error: 0.435887 \n","\n","Epoch 77   -------------------------------\n","Train loss: 0.18615369498729706\n","Test set => Mean Squared error: 10.7414, Mean Abs error: 0.438096 \n","\n","Epoch 78   -------------------------------\n","Train loss: 0.18294553458690643\n","Test set => Mean Squared error: 10.8359, Mean Abs error: 0.440255 \n","\n","Epoch 79   -------------------------------\n","Train loss: 0.17917676270008087\n","Test set => Mean Squared error: 10.9329, Mean Abs error: 0.442419 \n","\n","Epoch 80   -------------------------------\n","Train loss: 0.17526958882808685\n","Test set => Mean Squared error: 11.0352, Mean Abs error: 0.444692 \n","\n","Epoch 81   -------------------------------\n","Train loss: 0.17211796343326569\n","Test set => Mean Squared error: 11.1433, Mean Abs error: 0.447086 \n","\n","Epoch 82   -------------------------------\n","Train loss: 0.1685067117214203\n","Test set => Mean Squared error: 11.2485, Mean Abs error: 0.449366 \n","\n","Epoch 83   -------------------------------\n","Train loss: 0.16492347419261932\n","Test set => Mean Squared error: 11.3714, Mean Abs error: 0.451946 \n","\n","Epoch 84   -------------------------------\n","Train loss: 0.1628284454345703\n","Test set => Mean Squared error: 11.4698, Mean Abs error: 0.454100 \n","\n","Epoch 85   -------------------------------\n","Train loss: 0.161738321185112\n","Test set => Mean Squared error: 11.5843, Mean Abs error: 0.456470 \n","\n","Epoch 86   -------------------------------\n","Train loss: 0.15605051815509796\n","Test set => Mean Squared error: 11.6961, Mean Abs error: 0.458741 \n","\n","Epoch 87   -------------------------------\n","Train loss: 0.15722520649433136\n","Test set => Mean Squared error: 11.7900, Mean Abs error: 0.460760 \n","\n","Epoch 88   -------------------------------\n","Train loss: 0.15229550004005432\n","Test set => Mean Squared error: 11.8926, Mean Abs error: 0.462854 \n","\n","Epoch 89   -------------------------------\n","Train loss: 0.1502661406993866\n","Test set => Mean Squared error: 12.0052, Mean Abs error: 0.465036 \n","\n","Epoch 90   -------------------------------\n","Train loss: 0.147278293967247\n","Test set => Mean Squared error: 12.1130, Mean Abs error: 0.467183 \n","\n","Epoch 91   -------------------------------\n","Train loss: 0.14405372738838196\n","Test set => Mean Squared error: 12.2192, Mean Abs error: 0.469328 \n","\n","Epoch 92   -------------------------------\n","Train loss: 0.14244000613689423\n","Test set => Mean Squared error: 12.3183, Mean Abs error: 0.471246 \n","\n","Epoch 93   -------------------------------\n","Train loss: 0.1388147622346878\n","Test set => Mean Squared error: 12.4297, Mean Abs error: 0.473320 \n","\n","Epoch 94   -------------------------------\n","Train loss: 0.13670523464679718\n","Test set => Mean Squared error: 12.5457, Mean Abs error: 0.475591 \n","\n","Epoch 95   -------------------------------\n","Train loss: 0.13443274796009064\n","Test set => Mean Squared error: 12.6502, Mean Abs error: 0.477625 \n","\n","Epoch 96   -------------------------------\n","Train loss: 0.13126906752586365\n","Test set => Mean Squared error: 12.7587, Mean Abs error: 0.479764 \n","\n","Epoch 97   -------------------------------\n","Train loss: 0.12989498674869537\n","Test set => Mean Squared error: 12.8915, Mean Abs error: 0.482341 \n","\n","Epoch 98   -------------------------------\n","Train loss: 0.12741808593273163\n","Test set => Mean Squared error: 13.0106, Mean Abs error: 0.484575 \n","\n","Epoch 99   -------------------------------\n","Train loss: 0.12779009342193604\n","Test set => Mean Squared error: 13.1362, Mean Abs error: 0.487020 \n","\n","Epoch 100   -------------------------------\n","Train loss: 0.1289648413658142\n","Test set => Mean Squared error: 13.2168, Mean Abs error: 0.488540 \n","\n","Epoch 101   -------------------------------\n","Train loss: 0.13386669754981995\n","Test set => Mean Squared error: 13.2954, Mean Abs error: 0.489971 \n","\n","Epoch 102   -------------------------------\n","Train loss: 0.12577295303344727\n","Test set => Mean Squared error: 13.3934, Mean Abs error: 0.491842 \n","\n","Epoch 103   -------------------------------\n","Train loss: 0.12130635231733322\n","Test set => Mean Squared error: 13.4897, Mean Abs error: 0.493706 \n","\n","Epoch 104   -------------------------------\n","Train loss: 0.12419632077217102\n","Test set => Mean Squared error: 13.5617, Mean Abs error: 0.495049 \n","\n","Epoch 105   -------------------------------\n","Train loss: 0.11783988773822784\n","Test set => Mean Squared error: 13.6265, Mean Abs error: 0.496254 \n","\n","Epoch 106   -------------------------------\n","Train loss: 0.12181787192821503\n","Test set => Mean Squared error: 13.7073, Mean Abs error: 0.497783 \n","\n","Epoch 107   -------------------------------\n","Train loss: 0.11559311300516129\n","Test set => Mean Squared error: 13.7907, Mean Abs error: 0.499346 \n","\n","Epoch 108   -------------------------------\n","Train loss: 0.11750990152359009\n","Test set => Mean Squared error: 13.8601, Mean Abs error: 0.500602 \n","\n","Epoch 109   -------------------------------\n","Train loss: 0.11278409510850906\n","Test set => Mean Squared error: 13.9341, Mean Abs error: 0.501941 \n","\n","Epoch 110   -------------------------------\n","Train loss: 0.11326615512371063\n","Test set => Mean Squared error: 14.0239, Mean Abs error: 0.503596 \n","\n","Epoch 111   -------------------------------\n","Train loss: 0.11027266830205917\n","Test set => Mean Squared error: 14.1173, Mean Abs error: 0.505318 \n","\n","Epoch 112   -------------------------------\n","Train loss: 0.11052144318819046\n","Test set => Mean Squared error: 14.1885, Mean Abs error: 0.506597 \n","\n","Epoch 113   -------------------------------\n","Train loss: 0.10848274827003479\n","Test set => Mean Squared error: 14.2463, Mean Abs error: 0.507616 \n","\n","Epoch 114   -------------------------------\n","Train loss: 0.10848425328731537\n","Test set => Mean Squared error: 14.3164, Mean Abs error: 0.508867 \n","\n","Epoch 115   -------------------------------\n","Train loss: 0.10673746466636658\n","Test set => Mean Squared error: 14.4106, Mean Abs error: 0.510576 \n","\n","Epoch 116   -------------------------------\n","Train loss: 0.10608819127082825\n","Test set => Mean Squared error: 14.4775, Mean Abs error: 0.511754 \n","\n","Epoch 117   -------------------------------\n","Train loss: 0.10479836910963058\n","Test set => Mean Squared error: 14.5192, Mean Abs error: 0.512444 \n","\n","Epoch 118   -------------------------------\n","Train loss: 0.10411201417446136\n","Test set => Mean Squared error: 14.5824, Mean Abs error: 0.513534 \n","\n","Epoch 119   -------------------------------\n","Train loss: 0.10339637100696564\n","Test set => Mean Squared error: 14.6566, Mean Abs error: 0.514858 \n","\n","Epoch 120   -------------------------------\n","Train loss: 0.10212284326553345\n","Test set => Mean Squared error: 14.7290, Mean Abs error: 0.516116 \n","\n","Epoch 121   -------------------------------\n","Train loss: 0.1011369600892067\n","Test set => Mean Squared error: 14.7740, Mean Abs error: 0.516861 \n","\n","Epoch 122   -------------------------------\n","Train loss: 0.10074874013662338\n","Test set => Mean Squared error: 14.8267, Mean Abs error: 0.517767 \n","\n","Epoch 123   -------------------------------\n","Train loss: 0.09935178607702255\n","Test set => Mean Squared error: 14.8759, Mean Abs error: 0.518604 \n","\n","Epoch 124   -------------------------------\n","Train loss: 0.09908226877450943\n","Test set => Mean Squared error: 14.9146, Mean Abs error: 0.519221 \n","\n","Epoch 125   -------------------------------\n","Train loss: 0.09850893914699554\n","Test set => Mean Squared error: 14.9652, Mean Abs error: 0.520064 \n","\n","Epoch 126   -------------------------------\n","Train loss: 0.09774486720561981\n","Test set => Mean Squared error: 15.0226, Mean Abs error: 0.521008 \n","\n","Epoch 127   -------------------------------\n","Train loss: 0.09706372767686844\n","Test set => Mean Squared error: 15.0693, Mean Abs error: 0.521783 \n","\n","Epoch 128   -------------------------------\n","Train loss: 0.0963582769036293\n","Test set => Mean Squared error: 15.1086, Mean Abs error: 0.522423 \n","\n","Epoch 129   -------------------------------\n","Train loss: 0.09590035676956177\n","Test set => Mean Squared error: 15.1501, Mean Abs error: 0.523089 \n","\n","Epoch 130   -------------------------------\n","Train loss: 0.09561394900083542\n","Test set => Mean Squared error: 15.1841, Mean Abs error: 0.523640 \n","\n","Epoch 131   -------------------------------\n","Train loss: 0.09511901438236237\n","Test set => Mean Squared error: 15.2150, Mean Abs error: 0.524113 \n","\n","Epoch 132   -------------------------------\n","Train loss: 0.09455857425928116\n","Test set => Mean Squared error: 15.2270, Mean Abs error: 0.524280 \n","\n","Epoch 133   -------------------------------\n","Train loss: 0.09448022395372391\n","Test set => Mean Squared error: 15.2857, Mean Abs error: 0.525239 \n","\n","Epoch 134   -------------------------------\n","Train loss: 0.09347700327634811\n","Test set => Mean Squared error: 15.3210, Mean Abs error: 0.525813 \n","\n","Epoch 135   -------------------------------\n","Train loss: 0.09256072342395782\n","Test set => Mean Squared error: 15.3525, Mean Abs error: 0.526294 \n","\n","Epoch 136   -------------------------------\n","Train loss: 0.09239359945058823\n","Test set => Mean Squared error: 15.3907, Mean Abs error: 0.526888 \n","\n","Epoch 137   -------------------------------\n","Train loss: 0.09177999198436737\n","Test set => Mean Squared error: 15.4030, Mean Abs error: 0.527053 \n","\n","Epoch 138   -------------------------------\n","Train loss: 0.09142829477787018\n","Test set => Mean Squared error: 15.4311, Mean Abs error: 0.527472 \n","\n","Epoch 139   -------------------------------\n","Train loss: 0.09082451462745667\n","Test set => Mean Squared error: 15.4647, Mean Abs error: 0.527994 \n","\n","Epoch 140   -------------------------------\n","Train loss: 0.0903448686003685\n","Test set => Mean Squared error: 15.4832, Mean Abs error: 0.528242 \n","\n","Epoch 141   -------------------------------\n","Train loss: 0.09013166278600693\n","Test set => Mean Squared error: 15.5094, Mean Abs error: 0.528615 \n","\n","Epoch 142   -------------------------------\n","Train loss: 0.08947643637657166\n","Test set => Mean Squared error: 15.5247, Mean Abs error: 0.528799 \n","\n","Epoch 143   -------------------------------\n","Train loss: 0.08900181949138641\n","Test set => Mean Squared error: 15.5378, Mean Abs error: 0.528981 \n","\n","Epoch 144   -------------------------------\n","Train loss: 0.0887206643819809\n","Test set => Mean Squared error: 15.5572, Mean Abs error: 0.529280 \n","\n","Epoch 145   -------------------------------\n","Train loss: 0.0882367491722107\n","Test set => Mean Squared error: 15.5626, Mean Abs error: 0.529354 \n","\n","Epoch 146   -------------------------------\n","Train loss: 0.08799747377634048\n","Test set => Mean Squared error: 15.5850, Mean Abs error: 0.529677 \n","\n","Epoch 147   -------------------------------\n","Train loss: 0.08761882781982422\n","Test set => Mean Squared error: 15.6084, Mean Abs error: 0.530026 \n","\n","Epoch 148   -------------------------------\n","Train loss: 0.08752131462097168\n","Test set => Mean Squared error: 15.6003, Mean Abs error: 0.529903 \n","\n","Epoch 149   -------------------------------\n","Train loss: 0.08864376693964005\n","Test set => Mean Squared error: 15.6524, Mean Abs error: 0.530623 \n","\n","Epoch 150   -------------------------------\n","Train loss: 0.08893926441669464\n","Test set => Mean Squared error: 15.6258, Mean Abs error: 0.530272 \n","\n","Epoch 151   -------------------------------\n","Train loss: 0.0869082659482956\n","Test set => Mean Squared error: 15.6082, Mean Abs error: 0.529983 \n","\n","Epoch 152   -------------------------------\n","Train loss: 0.08671725541353226\n","Test set => Mean Squared error: 15.6154, Mean Abs error: 0.530111 \n","\n","Epoch 153   -------------------------------\n","Train loss: 0.08655637502670288\n","Test set => Mean Squared error: 15.6017, Mean Abs error: 0.529887 \n","\n","Epoch 154   -------------------------------\n","Train loss: 0.08683671057224274\n","Test set => Mean Squared error: 15.6238, Mean Abs error: 0.530182 \n","\n","Epoch 155   -------------------------------\n","Train loss: 0.091126449406147\n","Test set => Mean Squared error: 15.5646, Mean Abs error: 0.529196 \n","\n","Epoch 156   -------------------------------\n","Train loss: 0.10888246446847916\n","Test set => Mean Squared error: 15.5633, Mean Abs error: 0.528778 \n","\n","Epoch 157   -------------------------------\n","Train loss: 0.09257978200912476\n","Test set => Mean Squared error: 15.6106, Mean Abs error: 0.529437 \n","\n","Epoch 158   -------------------------------\n","Train loss: 0.10885768383741379\n","Test set => Mean Squared error: 15.5735, Mean Abs error: 0.528857 \n","\n","Epoch 159   -------------------------------\n","Train loss: 0.09387831389904022\n","Test set => Mean Squared error: 15.5177, Mean Abs error: 0.527896 \n","\n","Epoch 160   -------------------------------\n","Train loss: 0.0977705717086792\n","Test set => Mean Squared error: 15.4998, Mean Abs error: 0.527584 \n","\n","Epoch 161   -------------------------------\n","Train loss: 0.09830715507268906\n","Test set => Mean Squared error: 15.5171, Mean Abs error: 0.527799 \n","\n","Epoch 162   -------------------------------\n","Train loss: 0.09375947713851929\n","Test set => Mean Squared error: 15.5185, Mean Abs error: 0.527671 \n","\n","Epoch 163   -------------------------------\n","Train loss: 0.09379750490188599\n","Test set => Mean Squared error: 15.4964, Mean Abs error: 0.527254 \n","\n","Epoch 164   -------------------------------\n","Train loss: 0.09512436389923096\n","Test set => Mean Squared error: 15.4796, Mean Abs error: 0.527071 \n","\n","Epoch 165   -------------------------------\n","Train loss: 0.0910656675696373\n","Test set => Mean Squared error: 15.4858, Mean Abs error: 0.527241 \n","\n","Epoch 166   -------------------------------\n","Train loss: 0.09127592295408249\n","Test set => Mean Squared error: 15.4891, Mean Abs error: 0.527292 \n","\n","Epoch 167   -------------------------------\n","Train loss: 0.09212243556976318\n","Test set => Mean Squared error: 15.4773, Mean Abs error: 0.527118 \n","\n","Epoch 168   -------------------------------\n","Train loss: 0.0894668847322464\n","Test set => Mean Squared error: 15.4672, Mean Abs error: 0.526942 \n","\n","Epoch 169   -------------------------------\n","Train loss: 0.09111704677343369\n","Test set => Mean Squared error: 15.4649, Mean Abs error: 0.526879 \n","\n","Epoch 170   -------------------------------\n","Train loss: 0.08817879110574722\n","Test set => Mean Squared error: 15.4849, Mean Abs error: 0.527219 \n","\n","Epoch 171   -------------------------------\n","Train loss: 0.08801847696304321\n","Test set => Mean Squared error: 15.5162, Mean Abs error: 0.527793 \n","\n","Epoch 172   -------------------------------\n","Train loss: 0.0871567353606224\n","Test set => Mean Squared error: 15.5504, Mean Abs error: 0.528378 \n","\n","Epoch 173   -------------------------------\n","Train loss: 0.08650601655244827\n","Test set => Mean Squared error: 15.5861, Mean Abs error: 0.528954 \n","\n","Epoch 174   -------------------------------\n","Train loss: 0.08597873151302338\n","Test set => Mean Squared error: 15.6185, Mean Abs error: 0.529481 \n","\n","Epoch 175   -------------------------------\n","Train loss: 0.08573846518993378\n","Test set => Mean Squared error: 15.6475, Mean Abs error: 0.529954 \n","\n","Epoch 176   -------------------------------\n","Train loss: 0.08511332422494888\n","Test set => Mean Squared error: 15.6746, Mean Abs error: 0.530373 \n","\n","Epoch 177   -------------------------------\n","Train loss: 0.08468758314847946\n","Test set => Mean Squared error: 15.6779, Mean Abs error: 0.530402 \n","\n","Epoch 178   -------------------------------\n","Train loss: 0.08472461253404617\n","Test set => Mean Squared error: 15.6569, Mean Abs error: 0.530041 \n","\n","Epoch 179   -------------------------------\n","Train loss: 0.08433418720960617\n","Test set => Mean Squared error: 15.6441, Mean Abs error: 0.529843 \n","\n","Epoch 180   -------------------------------\n","Train loss: 0.08366259932518005\n","Test set => Mean Squared error: 15.6561, Mean Abs error: 0.530051 \n","\n","Epoch 181   -------------------------------\n","Train loss: 0.08348757028579712\n","Test set => Mean Squared error: 15.6596, Mean Abs error: 0.530112 \n","\n","Epoch 182   -------------------------------\n","Train loss: 0.08323745429515839\n","Test set => Mean Squared error: 15.6515, Mean Abs error: 0.529933 \n","\n","Epoch 183   -------------------------------\n","Train loss: 0.08261610567569733\n","Test set => Mean Squared error: 15.6488, Mean Abs error: 0.529845 \n","\n","Epoch 184   -------------------------------\n","Train loss: 0.08250493556261063\n","Test set => Mean Squared error: 15.6525, Mean Abs error: 0.529878 \n","\n","Epoch 185   -------------------------------\n","Train loss: 0.0827292650938034\n","Test set => Mean Squared error: 15.6733, Mean Abs error: 0.530167 \n","\n","Epoch 186   -------------------------------\n","Train loss: 0.08270344883203506\n","Test set => Mean Squared error: 15.6803, Mean Abs error: 0.530250 \n","\n","Epoch 187   -------------------------------\n","Train loss: 0.08297416567802429\n","Test set => Mean Squared error: 15.7183, Mean Abs error: 0.530819 \n","\n","Epoch 188   -------------------------------\n","Train loss: 0.08339934051036835\n","Test set => Mean Squared error: 15.7109, Mean Abs error: 0.530711 \n","\n","Epoch 189   -------------------------------\n","Train loss: 0.08697682619094849\n","Test set => Mean Squared error: 15.7670, Mean Abs error: 0.531481 \n","\n","Epoch 190   -------------------------------\n","Train loss: 0.08439682424068451\n","Test set => Mean Squared error: 15.7751, Mean Abs error: 0.531609 \n","\n","Epoch 191   -------------------------------\n","Train loss: 0.08239442855119705\n","Test set => Mean Squared error: 15.7820, Mean Abs error: 0.531726 \n","\n","Epoch 192   -------------------------------\n","Train loss: 0.08414904773235321\n","Test set => Mean Squared error: 15.8236, Mean Abs error: 0.532316 \n","\n","Epoch 193   -------------------------------\n","Train loss: 0.08320312201976776\n","Test set => Mean Squared error: 15.8279, Mean Abs error: 0.532383 \n","\n","Epoch 194   -------------------------------\n","Train loss: 0.08189398795366287\n","Test set => Mean Squared error: 15.8309, Mean Abs error: 0.532452 \n","\n","Epoch 195   -------------------------------\n","Train loss: 0.0824294164776802\n","Test set => Mean Squared error: 15.8559, Mean Abs error: 0.532819 \n","\n","Epoch 196   -------------------------------\n","Train loss: 0.08187957108020782\n","Test set => Mean Squared error: 15.8737, Mean Abs error: 0.533105 \n","\n","Epoch 197   -------------------------------\n","Train loss: 0.08190365135669708\n","Test set => Mean Squared error: 15.8810, Mean Abs error: 0.533276 \n","\n","Epoch 198   -------------------------------\n","Train loss: 0.08276422321796417\n","Test set => Mean Squared error: 15.9274, Mean Abs error: 0.533992 \n","\n","Epoch 199   -------------------------------\n","Train loss: 0.08235891163349152\n","Test set => Mean Squared error: 15.9267, Mean Abs error: 0.534003 \n","\n","Epoch 200   -------------------------------\n","Train loss: 0.08129670470952988\n","Test set => Mean Squared error: 15.9451, Mean Abs error: 0.534283 \n","\n","Epoch 201   -------------------------------\n","Train loss: 0.08078279346227646\n","Test set => Mean Squared error: 15.9758, Mean Abs error: 0.534758 \n","\n","Epoch 202   -------------------------------\n","Train loss: 0.0816575214266777\n","Test set => Mean Squared error: 15.9619, Mean Abs error: 0.534566 \n","\n","Epoch 203   -------------------------------\n","Train loss: 0.08413909375667572\n","Test set => Mean Squared error: 16.0023, Mean Abs error: 0.535135 \n","\n","Epoch 204   -------------------------------\n","Train loss: 0.08090659230947495\n","Test set => Mean Squared error: 16.0281, Mean Abs error: 0.535538 \n","\n","Epoch 205   -------------------------------\n","Train loss: 0.08410560339689255\n","Test set => Mean Squared error: 15.9964, Mean Abs error: 0.535124 \n","\n","Epoch 206   -------------------------------\n","Train loss: 0.08534214645624161\n","Test set => Mean Squared error: 16.0272, Mean Abs error: 0.535585 \n","\n","Epoch 207   -------------------------------\n","Train loss: 0.08344835042953491\n","Test set => Mean Squared error: 16.0778, Mean Abs error: 0.536371 \n","\n","Epoch 208   -------------------------------\n","Train loss: 0.08596747368574142\n","Test set => Mean Squared error: 16.0379, Mean Abs error: 0.535722 \n","\n","Epoch 209   -------------------------------\n","Train loss: 0.08215620368719101\n","Test set => Mean Squared error: 16.0018, Mean Abs error: 0.535157 \n","\n","Epoch 210   -------------------------------\n","Train loss: 0.08351674675941467\n","Test set => Mean Squared error: 16.0195, Mean Abs error: 0.535389 \n","\n","Epoch 211   -------------------------------\n","Train loss: 0.08288992196321487\n","Test set => Mean Squared error: 16.0333, Mean Abs error: 0.535647 \n","\n","Epoch 212   -------------------------------\n","Train loss: 0.08233300596475601\n","Test set => Mean Squared error: 16.0329, Mean Abs error: 0.535661 \n","\n","Epoch 213   -------------------------------\n","Train loss: 0.08200541883707047\n","Test set => Mean Squared error: 16.0325, Mean Abs error: 0.535635 \n","\n","Epoch 214   -------------------------------\n","Train loss: 0.08238159120082855\n","Test set => Mean Squared error: 16.0348, Mean Abs error: 0.535690 \n","\n","Epoch 215   -------------------------------\n","Train loss: 0.08171719312667847\n","Test set => Mean Squared error: 16.0699, Mean Abs error: 0.536226 \n","\n","Epoch 216   -------------------------------\n","Train loss: 0.0806197002530098\n","Test set => Mean Squared error: 16.0910, Mean Abs error: 0.536536 \n","\n","Epoch 217   -------------------------------\n","Train loss: 0.08053713291883469\n","Test set => Mean Squared error: 16.0834, Mean Abs error: 0.536417 \n","\n","Epoch 218   -------------------------------\n","Train loss: 0.08083412796258926\n","Test set => Mean Squared error: 16.0944, Mean Abs error: 0.536552 \n","\n","Epoch 219   -------------------------------\n","Train loss: 0.07998179644346237\n","Test set => Mean Squared error: 16.0967, Mean Abs error: 0.536560 \n","\n","Epoch 220   -------------------------------\n","Train loss: 0.08137776702642441\n","Test set => Mean Squared error: 16.0489, Mean Abs error: 0.535808 \n","\n","Epoch 221   -------------------------------\n","Train loss: 0.08502718806266785\n","Test set => Mean Squared error: 16.0659, Mean Abs error: 0.536003 \n","\n","Epoch 222   -------------------------------\n","Train loss: 0.08130401372909546\n","Test set => Mean Squared error: 16.0955, Mean Abs error: 0.536457 \n","\n","Epoch 223   -------------------------------\n","Train loss: 0.08512351661920547\n","Test set => Mean Squared error: 16.0556, Mean Abs error: 0.535838 \n","\n","Epoch 224   -------------------------------\n","Train loss: 0.08180678635835648\n","Test set => Mean Squared error: 16.0276, Mean Abs error: 0.535382 \n","\n","Epoch 225   -------------------------------\n","Train loss: 0.08355792611837387\n","Test set => Mean Squared error: 16.0564, Mean Abs error: 0.535819 \n","\n","Epoch 226   -------------------------------\n","Train loss: 0.08145780861377716\n","Test set => Mean Squared error: 16.0744, Mean Abs error: 0.536079 \n","\n","Epoch 227   -------------------------------\n","Train loss: 0.08220754563808441\n","Test set => Mean Squared error: 16.0476, Mean Abs error: 0.535620 \n","\n","Epoch 228   -------------------------------\n","Train loss: 0.08084017038345337\n","Test set => Mean Squared error: 16.0305, Mean Abs error: 0.535335 \n","\n","Epoch 229   -------------------------------\n","Train loss: 0.08155487477779388\n","Test set => Mean Squared error: 16.0607, Mean Abs error: 0.535825 \n","\n","Epoch 230   -------------------------------\n","Train loss: 0.08033709228038788\n","Test set => Mean Squared error: 16.1035, Mean Abs error: 0.536560 \n","\n","Epoch 231   -------------------------------\n","Train loss: 0.08024884760379791\n","Test set => Mean Squared error: 16.1122, Mean Abs error: 0.536715 \n","\n","Epoch 232   -------------------------------\n","Train loss: 0.08057083934545517\n","Test set => Mean Squared error: 16.1059, Mean Abs error: 0.536568 \n","\n","Epoch 233   -------------------------------\n","Train loss: 0.07982341945171356\n","Test set => Mean Squared error: 16.1009, Mean Abs error: 0.536449 \n","\n","Epoch 234   -------------------------------\n","Train loss: 0.07992248237133026\n","Test set => Mean Squared error: 16.0864, Mean Abs error: 0.536231 \n","\n","Epoch 235   -------------------------------\n","Train loss: 0.08015764504671097\n","Test set => Mean Squared error: 16.0868, Mean Abs error: 0.536211 \n","\n","Epoch 236   -------------------------------\n","Train loss: 0.079602912068367\n","Test set => Mean Squared error: 16.0742, Mean Abs error: 0.536035 \n","\n","Epoch 237   -------------------------------\n","Train loss: 0.07952995598316193\n","Test set => Mean Squared error: 16.0793, Mean Abs error: 0.536118 \n","\n","Epoch 238   -------------------------------\n","Train loss: 0.07939332723617554\n","Test set => Mean Squared error: 16.0823, Mean Abs error: 0.536139 \n","\n","Epoch 239   -------------------------------\n","Train loss: 0.07914776355028152\n","Test set => Mean Squared error: 16.0612, Mean Abs error: 0.535804 \n","\n","Epoch 240   -------------------------------\n","Train loss: 0.07933950424194336\n","Test set => Mean Squared error: 16.0780, Mean Abs error: 0.536033 \n","\n","Epoch 241   -------------------------------\n","Train loss: 0.07937820255756378\n","Test set => Mean Squared error: 16.0761, Mean Abs error: 0.536021 \n","\n","Epoch 242   -------------------------------\n","Train loss: 0.07910363376140594\n","Test set => Mean Squared error: 16.0831, Mean Abs error: 0.536120 \n","\n","Epoch 243   -------------------------------\n","Train loss: 0.07881252467632294\n","Test set => Mean Squared error: 16.0872, Mean Abs error: 0.536180 \n","\n","Epoch 244   -------------------------------\n","Train loss: 0.0787326917052269\n","Test set => Mean Squared error: 16.0895, Mean Abs error: 0.536231 \n","\n","Epoch 245   -------------------------------\n","Train loss: 0.0786692425608635\n","Test set => Mean Squared error: 16.1043, Mean Abs error: 0.536443 \n","\n","Epoch 246   -------------------------------\n","Train loss: 0.07888082414865494\n","Test set => Mean Squared error: 16.0947, Mean Abs error: 0.536303 \n","\n","Epoch 247   -------------------------------\n","Train loss: 0.08021717518568039\n","Test set => Mean Squared error: 16.1409, Mean Abs error: 0.536993 \n","\n","Epoch 248   -------------------------------\n","Train loss: 0.08258386701345444\n","Test set => Mean Squared error: 16.1109, Mean Abs error: 0.536589 \n","\n","Epoch 249   -------------------------------\n","Train loss: 0.08721887320280075\n","Test set => Mean Squared error: 16.1565, Mean Abs error: 0.537335 \n","\n","Epoch 250   -------------------------------\n","Train loss: 0.08150290697813034\n","Test set => Mean Squared error: 16.1793, Mean Abs error: 0.537662 \n","\n","Epoch 251   -------------------------------\n","Train loss: 0.08797409385442734\n","Test set => Mean Squared error: 16.1266, Mean Abs error: 0.536810 \n","\n","Epoch 252   -------------------------------\n","Train loss: 0.08160492032766342\n","Test set => Mean Squared error: 16.0854, Mean Abs error: 0.536115 \n","\n","Epoch 253   -------------------------------\n","Train loss: 0.08389037847518921\n","Test set => Mean Squared error: 16.0855, Mean Abs error: 0.536079 \n","\n","Epoch 254   -------------------------------\n","Train loss: 0.08278263360261917\n","Test set => Mean Squared error: 16.0796, Mean Abs error: 0.535975 \n","\n","Epoch 255   -------------------------------\n","Train loss: 0.08297480642795563\n","Test set => Mean Squared error: 16.0421, Mean Abs error: 0.535344 \n","\n","Epoch 256   -------------------------------\n","Train loss: 0.08236224949359894\n","Test set => Mean Squared error: 16.0041, Mean Abs error: 0.534715 \n","\n","Epoch 257   -------------------------------\n","Train loss: 0.08257605135440826\n","Test set => Mean Squared error: 16.0319, Mean Abs error: 0.535170 \n","\n","Epoch 258   -------------------------------\n","Train loss: 0.08158843219280243\n","Test set => Mean Squared error: 16.0479, Mean Abs error: 0.535407 \n","\n","Epoch 259   -------------------------------\n","Train loss: 0.08132929354906082\n","Test set => Mean Squared error: 16.0254, Mean Abs error: 0.535017 \n","\n","Epoch 260   -------------------------------\n","Train loss: 0.08041872084140778\n","Test set => Mean Squared error: 16.0099, Mean Abs error: 0.534760 \n","\n","Epoch 261   -------------------------------\n","Train loss: 0.0804612934589386\n","Test set => Mean Squared error: 16.0318, Mean Abs error: 0.535123 \n","\n","Epoch 262   -------------------------------\n","Train loss: 0.07987061142921448\n","Test set => Mean Squared error: 16.0324, Mean Abs error: 0.535096 \n","\n","Epoch 263   -------------------------------\n","Train loss: 0.07987063378095627\n","Test set => Mean Squared error: 16.0314, Mean Abs error: 0.535102 \n","\n","Epoch 264   -------------------------------\n","Train loss: 0.07957783341407776\n","Test set => Mean Squared error: 16.0536, Mean Abs error: 0.535490 \n","\n","Epoch 265   -------------------------------\n","Train loss: 0.07971546798944473\n","Test set => Mean Squared error: 16.0544, Mean Abs error: 0.535444 \n","\n","Epoch 266   -------------------------------\n","Train loss: 0.07982099056243896\n","Test set => Mean Squared error: 16.0296, Mean Abs error: 0.535065 \n","\n","Epoch 267   -------------------------------\n","Train loss: 0.08039292693138123\n","Test set => Mean Squared error: 16.0427, Mean Abs error: 0.535284 \n","\n","Epoch 268   -------------------------------\n","Train loss: 0.07952506095170975\n","Test set => Mean Squared error: 16.0257, Mean Abs error: 0.534986 \n","\n","Epoch 269   -------------------------------\n","Train loss: 0.08002118766307831\n","Test set => Mean Squared error: 15.9920, Mean Abs error: 0.534466 \n","\n","Epoch 270   -------------------------------\n","Train loss: 0.08046772330999374\n","Test set => Mean Squared error: 16.0149, Mean Abs error: 0.534831 \n","\n","Epoch 271   -------------------------------\n","Train loss: 0.07936371862888336\n","Test set => Mean Squared error: 16.0348, Mean Abs error: 0.535126 \n","\n","Epoch 272   -------------------------------\n","Train loss: 0.08005043864250183\n","Test set => Mean Squared error: 16.0165, Mean Abs error: 0.534824 \n","\n","Epoch 273   -------------------------------\n","Train loss: 0.07907959073781967\n","Test set => Mean Squared error: 16.0041, Mean Abs error: 0.534631 \n","\n","Epoch 274   -------------------------------\n","Train loss: 0.07957730442285538\n","Test set => Mean Squared error: 16.0175, Mean Abs error: 0.534829 \n","\n","Epoch 275   -------------------------------\n","Train loss: 0.07881644368171692\n","Test set => Mean Squared error: 16.0191, Mean Abs error: 0.534844 \n","\n","Epoch 276   -------------------------------\n","Train loss: 0.07938358187675476\n","Test set => Mean Squared error: 16.0060, Mean Abs error: 0.534654 \n","\n","Epoch 277   -------------------------------\n","Train loss: 0.07894247025251389\n","Test set => Mean Squared error: 16.0292, Mean Abs error: 0.535042 \n","\n","Epoch 278   -------------------------------\n","Train loss: 0.0789123922586441\n","Test set => Mean Squared error: 16.0697, Mean Abs error: 0.535680 \n","\n","Epoch 279   -------------------------------\n","Train loss: 0.07914736121892929\n","Test set => Mean Squared error: 16.0671, Mean Abs error: 0.535623 \n","\n","Epoch 280   -------------------------------\n","Train loss: 0.07811649143695831\n","Test set => Mean Squared error: 16.0550, Mean Abs error: 0.535429 \n","\n","Epoch 281   -------------------------------\n","Train loss: 0.07846944034099579\n","Test set => Mean Squared error: 16.0737, Mean Abs error: 0.535716 \n","\n","Epoch 282   -------------------------------\n","Train loss: 0.077860027551651\n","Test set => Mean Squared error: 16.0913, Mean Abs error: 0.536001 \n","\n","Epoch 283   -------------------------------\n","Train loss: 0.07822813093662262\n","Test set => Mean Squared error: 16.0907, Mean Abs error: 0.536007 \n","\n","Epoch 284   -------------------------------\n","Train loss: 0.07812632620334625\n","Test set => Mean Squared error: 16.1049, Mean Abs error: 0.536224 \n","\n","Epoch 285   -------------------------------\n","Train loss: 0.07769785076379776\n","Test set => Mean Squared error: 16.1257, Mean Abs error: 0.536547 \n","\n","Epoch 286   -------------------------------\n","Train loss: 0.07770726829767227\n","Test set => Mean Squared error: 16.1355, Mean Abs error: 0.536712 \n","\n","Epoch 287   -------------------------------\n","Train loss: 0.07784564048051834\n","Test set => Mean Squared error: 16.1309, Mean Abs error: 0.536624 \n","\n","Epoch 288   -------------------------------\n","Train loss: 0.07753746211528778\n","Test set => Mean Squared error: 16.1262, Mean Abs error: 0.536546 \n","\n","Epoch 289   -------------------------------\n","Train loss: 0.0775434821844101\n","Test set => Mean Squared error: 16.1252, Mean Abs error: 0.536537 \n","\n","Epoch 290   -------------------------------\n","Train loss: 0.07755734771490097\n","Test set => Mean Squared error: 16.1394, Mean Abs error: 0.536753 \n","\n","Epoch 291   -------------------------------\n","Train loss: 0.07756391912698746\n","Test set => Mean Squared error: 16.1396, Mean Abs error: 0.536747 \n","\n","Epoch 292   -------------------------------\n","Train loss: 0.07736298441886902\n","Test set => Mean Squared error: 16.1407, Mean Abs error: 0.536748 \n","\n","Epoch 293   -------------------------------\n","Train loss: 0.07722025364637375\n","Test set => Mean Squared error: 16.1436, Mean Abs error: 0.536788 \n","\n","Epoch 294   -------------------------------\n","Train loss: 0.07709519565105438\n","Test set => Mean Squared error: 16.1469, Mean Abs error: 0.536843 \n","\n","Epoch 295   -------------------------------\n","Train loss: 0.07712823152542114\n","Test set => Mean Squared error: 16.1531, Mean Abs error: 0.536931 \n","\n","Epoch 296   -------------------------------\n","Train loss: 0.07717030495405197\n","Test set => Mean Squared error: 16.1443, Mean Abs error: 0.536789 \n","\n","Epoch 297   -------------------------------\n","Train loss: 0.07731980085372925\n","Test set => Mean Squared error: 16.1613, Mean Abs error: 0.537035 \n","\n","Epoch 298   -------------------------------\n","Train loss: 0.07740844041109085\n","Test set => Mean Squared error: 16.1509, Mean Abs error: 0.536880 \n","\n","Epoch 299   -------------------------------\n","Train loss: 0.07730603218078613\n","Test set => Mean Squared error: 16.1654, Mean Abs error: 0.537095 \n","\n","Epoch 300   -------------------------------\n","Train loss: 0.0769706442952156\n","Test set => Mean Squared error: 16.1638, Mean Abs error: 0.537072 \n","\n","Epoch 301   -------------------------------\n","Train loss: 0.07670755684375763\n","Test set => Mean Squared error: 16.1659, Mean Abs error: 0.537098 \n","\n","Epoch 302   -------------------------------\n","Train loss: 0.0765988677740097\n","Test set => Mean Squared error: 16.1740, Mean Abs error: 0.537210 \n","\n","Epoch 303   -------------------------------\n","Train loss: 0.07665470242500305\n","Test set => Mean Squared error: 16.1678, Mean Abs error: 0.537107 \n","\n","Epoch 304   -------------------------------\n","Train loss: 0.07670409977436066\n","Test set => Mean Squared error: 16.1753, Mean Abs error: 0.537202 \n","\n","Epoch 305   -------------------------------\n","Train loss: 0.07662289589643478\n","Test set => Mean Squared error: 16.1721, Mean Abs error: 0.537158 \n","\n","Epoch 306   -------------------------------\n","Train loss: 0.07659431546926498\n","Test set => Mean Squared error: 16.1895, Mean Abs error: 0.537424 \n","\n","Epoch 307   -------------------------------\n","Train loss: 0.07644595205783844\n","Test set => Mean Squared error: 16.1900, Mean Abs error: 0.537432 \n","\n","Epoch 308   -------------------------------\n","Train loss: 0.07627275586128235\n","Test set => Mean Squared error: 16.1917, Mean Abs error: 0.537457 \n","\n","Epoch 309   -------------------------------\n","Train loss: 0.07623876631259918\n","Test set => Mean Squared error: 16.2039, Mean Abs error: 0.537641 \n","\n","Epoch 310   -------------------------------\n","Train loss: 0.07625998556613922\n","Test set => Mean Squared error: 16.1932, Mean Abs error: 0.537473 \n","\n","Epoch 311   -------------------------------\n","Train loss: 0.07634679228067398\n","Test set => Mean Squared error: 16.2105, Mean Abs error: 0.537739 \n","\n","Epoch 312   -------------------------------\n","Train loss: 0.07623923569917679\n","Test set => Mean Squared error: 16.2042, Mean Abs error: 0.537638 \n","\n","Epoch 313   -------------------------------\n","Train loss: 0.07609811425209045\n","Test set => Mean Squared error: 16.2136, Mean Abs error: 0.537781 \n","\n","Epoch 314   -------------------------------\n","Train loss: 0.07596080005168915\n","Test set => Mean Squared error: 16.2190, Mean Abs error: 0.537858 \n","\n","Epoch 315   -------------------------------\n","Train loss: 0.07590766251087189\n","Test set => Mean Squared error: 16.2086, Mean Abs error: 0.537686 \n","\n","Epoch 316   -------------------------------\n","Train loss: 0.07589434087276459\n","Test set => Mean Squared error: 16.2133, Mean Abs error: 0.537748 \n","\n","Epoch 317   -------------------------------\n","Train loss: 0.07597684860229492\n","Test set => Mean Squared error: 16.2032, Mean Abs error: 0.537592 \n","\n","Epoch 318   -------------------------------\n","Train loss: 0.0764932781457901\n","Test set => Mean Squared error: 16.2251, Mean Abs error: 0.537960 \n","\n","Epoch 319   -------------------------------\n","Train loss: 0.07674386352300644\n","Test set => Mean Squared error: 16.2012, Mean Abs error: 0.537565 \n","\n","Epoch 320   -------------------------------\n","Train loss: 0.07668297737836838\n","Test set => Mean Squared error: 16.2151, Mean Abs error: 0.537786 \n","\n","Epoch 321   -------------------------------\n","Train loss: 0.07591772824525833\n","Test set => Mean Squared error: 16.2080, Mean Abs error: 0.537643 \n","\n","Epoch 322   -------------------------------\n","Train loss: 0.07578562200069427\n","Test set => Mean Squared error: 16.1907, Mean Abs error: 0.537351 \n","\n","Epoch 323   -------------------------------\n","Train loss: 0.07620527595281601\n","Test set => Mean Squared error: 16.2139, Mean Abs error: 0.537715 \n","\n","Epoch 324   -------------------------------\n","Train loss: 0.076420359313488\n","Test set => Mean Squared error: 16.1972, Mean Abs error: 0.537450 \n","\n","Epoch 325   -------------------------------\n","Train loss: 0.07623325288295746\n","Test set => Mean Squared error: 16.2181, Mean Abs error: 0.537798 \n","\n","Epoch 326   -------------------------------\n","Train loss: 0.07547254860401154\n","Test set => Mean Squared error: 16.2184, Mean Abs error: 0.537785 \n","\n","Epoch 327   -------------------------------\n","Train loss: 0.0755293220281601\n","Test set => Mean Squared error: 16.1963, Mean Abs error: 0.537405 \n","\n","Epoch 328   -------------------------------\n","Train loss: 0.07642796635627747\n","Test set => Mean Squared error: 16.2211, Mean Abs error: 0.537810 \n","\n","Epoch 329   -------------------------------\n","Train loss: 0.07566298544406891\n","Test set => Mean Squared error: 16.2135, Mean Abs error: 0.537687 \n","\n","Epoch 330   -------------------------------\n","Train loss: 0.07523086667060852\n","Test set => Mean Squared error: 16.2071, Mean Abs error: 0.537583 \n","\n","Epoch 331   -------------------------------\n","Train loss: 0.07531292736530304\n","Test set => Mean Squared error: 16.2200, Mean Abs error: 0.537790 \n","\n","Epoch 332   -------------------------------\n","Train loss: 0.0752970352768898\n","Test set => Mean Squared error: 16.2150, Mean Abs error: 0.537697 \n","\n","Epoch 333   -------------------------------\n","Train loss: 0.07502172142267227\n","Test set => Mean Squared error: 16.2197, Mean Abs error: 0.537769 \n","\n","Epoch 334   -------------------------------\n","Train loss: 0.07495146244764328\n","Test set => Mean Squared error: 16.2140, Mean Abs error: 0.537671 \n","\n","Epoch 335   -------------------------------\n","Train loss: 0.07487054169178009\n","Test set => Mean Squared error: 16.2301, Mean Abs error: 0.537910 \n","\n","Epoch 336   -------------------------------\n","Train loss: 0.07496577501296997\n","Test set => Mean Squared error: 16.2051, Mean Abs error: 0.537566 \n","\n","Epoch 337   -------------------------------\n","Train loss: 0.07573951780796051\n","Test set => Mean Squared error: 16.2438, Mean Abs error: 0.538150 \n","\n","Epoch 338   -------------------------------\n","Train loss: 0.07576563954353333\n","Test set => Mean Squared error: 16.2023, Mean Abs error: 0.537546 \n","\n","Epoch 339   -------------------------------\n","Train loss: 0.07516396045684814\n","Test set => Mean Squared error: 16.2090, Mean Abs error: 0.537656 \n","\n","Epoch 340   -------------------------------\n","Train loss: 0.0745517835021019\n","Test set => Mean Squared error: 16.2120, Mean Abs error: 0.537671 \n","\n","Epoch 341   -------------------------------\n","Train loss: 0.07488273829221725\n","Test set => Mean Squared error: 16.1766, Mean Abs error: 0.537131 \n","\n","Epoch 342   -------------------------------\n","Train loss: 0.07509349286556244\n","Test set => Mean Squared error: 16.1929, Mean Abs error: 0.537394 \n","\n","Epoch 343   -------------------------------\n","Train loss: 0.07471496611833572\n","Test set => Mean Squared error: 16.1816, Mean Abs error: 0.537163 \n","\n","Epoch 344   -------------------------------\n","Train loss: 0.07484471797943115\n","Test set => Mean Squared error: 16.1798, Mean Abs error: 0.537189 \n","\n","Epoch 345   -------------------------------\n","Train loss: 0.07432882487773895\n","Test set => Mean Squared error: 16.1875, Mean Abs error: 0.537323 \n","\n","Epoch 346   -------------------------------\n","Train loss: 0.07437247037887573\n","Test set => Mean Squared error: 16.1949, Mean Abs error: 0.537405 \n","\n","Epoch 347   -------------------------------\n","Train loss: 0.07442597299814224\n","Test set => Mean Squared error: 16.1968, Mean Abs error: 0.537439 \n","\n","Epoch 348   -------------------------------\n","Train loss: 0.07409602403640747\n","Test set => Mean Squared error: 16.1946, Mean Abs error: 0.537395 \n","\n","Epoch 349   -------------------------------\n","Train loss: 0.07412334531545639\n","Test set => Mean Squared error: 16.2131, Mean Abs error: 0.537666 \n","\n","Epoch 350   -------------------------------\n","Train loss: 0.07397367805242538\n","Test set => Mean Squared error: 16.2153, Mean Abs error: 0.537710 \n","\n","Epoch 351   -------------------------------\n","Train loss: 0.07382456213235855\n","Test set => Mean Squared error: 16.2090, Mean Abs error: 0.537620 \n","\n","Epoch 352   -------------------------------\n","Train loss: 0.07390668243169785\n","Test set => Mean Squared error: 16.2257, Mean Abs error: 0.537879 \n","\n","Epoch 353   -------------------------------\n","Train loss: 0.07384861260652542\n","Test set => Mean Squared error: 16.2199, Mean Abs error: 0.537753 \n","\n","Epoch 354   -------------------------------\n","Train loss: 0.07393850386142731\n","Test set => Mean Squared error: 16.2169, Mean Abs error: 0.537695 \n","\n","Epoch 355   -------------------------------\n","Train loss: 0.07434433698654175\n","Test set => Mean Squared error: 16.1902, Mean Abs error: 0.537222 \n","\n","Epoch 356   -------------------------------\n","Train loss: 0.0760524570941925\n","Test set => Mean Squared error: 16.2412, Mean Abs error: 0.538154 \n","\n","Epoch 357   -------------------------------\n","Train loss: 0.07588064670562744\n","Test set => Mean Squared error: 16.2290, Mean Abs error: 0.537952 \n","\n","Epoch 358   -------------------------------\n","Train loss: 0.07409919798374176\n","Test set => Mean Squared error: 16.2140, Mean Abs error: 0.537705 \n","\n","Epoch 359   -------------------------------\n","Train loss: 0.07468552142381668\n","Test set => Mean Squared error: 16.2445, Mean Abs error: 0.538235 \n","\n","Epoch 360   -------------------------------\n","Train loss: 0.07488347589969635\n","Test set => Mean Squared error: 16.2320, Mean Abs error: 0.537967 \n","\n","Epoch 361   -------------------------------\n","Train loss: 0.07386821508407593\n","Test set => Mean Squared error: 16.2314, Mean Abs error: 0.537942 \n","\n","Epoch 362   -------------------------------\n","Train loss: 0.07357164472341537\n","Test set => Mean Squared error: 16.2552, Mean Abs error: 0.538326 \n","\n","Epoch 363   -------------------------------\n","Train loss: 0.07462543249130249\n","Test set => Mean Squared error: 16.2428, Mean Abs error: 0.538101 \n","\n","Epoch 364   -------------------------------\n","Train loss: 0.07599256187677383\n","Test set => Mean Squared error: 16.2659, Mean Abs error: 0.538566 \n","\n","Epoch 365   -------------------------------\n","Train loss: 0.07373513281345367\n","Test set => Mean Squared error: 16.2647, Mean Abs error: 0.538538 \n","\n","Epoch 366   -------------------------------\n","Train loss: 0.07413756847381592\n","Test set => Mean Squared error: 16.2358, Mean Abs error: 0.537973 \n","\n","Epoch 367   -------------------------------\n","Train loss: 0.07471296191215515\n","Test set => Mean Squared error: 16.2488, Mean Abs error: 0.538204 \n","\n","Epoch 368   -------------------------------\n","Train loss: 0.07354134321212769\n","Test set => Mean Squared error: 16.2531, Mean Abs error: 0.538211 \n","\n","Epoch 369   -------------------------------\n","Train loss: 0.07339569926261902\n","Test set => Mean Squared error: 16.2304, Mean Abs error: 0.537767 \n","\n","Epoch 370   -------------------------------\n","Train loss: 0.07449564337730408\n","Test set => Mean Squared error: 16.2486, Mean Abs error: 0.538114 \n","\n","Epoch 371   -------------------------------\n","Train loss: 0.075713150203228\n","Test set => Mean Squared error: 16.2284, Mean Abs error: 0.537815 \n","\n","Epoch 372   -------------------------------\n","Train loss: 0.07871963083744049\n","Test set => Mean Squared error: 16.2739, Mean Abs error: 0.538690 \n","\n","Epoch 373   -------------------------------\n","Train loss: 0.07552598416805267\n","Test set => Mean Squared error: 16.2466, Mean Abs error: 0.538230 \n","\n","Epoch 374   -------------------------------\n","Train loss: 0.07600095868110657\n","Test set => Mean Squared error: 16.1663, Mean Abs error: 0.536804 \n","\n","Epoch 375   -------------------------------\n","Train loss: 0.07940888404846191\n","Test set => Mean Squared error: 16.1765, Mean Abs error: 0.537008 \n","\n","Epoch 376   -------------------------------\n","Train loss: 0.07467494159936905\n","Test set => Mean Squared error: 16.2009, Mean Abs error: 0.537427 \n","\n","Epoch 377   -------------------------------\n","Train loss: 0.07936549186706543\n","Test set => Mean Squared error: 16.1450, Mean Abs error: 0.536455 \n","\n","Epoch 378   -------------------------------\n","Train loss: 0.07862458378076553\n","Test set => Mean Squared error: 16.1416, Mean Abs error: 0.536455 \n","\n","Epoch 379   -------------------------------\n","Train loss: 0.07502014935016632\n","Test set => Mean Squared error: 16.1871, Mean Abs error: 0.537234 \n","\n","Epoch 380   -------------------------------\n","Train loss: 0.07868088036775589\n","Test set => Mean Squared error: 16.1577, Mean Abs error: 0.536684 \n","\n","Epoch 381   -------------------------------\n","Train loss: 0.07505294680595398\n","Test set => Mean Squared error: 16.1388, Mean Abs error: 0.536418 \n","\n","Epoch 382   -------------------------------\n","Train loss: 0.07549481838941574\n","Test set => Mean Squared error: 16.1754, Mean Abs error: 0.537104 \n","\n","Epoch 383   -------------------------------\n","Train loss: 0.07435505837202072\n","Test set => Mean Squared error: 16.2014, Mean Abs error: 0.537472 \n","\n","Epoch 384   -------------------------------\n","Train loss: 0.07430655509233475\n","Test set => Mean Squared error: 16.1726, Mean Abs error: 0.536886 \n","\n","Epoch 385   -------------------------------\n","Train loss: 0.07241697609424591\n","Test set => Mean Squared error: 16.1508, Mean Abs error: 0.536487 \n","\n","Epoch 386   -------------------------------\n","Train loss: 0.07334300130605698\n","Test set => Mean Squared error: 16.1684, Mean Abs error: 0.536741 \n","\n","Epoch 387   -------------------------------\n","Train loss: 0.07310592383146286\n","Test set => Mean Squared error: 16.1717, Mean Abs error: 0.536792 \n","\n","Epoch 388   -------------------------------\n","Train loss: 0.07474055886268616\n","Test set => Mean Squared error: 16.1628, Mean Abs error: 0.536679 \n","\n","Epoch 389   -------------------------------\n","Train loss: 0.07307537645101547\n","Test set => Mean Squared error: 16.1392, Mean Abs error: 0.536266 \n","\n","Epoch 390   -------------------------------\n","Train loss: 0.07340395450592041\n","Test set => Mean Squared error: 16.1451, Mean Abs error: 0.536379 \n","\n","Epoch 391   -------------------------------\n","Train loss: 0.0735156387090683\n","Test set => Mean Squared error: 16.1504, Mean Abs error: 0.536471 \n","\n","Epoch 392   -------------------------------\n","Train loss: 0.07299581170082092\n","Test set => Mean Squared error: 16.1351, Mean Abs error: 0.536177 \n","\n","Epoch 393   -------------------------------\n","Train loss: 0.07261304557323456\n","Test set => Mean Squared error: 16.1426, Mean Abs error: 0.536364 \n","\n","Epoch 394   -------------------------------\n","Train loss: 0.0724954828619957\n","Test set => Mean Squared error: 16.1518, Mean Abs error: 0.536534 \n","\n","Epoch 395   -------------------------------\n","Train loss: 0.07237246632575989\n","Test set => Mean Squared error: 16.1564, Mean Abs error: 0.536570 \n","\n","Epoch 396   -------------------------------\n","Train loss: 0.07232500612735748\n","Test set => Mean Squared error: 16.1800, Mean Abs error: 0.537015 \n","\n","Epoch 397   -------------------------------\n","Train loss: 0.07163535058498383\n","Test set => Mean Squared error: 16.1955, Mean Abs error: 0.537262 \n","\n","Epoch 398   -------------------------------\n","Train loss: 0.07156027853488922\n","Test set => Mean Squared error: 16.2163, Mean Abs error: 0.537527 \n","\n","Epoch 399   -------------------------------\n","Train loss: 0.0715150237083435\n","Test set => Mean Squared error: 16.2225, Mean Abs error: 0.537554 \n","\n","Epoch 400   -------------------------------\n","Train loss: 0.0716964602470398\n","Test set => Mean Squared error: 16.2084, Mean Abs error: 0.537335 \n","\n","Epoch 401   -------------------------------\n","Train loss: 0.07162810117006302\n","Test set => Mean Squared error: 16.2369, Mean Abs error: 0.537908 \n","\n","Epoch 402   -------------------------------\n","Train loss: 0.07155033946037292\n","Test set => Mean Squared error: 16.2600, Mean Abs error: 0.538329 \n","\n","Epoch 403   -------------------------------\n","Train loss: 0.07079896330833435\n","Test set => Mean Squared error: 16.2569, Mean Abs error: 0.538290 \n","\n","Epoch 404   -------------------------------\n","Train loss: 0.0706106498837471\n","Test set => Mean Squared error: 16.2465, Mean Abs error: 0.538131 \n","\n","Epoch 405   -------------------------------\n","Train loss: 0.07087557017803192\n","Test set => Mean Squared error: 16.2247, Mean Abs error: 0.537758 \n","\n","Epoch 406   -------------------------------\n","Train loss: 0.07128173857927322\n","Test set => Mean Squared error: 16.2440, Mean Abs error: 0.538072 \n","\n","Epoch 407   -------------------------------\n","Train loss: 0.07038547843694687\n","Test set => Mean Squared error: 16.2591, Mean Abs error: 0.538311 \n","\n","Epoch 408   -------------------------------\n","Train loss: 0.07040666788816452\n","Test set => Mean Squared error: 16.2577, Mean Abs error: 0.538277 \n","\n","Epoch 409   -------------------------------\n","Train loss: 0.07105927169322968\n","Test set => Mean Squared error: 16.2610, Mean Abs error: 0.538317 \n","\n","Epoch 410   -------------------------------\n","Train loss: 0.07079701125621796\n","Test set => Mean Squared error: 16.2745, Mean Abs error: 0.538555 \n","\n","Epoch 411   -------------------------------\n","Train loss: 0.06978170573711395\n","Test set => Mean Squared error: 16.2838, Mean Abs error: 0.538693 \n","\n","Epoch 412   -------------------------------\n","Train loss: 0.07006271928548813\n","Test set => Mean Squared error: 16.2633, Mean Abs error: 0.538292 \n","\n","Epoch 413   -------------------------------\n","Train loss: 0.0701432079076767\n","Test set => Mean Squared error: 16.2683, Mean Abs error: 0.538381 \n","\n","Epoch 414   -------------------------------\n","Train loss: 0.07044228911399841\n","Test set => Mean Squared error: 16.2997, Mean Abs error: 0.538887 \n","\n","Epoch 415   -------------------------------\n","Train loss: 0.07060235738754272\n","Test set => Mean Squared error: 16.2846, Mean Abs error: 0.538615 \n","\n","Epoch 416   -------------------------------\n","Train loss: 0.06999550759792328\n","Test set => Mean Squared error: 16.2815, Mean Abs error: 0.538578 \n","\n","Epoch 417   -------------------------------\n","Train loss: 0.06909099966287613\n","Test set => Mean Squared error: 16.2881, Mean Abs error: 0.538707 \n","\n","Epoch 418   -------------------------------\n","Train loss: 0.06959084421396255\n","Test set => Mean Squared error: 16.2949, Mean Abs error: 0.538799 \n","\n","Epoch 419   -------------------------------\n","Train loss: 0.06955071538686752\n","Test set => Mean Squared error: 16.3127, Mean Abs error: 0.539054 \n","\n","Epoch 420   -------------------------------\n","Train loss: 0.06926032900810242\n","Test set => Mean Squared error: 16.3075, Mean Abs error: 0.538982 \n","\n","Epoch 421   -------------------------------\n","Train loss: 0.06901014596223831\n","Test set => Mean Squared error: 16.3101, Mean Abs error: 0.539018 \n","\n","Epoch 422   -------------------------------\n","Train loss: 0.06887052208185196\n","Test set => Mean Squared error: 16.3305, Mean Abs error: 0.539297 \n","\n","Epoch 423   -------------------------------\n","Train loss: 0.06887713819742203\n","Test set => Mean Squared error: 16.3262, Mean Abs error: 0.539190 \n","\n","Epoch 424   -------------------------------\n","Train loss: 0.06899258494377136\n","Test set => Mean Squared error: 16.3182, Mean Abs error: 0.539066 \n","\n","Epoch 425   -------------------------------\n","Train loss: 0.06872823089361191\n","Test set => Mean Squared error: 16.3182, Mean Abs error: 0.539099 \n","\n","Epoch 426   -------------------------------\n","Train loss: 0.06852171570062637\n","Test set => Mean Squared error: 16.3372, Mean Abs error: 0.539411 \n","\n","Epoch 427   -------------------------------\n","Train loss: 0.06828814744949341\n","Test set => Mean Squared error: 16.3507, Mean Abs error: 0.539649 \n","\n","Epoch 428   -------------------------------\n","Train loss: 0.06824085116386414\n","Test set => Mean Squared error: 16.3409, Mean Abs error: 0.539494 \n","\n","Epoch 429   -------------------------------\n","Train loss: 0.06822539120912552\n","Test set => Mean Squared error: 16.3345, Mean Abs error: 0.539366 \n","\n","Epoch 430   -------------------------------\n","Train loss: 0.0681062862277031\n","Test set => Mean Squared error: 16.3436, Mean Abs error: 0.539527 \n","\n","Epoch 431   -------------------------------\n","Train loss: 0.06788918375968933\n","Test set => Mean Squared error: 16.3482, Mean Abs error: 0.539589 \n","\n","Epoch 432   -------------------------------\n","Train loss: 0.06783854216337204\n","Test set => Mean Squared error: 16.3406, Mean Abs error: 0.539445 \n","\n","Epoch 433   -------------------------------\n","Train loss: 0.06774435192346573\n","Test set => Mean Squared error: 16.3485, Mean Abs error: 0.539583 \n","\n","Epoch 434   -------------------------------\n","Train loss: 0.0678265169262886\n","Test set => Mean Squared error: 16.3527, Mean Abs error: 0.539670 \n","\n","Epoch 435   -------------------------------\n","Train loss: 0.06830918043851852\n","Test set => Mean Squared error: 16.3715, Mean Abs error: 0.540010 \n","\n","Epoch 436   -------------------------------\n","Train loss: 0.06761615723371506\n","Test set => Mean Squared error: 16.3705, Mean Abs error: 0.539981 \n","\n","Epoch 437   -------------------------------\n","Train loss: 0.06772655248641968\n","Test set => Mean Squared error: 16.3766, Mean Abs error: 0.540094 \n","\n","Epoch 438   -------------------------------\n","Train loss: 0.06759735941886902\n","Test set => Mean Squared error: 16.3893, Mean Abs error: 0.540283 \n","\n","Epoch 439   -------------------------------\n","Train loss: 0.06776216626167297\n","Test set => Mean Squared error: 16.3771, Mean Abs error: 0.540028 \n","\n","Epoch 440   -------------------------------\n","Train loss: 0.0673099011182785\n","Test set => Mean Squared error: 16.3699, Mean Abs error: 0.539906 \n","\n","Epoch 441   -------------------------------\n","Train loss: 0.06744163483381271\n","Test set => Mean Squared error: 16.3804, Mean Abs error: 0.540062 \n","\n","Epoch 442   -------------------------------\n","Train loss: 0.06706338375806808\n","Test set => Mean Squared error: 16.3907, Mean Abs error: 0.540192 \n","\n","Epoch 443   -------------------------------\n","Train loss: 0.06716740876436234\n","Test set => Mean Squared error: 16.3927, Mean Abs error: 0.540224 \n","\n","Epoch 444   -------------------------------\n","Train loss: 0.06698659062385559\n","Test set => Mean Squared error: 16.3839, Mean Abs error: 0.540084 \n","\n","Epoch 445   -------------------------------\n","Train loss: 0.0667232871055603\n","Test set => Mean Squared error: 16.3845, Mean Abs error: 0.540096 \n","\n","Epoch 446   -------------------------------\n","Train loss: 0.06669729948043823\n","Test set => Mean Squared error: 16.3886, Mean Abs error: 0.540143 \n","\n","Epoch 447   -------------------------------\n","Train loss: 0.06667392700910568\n","Test set => Mean Squared error: 16.3888, Mean Abs error: 0.540136 \n","\n","Epoch 448   -------------------------------\n","Train loss: 0.06670265644788742\n","Test set => Mean Squared error: 16.3873, Mean Abs error: 0.540109 \n","\n","Epoch 449   -------------------------------\n","Train loss: 0.06693204492330551\n","Test set => Mean Squared error: 16.3775, Mean Abs error: 0.539964 \n","\n","Epoch 450   -------------------------------\n","Train loss: 0.06819556653499603\n","Test set => Mean Squared error: 16.3944, Mean Abs error: 0.540320 \n","\n","Epoch 451   -------------------------------\n","Train loss: 0.06698737293481827\n","Test set => Mean Squared error: 16.3895, Mean Abs error: 0.540229 \n","\n","Epoch 452   -------------------------------\n","Train loss: 0.06778261810541153\n","Test set => Mean Squared error: 16.3747, Mean Abs error: 0.539955 \n","\n","Epoch 453   -------------------------------\n","Train loss: 0.06666776537895203\n","Test set => Mean Squared error: 16.3699, Mean Abs error: 0.539841 \n","\n","Epoch 454   -------------------------------\n","Train loss: 0.06713925302028656\n","Test set => Mean Squared error: 16.3736, Mean Abs error: 0.539877 \n","\n","Epoch 455   -------------------------------\n","Train loss: 0.06721626967191696\n","Test set => Mean Squared error: 16.3677, Mean Abs error: 0.539781 \n","\n","Epoch 456   -------------------------------\n","Train loss: 0.06688852608203888\n","Test set => Mean Squared error: 16.3859, Mean Abs error: 0.540167 \n","\n","Epoch 457   -------------------------------\n","Train loss: 0.06700456887483597\n","Test set => Mean Squared error: 16.4053, Mean Abs error: 0.540483 \n","\n","Epoch 458   -------------------------------\n","Train loss: 0.06671031564474106\n","Test set => Mean Squared error: 16.3912, Mean Abs error: 0.540186 \n","\n","Epoch 459   -------------------------------\n","Train loss: 0.06666842848062515\n","Test set => Mean Squared error: 16.3909, Mean Abs error: 0.540206 \n","\n","Epoch 460   -------------------------------\n","Train loss: 0.06664635241031647\n","Test set => Mean Squared error: 16.3904, Mean Abs error: 0.540161 \n","\n","Epoch 461   -------------------------------\n","Train loss: 0.06655358523130417\n","Test set => Mean Squared error: 16.3893, Mean Abs error: 0.540098 \n","\n","Epoch 462   -------------------------------\n","Train loss: 0.06654269248247147\n","Test set => Mean Squared error: 16.3916, Mean Abs error: 0.540127 \n","\n","Epoch 463   -------------------------------\n","Train loss: 0.06623580306768417\n","Test set => Mean Squared error: 16.3807, Mean Abs error: 0.539908 \n","\n","Epoch 464   -------------------------------\n","Train loss: 0.06592388451099396\n","Test set => Mean Squared error: 16.3774, Mean Abs error: 0.539836 \n","\n","Epoch 465   -------------------------------\n","Train loss: 0.06580549478530884\n","Test set => Mean Squared error: 16.3893, Mean Abs error: 0.540016 \n","\n","Epoch 466   -------------------------------\n","Train loss: 0.06576371192932129\n","Test set => Mean Squared error: 16.3802, Mean Abs error: 0.539853 \n","\n","Epoch 467   -------------------------------\n","Train loss: 0.06699560582637787\n","Test set => Mean Squared error: 16.3837, Mean Abs error: 0.539883 \n","\n","Epoch 468   -------------------------------\n","Train loss: 0.07839824259281158\n","Test set => Mean Squared error: 16.2971, Mean Abs error: 0.538582 \n","\n","Epoch 469   -------------------------------\n","Train loss: 0.09770376980304718\n","Test set => Mean Squared error: 16.3579, Mean Abs error: 0.540002 \n","\n","Epoch 470   -------------------------------\n","Train loss: 0.0783277302980423\n","Test set => Mean Squared error: 16.3773, Mean Abs error: 0.540555 \n","\n","Epoch 471   -------------------------------\n","Train loss: 0.10089962184429169\n","Test set => Mean Squared error: 16.2940, Mean Abs error: 0.539125 \n","\n","Epoch 472   -------------------------------\n","Train loss: 0.07997071743011475\n","Test set => Mean Squared error: 16.2105, Mean Abs error: 0.537594 \n","\n","Epoch 473   -------------------------------\n","Train loss: 0.09117352962493896\n","Test set => Mean Squared error: 16.1021, Mean Abs error: 0.535673 \n","\n","Epoch 474   -------------------------------\n","Train loss: 0.08247572928667068\n","Test set => Mean Squared error: 16.0755, Mean Abs error: 0.535201 \n","\n","Epoch 475   -------------------------------\n","Train loss: 0.09520057588815689\n","Test set => Mean Squared error: 15.9792, Mean Abs error: 0.533542 \n","\n","Epoch 476   -------------------------------\n","Train loss: 0.08474830538034439\n","Test set => Mean Squared error: 15.9338, Mean Abs error: 0.532745 \n","\n","Epoch 477   -------------------------------\n","Train loss: 0.08813198655843735\n","Test set => Mean Squared error: 15.9905, Mean Abs error: 0.533703 \n","\n","Epoch 478   -------------------------------\n","Train loss: 0.07783035188913345\n","Test set => Mean Squared error: 16.0654, Mean Abs error: 0.534889 \n","\n","Epoch 479   -------------------------------\n","Train loss: 0.08154890686273575\n","Test set => Mean Squared error: 16.0859, Mean Abs error: 0.535043 \n","\n","Epoch 480   -------------------------------\n","Train loss: 0.07807944715023041\n","Test set => Mean Squared error: 16.0999, Mean Abs error: 0.535167 \n","\n","Epoch 481   -------------------------------\n","Train loss: 0.08187127113342285\n","Test set => Mean Squared error: 16.1418, Mean Abs error: 0.535793 \n","\n","Epoch 482   -------------------------------\n","Train loss: 0.0788235142827034\n","Test set => Mean Squared error: 16.1634, Mean Abs error: 0.536100 \n","\n","Epoch 483   -------------------------------\n","Train loss: 0.07779678702354431\n","Test set => Mean Squared error: 16.1503, Mean Abs error: 0.535894 \n","\n","Epoch 484   -------------------------------\n","Train loss: 0.07484795898199081\n","Test set => Mean Squared error: 16.1206, Mean Abs error: 0.535458 \n","\n","Epoch 485   -------------------------------\n","Train loss: 0.07222359627485275\n","Test set => Mean Squared error: 16.1071, Mean Abs error: 0.535258 \n","\n","Epoch 486   -------------------------------\n","Train loss: 0.07183557003736496\n","Test set => Mean Squared error: 16.1202, Mean Abs error: 0.535414 \n","\n","Epoch 487   -------------------------------\n","Train loss: 0.072689950466156\n","Test set => Mean Squared error: 16.1412, Mean Abs error: 0.535766 \n","\n","Epoch 488   -------------------------------\n","Train loss: 0.07191126048564911\n","Test set => Mean Squared error: 16.1204, Mean Abs error: 0.535414 \n","\n","Epoch 489   -------------------------------\n","Train loss: 0.07064495980739594\n","Test set => Mean Squared error: 16.1154, Mean Abs error: 0.535281 \n","\n","Epoch 490   -------------------------------\n","Train loss: 0.07048371434211731\n","Test set => Mean Squared error: 16.1529, Mean Abs error: 0.535895 \n","\n","Epoch 491   -------------------------------\n","Train loss: 0.07002133876085281\n","Test set => Mean Squared error: 16.1509, Mean Abs error: 0.535886 \n","\n","Epoch 492   -------------------------------\n","Train loss: 0.06925257295370102\n","Test set => Mean Squared error: 16.1183, Mean Abs error: 0.535377 \n","\n","Epoch 493   -------------------------------\n","Train loss: 0.06920339167118073\n","Test set => Mean Squared error: 16.1130, Mean Abs error: 0.535398 \n","\n","Epoch 494   -------------------------------\n","Train loss: 0.06756455451250076\n","Test set => Mean Squared error: 16.1207, Mean Abs error: 0.535630 \n","\n","Epoch 495   -------------------------------\n","Train loss: 0.06770370900630951\n","Test set => Mean Squared error: 16.1261, Mean Abs error: 0.535760 \n","\n","Epoch 496   -------------------------------\n","Train loss: 0.06686238944530487\n","Test set => Mean Squared error: 16.1294, Mean Abs error: 0.535828 \n","\n","Epoch 497   -------------------------------\n","Train loss: 0.06757211685180664\n","Test set => Mean Squared error: 16.1512, Mean Abs error: 0.536228 \n","\n","Epoch 498   -------------------------------\n","Train loss: 0.0660003051161766\n","Test set => Mean Squared error: 16.1884, Mean Abs error: 0.536890 \n","\n","Epoch 499   -------------------------------\n","Train loss: 0.06657325476408005\n","Test set => Mean Squared error: 16.2137, Mean Abs error: 0.537306 \n","\n","Epoch 500   -------------------------------\n","Train loss: 0.06589080393314362\n","Test set => Mean Squared error: 16.2168, Mean Abs error: 0.537341 \n","\n","Epoch 501   -------------------------------\n","Train loss: 0.06687843799591064\n","Test set => Mean Squared error: 16.2302, Mean Abs error: 0.537599 \n","\n","Epoch 502   -------------------------------\n","Train loss: 0.0658772885799408\n","Test set => Mean Squared error: 16.2400, Mean Abs error: 0.537763 \n","\n","Epoch 503   -------------------------------\n","Train loss: 0.06651628017425537\n","Test set => Mean Squared error: 16.2264, Mean Abs error: 0.537504 \n","\n","Epoch 504   -------------------------------\n","Train loss: 0.06552240997552872\n","Test set => Mean Squared error: 16.2245, Mean Abs error: 0.537409 \n","\n","Epoch 505   -------------------------------\n","Train loss: 0.06542631983757019\n","Test set => Mean Squared error: 16.2475, Mean Abs error: 0.537761 \n","\n","Epoch 506   -------------------------------\n","Train loss: 0.06491731107234955\n","Test set => Mean Squared error: 16.2662, Mean Abs error: 0.538055 \n","\n","Epoch 507   -------------------------------\n","Train loss: 0.0653986930847168\n","Test set => Mean Squared error: 16.2623, Mean Abs error: 0.537940 \n","\n","Epoch 508   -------------------------------\n","Train loss: 0.06480193883180618\n","Test set => Mean Squared error: 16.2704, Mean Abs error: 0.538048 \n","\n","Epoch 509   -------------------------------\n","Train loss: 0.0645325630903244\n","Test set => Mean Squared error: 16.2902, Mean Abs error: 0.538357 \n","\n","Epoch 510   -------------------------------\n","Train loss: 0.06451041996479034\n","Test set => Mean Squared error: 16.2974, Mean Abs error: 0.538435 \n","\n","Epoch 511   -------------------------------\n","Train loss: 0.06473508477210999\n","Test set => Mean Squared error: 16.3106, Mean Abs error: 0.538647 \n","\n","Epoch 512   -------------------------------\n","Train loss: 0.06439279764890671\n","Test set => Mean Squared error: 16.3286, Mean Abs error: 0.538936 \n","\n","Epoch 513   -------------------------------\n","Train loss: 0.06415612250566483\n","Test set => Mean Squared error: 16.3546, Mean Abs error: 0.539356 \n","\n","Epoch 514   -------------------------------\n","Train loss: 0.06403694301843643\n","Test set => Mean Squared error: 16.3698, Mean Abs error: 0.539592 \n","\n","Epoch 515   -------------------------------\n","Train loss: 0.06399314850568771\n","Test set => Mean Squared error: 16.3761, Mean Abs error: 0.539689 \n","\n","Epoch 516   -------------------------------\n","Train loss: 0.06385671347379684\n","Test set => Mean Squared error: 16.3937, Mean Abs error: 0.539998 \n","\n","Epoch 517   -------------------------------\n","Train loss: 0.06367915868759155\n","Test set => Mean Squared error: 16.4132, Mean Abs error: 0.540327 \n","\n","Epoch 518   -------------------------------\n","Train loss: 0.06354184448719025\n","Test set => Mean Squared error: 16.4260, Mean Abs error: 0.540528 \n","\n","Epoch 519   -------------------------------\n","Train loss: 0.06346474587917328\n","Test set => Mean Squared error: 16.4422, Mean Abs error: 0.540803 \n","\n","Epoch 520   -------------------------------\n","Train loss: 0.06329993903636932\n","Test set => Mean Squared error: 16.4605, Mean Abs error: 0.541127 \n","\n","Epoch 521   -------------------------------\n","Train loss: 0.06332719326019287\n","Test set => Mean Squared error: 16.4745, Mean Abs error: 0.541379 \n","\n","Epoch 522   -------------------------------\n","Train loss: 0.06305616348981857\n","Test set => Mean Squared error: 16.4855, Mean Abs error: 0.541575 \n","\n","Epoch 523   -------------------------------\n","Train loss: 0.06308185309171677\n","Test set => Mean Squared error: 16.4945, Mean Abs error: 0.541736 \n","\n","Epoch 524   -------------------------------\n","Train loss: 0.06292299181222916\n","Test set => Mean Squared error: 16.4973, Mean Abs error: 0.541788 \n","\n","Epoch 525   -------------------------------\n","Train loss: 0.06291016191244125\n","Test set => Mean Squared error: 16.4994, Mean Abs error: 0.541824 \n","\n","Epoch 526   -------------------------------\n","Train loss: 0.06281548738479614\n","Test set => Mean Squared error: 16.4963, Mean Abs error: 0.541763 \n","\n","Epoch 527   -------------------------------\n","Train loss: 0.06286594271659851\n","Test set => Mean Squared error: 16.5038, Mean Abs error: 0.541872 \n","\n","Epoch 528   -------------------------------\n","Train loss: 0.06325022131204605\n","Test set => Mean Squared error: 16.4916, Mean Abs error: 0.541655 \n","\n","Epoch 529   -------------------------------\n","Train loss: 0.06555193662643433\n","Test set => Mean Squared error: 16.5180, Mean Abs error: 0.542138 \n","\n","Epoch 530   -------------------------------\n","Train loss: 0.063727468252182\n","Test set => Mean Squared error: 16.5133, Mean Abs error: 0.542047 \n","\n","Epoch 531   -------------------------------\n","Train loss: 0.06351132690906525\n","Test set => Mean Squared error: 16.4864, Mean Abs error: 0.541556 \n","\n","Epoch 532   -------------------------------\n","Train loss: 0.06449487060308456\n","Test set => Mean Squared error: 16.5125, Mean Abs error: 0.541981 \n","\n","Epoch 533   -------------------------------\n","Train loss: 0.06363511085510254\n","Test set => Mean Squared error: 16.5293, Mean Abs error: 0.542235 \n","\n","Epoch 534   -------------------------------\n","Train loss: 0.06344109028577805\n","Test set => Mean Squared error: 16.5156, Mean Abs error: 0.541967 \n","\n","Epoch 535   -------------------------------\n","Train loss: 0.06378638744354248\n","Test set => Mean Squared error: 16.5317, Mean Abs error: 0.542212 \n","\n","Epoch 536   -------------------------------\n","Train loss: 0.06357759982347488\n","Test set => Mean Squared error: 16.5368, Mean Abs error: 0.542292 \n","\n","Epoch 537   -------------------------------\n","Train loss: 0.06294095516204834\n","Test set => Mean Squared error: 16.5286, Mean Abs error: 0.542141 \n","\n","Epoch 538   -------------------------------\n","Train loss: 0.06330416351556778\n","Test set => Mean Squared error: 16.5331, Mean Abs error: 0.542208 \n","\n","Epoch 539   -------------------------------\n","Train loss: 0.0633079782128334\n","Test set => Mean Squared error: 16.5385, Mean Abs error: 0.542283 \n","\n","Epoch 540   -------------------------------\n","Train loss: 0.06265446543693542\n","Test set => Mean Squared error: 16.5346, Mean Abs error: 0.542207 \n","\n","Epoch 541   -------------------------------\n","Train loss: 0.0628112182021141\n","Test set => Mean Squared error: 16.5408, Mean Abs error: 0.542310 \n","\n","Epoch 542   -------------------------------\n","Train loss: 0.06271987408399582\n","Test set => Mean Squared error: 16.5389, Mean Abs error: 0.542255 \n","\n","Epoch 543   -------------------------------\n","Train loss: 0.06228046491742134\n","Test set => Mean Squared error: 16.5476, Mean Abs error: 0.542381 \n","\n","Epoch 544   -------------------------------\n","Train loss: 0.06270704418420792\n","Test set => Mean Squared error: 16.5582, Mean Abs error: 0.542576 \n","\n","Epoch 545   -------------------------------\n","Train loss: 0.0622914619743824\n","Test set => Mean Squared error: 16.5508, Mean Abs error: 0.542460 \n","\n","Epoch 546   -------------------------------\n","Train loss: 0.062425047159194946\n","Test set => Mean Squared error: 16.5494, Mean Abs error: 0.542436 \n","\n","Epoch 547   -------------------------------\n","Train loss: 0.06243650242686272\n","Test set => Mean Squared error: 16.5645, Mean Abs error: 0.542698 \n","\n","Epoch 548   -------------------------------\n","Train loss: 0.06229205057024956\n","Test set => Mean Squared error: 16.5577, Mean Abs error: 0.542588 \n","\n","Epoch 549   -------------------------------\n","Train loss: 0.06248221546411514\n","Test set => Mean Squared error: 16.5468, Mean Abs error: 0.542411 \n","\n","Epoch 550   -------------------------------\n","Train loss: 0.06232443451881409\n","Test set => Mean Squared error: 16.5557, Mean Abs error: 0.542570 \n","\n","Epoch 551   -------------------------------\n","Train loss: 0.06219840794801712\n","Test set => Mean Squared error: 16.5557, Mean Abs error: 0.542574 \n","\n","Epoch 552   -------------------------------\n","Train loss: 0.062154896557331085\n","Test set => Mean Squared error: 16.5451, Mean Abs error: 0.542385 \n","\n","Epoch 553   -------------------------------\n","Train loss: 0.0621102899312973\n","Test set => Mean Squared error: 16.5412, Mean Abs error: 0.542308 \n","\n","Epoch 554   -------------------------------\n","Train loss: 0.06199709698557854\n","Test set => Mean Squared error: 16.5418, Mean Abs error: 0.542309 \n","\n","Epoch 555   -------------------------------\n","Train loss: 0.06187071651220322\n","Test set => Mean Squared error: 16.5443, Mean Abs error: 0.542335 \n","\n","Epoch 556   -------------------------------\n","Train loss: 0.06188354641199112\n","Test set => Mean Squared error: 16.5485, Mean Abs error: 0.542379 \n","\n","Epoch 557   -------------------------------\n","Train loss: 0.06182336062192917\n","Test set => Mean Squared error: 16.5588, Mean Abs error: 0.542529 \n","\n","Epoch 558   -------------------------------\n","Train loss: 0.06172135844826698\n","Test set => Mean Squared error: 16.5588, Mean Abs error: 0.542520 \n","\n","Epoch 559   -------------------------------\n","Train loss: 0.061633504927158356\n","Test set => Mean Squared error: 16.5531, Mean Abs error: 0.542420 \n","\n","Epoch 560   -------------------------------\n","Train loss: 0.06159470975399017\n","Test set => Mean Squared error: 16.5622, Mean Abs error: 0.542564 \n","\n","Epoch 561   -------------------------------\n","Train loss: 0.061477310955524445\n","Test set => Mean Squared error: 16.5636, Mean Abs error: 0.542581 \n","\n","Epoch 562   -------------------------------\n","Train loss: 0.061436977237463\n","Test set => Mean Squared error: 16.5575, Mean Abs error: 0.542478 \n","\n","Epoch 563   -------------------------------\n","Train loss: 0.06146964058279991\n","Test set => Mean Squared error: 16.5581, Mean Abs error: 0.542481 \n","\n","Epoch 564   -------------------------------\n","Train loss: 0.06132503226399422\n","Test set => Mean Squared error: 16.5640, Mean Abs error: 0.542577 \n","\n","Epoch 565   -------------------------------\n","Train loss: 0.06140991300344467\n","Test set => Mean Squared error: 16.5531, Mean Abs error: 0.542389 \n","\n","Epoch 566   -------------------------------\n","Train loss: 0.06158507615327835\n","Test set => Mean Squared error: 16.5589, Mean Abs error: 0.542482 \n","\n","Epoch 567   -------------------------------\n","Train loss: 0.061945270746946335\n","Test set => Mean Squared error: 16.5538, Mean Abs error: 0.542400 \n","\n","Epoch 568   -------------------------------\n","Train loss: 0.06209717318415642\n","Test set => Mean Squared error: 16.5518, Mean Abs error: 0.542381 \n","\n","Epoch 569   -------------------------------\n","Train loss: 0.061607494950294495\n","Test set => Mean Squared error: 16.5433, Mean Abs error: 0.542248 \n","\n","Epoch 570   -------------------------------\n","Train loss: 0.06136852502822876\n","Test set => Mean Squared error: 16.5423, Mean Abs error: 0.542232 \n","\n","Epoch 571   -------------------------------\n","Train loss: 0.06144057586789131\n","Test set => Mean Squared error: 16.5435, Mean Abs error: 0.542238 \n","\n","Epoch 572   -------------------------------\n","Train loss: 0.06137903034687042\n","Test set => Mean Squared error: 16.5320, Mean Abs error: 0.542017 \n","\n","Epoch 573   -------------------------------\n","Train loss: 0.0612003356218338\n","Test set => Mean Squared error: 16.5307, Mean Abs error: 0.541971 \n","\n","Epoch 574   -------------------------------\n","Train loss: 0.061269912868738174\n","Test set => Mean Squared error: 16.5421, Mean Abs error: 0.542150 \n","\n","Epoch 575   -------------------------------\n","Train loss: 0.06135442852973938\n","Test set => Mean Squared error: 16.5410, Mean Abs error: 0.542139 \n","\n","Epoch 576   -------------------------------\n","Train loss: 0.06147313117980957\n","Test set => Mean Squared error: 16.5532, Mean Abs error: 0.542373 \n","\n","Epoch 577   -------------------------------\n","Train loss: 0.06133251637220383\n","Test set => Mean Squared error: 16.5515, Mean Abs error: 0.542336 \n","\n","Epoch 578   -------------------------------\n","Train loss: 0.0613364651799202\n","Test set => Mean Squared error: 16.5399, Mean Abs error: 0.542128 \n","\n","Epoch 579   -------------------------------\n","Train loss: 0.061271559447050095\n","Test set => Mean Squared error: 16.5506, Mean Abs error: 0.542289 \n","\n","Epoch 580   -------------------------------\n","Train loss: 0.061150986701250076\n","Test set => Mean Squared error: 16.5523, Mean Abs error: 0.542301 \n","\n","Epoch 581   -------------------------------\n","Train loss: 0.06116531044244766\n","Test set => Mean Squared error: 16.5441, Mean Abs error: 0.542139 \n","\n","Epoch 582   -------------------------------\n","Train loss: 0.061135612428188324\n","Test set => Mean Squared error: 16.5599, Mean Abs error: 0.542381 \n","\n","Epoch 583   -------------------------------\n","Train loss: 0.06099851056933403\n","Test set => Mean Squared error: 16.5692, Mean Abs error: 0.542529 \n","\n","Epoch 584   -------------------------------\n","Train loss: 0.060966528952121735\n","Test set => Mean Squared error: 16.5669, Mean Abs error: 0.542487 \n","\n","Epoch 585   -------------------------------\n","Train loss: 0.06097688525915146\n","Test set => Mean Squared error: 16.5800, Mean Abs error: 0.542703 \n","\n","Epoch 586   -------------------------------\n","Train loss: 0.06082143262028694\n","Test set => Mean Squared error: 16.5926, Mean Abs error: 0.542906 \n","\n","Epoch 587   -------------------------------\n","Train loss: 0.060836344957351685\n","Test set => Mean Squared error: 16.5912, Mean Abs error: 0.542876 \n","\n","Epoch 588   -------------------------------\n","Train loss: 0.06080193445086479\n","Test set => Mean Squared error: 16.6061, Mean Abs error: 0.543114 \n","\n","Epoch 589   -------------------------------\n","Train loss: 0.060670968145132065\n","Test set => Mean Squared error: 16.6234, Mean Abs error: 0.543390 \n","\n","Epoch 590   -------------------------------\n","Train loss: 0.06068653613328934\n","Test set => Mean Squared error: 16.6193, Mean Abs error: 0.543318 \n","\n","Epoch 591   -------------------------------\n","Train loss: 0.06056389585137367\n","Test set => Mean Squared error: 16.6212, Mean Abs error: 0.543350 \n","\n","Epoch 592   -------------------------------\n","Train loss: 0.0605466403067112\n","Test set => Mean Squared error: 16.6316, Mean Abs error: 0.543524 \n","\n","Epoch 593   -------------------------------\n","Train loss: 0.0605572834610939\n","Test set => Mean Squared error: 16.6234, Mean Abs error: 0.543389 \n","\n","Epoch 594   -------------------------------\n","Train loss: 0.06049323454499245\n","Test set => Mean Squared error: 16.6192, Mean Abs error: 0.543332 \n","\n","Epoch 595   -------------------------------\n","Train loss: 0.06041613593697548\n","Test set => Mean Squared error: 16.6178, Mean Abs error: 0.543312 \n","\n","Epoch 596   -------------------------------\n","Train loss: 0.060386158525943756\n","Test set => Mean Squared error: 16.6198, Mean Abs error: 0.543343 \n","\n","Epoch 597   -------------------------------\n","Train loss: 0.06035185977816582\n","Test set => Mean Squared error: 16.6244, Mean Abs error: 0.543422 \n","\n","Epoch 598   -------------------------------\n","Train loss: 0.06037285551428795\n","Test set => Mean Squared error: 16.6231, Mean Abs error: 0.543397 \n","\n","Epoch 599   -------------------------------\n","Train loss: 0.06062433868646622\n","Test set => Mean Squared error: 16.6342, Mean Abs error: 0.543574 \n","\n","Epoch 600   -------------------------------\n","Train loss: 0.06206348538398743\n","Test set => Mean Squared error: 16.6031, Mean Abs error: 0.543050 \n","\n","Epoch 601   -------------------------------\n","Train loss: 0.06675869971513748\n","Test set => Mean Squared error: 16.6033, Mean Abs error: 0.543234 \n","\n","Epoch 602   -------------------------------\n","Train loss: 0.06227152794599533\n","Test set => Mean Squared error: 16.5624, Mean Abs error: 0.542621 \n","\n","Epoch 603   -------------------------------\n","Train loss: 0.06539356708526611\n","Test set => Mean Squared error: 16.5071, Mean Abs error: 0.541634 \n","\n","Epoch 604   -------------------------------\n","Train loss: 0.06400042772293091\n","Test set => Mean Squared error: 16.5332, Mean Abs error: 0.542188 \n","\n","Epoch 605   -------------------------------\n","Train loss: 0.06524014472961426\n","Test set => Mean Squared error: 16.5257, Mean Abs error: 0.542054 \n","\n","Epoch 606   -------------------------------\n","Train loss: 0.06405258178710938\n","Test set => Mean Squared error: 16.4895, Mean Abs error: 0.541397 \n","\n","Epoch 607   -------------------------------\n","Train loss: 0.064212366938591\n","Test set => Mean Squared error: 16.5260, Mean Abs error: 0.541924 \n","\n","Epoch 608   -------------------------------\n","Train loss: 0.0641527771949768\n","Test set => Mean Squared error: 16.5185, Mean Abs error: 0.541708 \n","\n","Epoch 609   -------------------------------\n","Train loss: 0.06258810311555862\n","Test set => Mean Squared error: 16.4826, Mean Abs error: 0.541028 \n","\n","Epoch 610   -------------------------------\n","Train loss: 0.06454499065876007\n","Test set => Mean Squared error: 16.4905, Mean Abs error: 0.541142 \n","\n","Epoch 611   -------------------------------\n","Train loss: 0.061616260558366776\n","Test set => Mean Squared error: 16.4889, Mean Abs error: 0.541092 \n","\n","Epoch 612   -------------------------------\n","Train loss: 0.06416461616754532\n","Test set => Mean Squared error: 16.4810, Mean Abs error: 0.540921 \n","\n","Epoch 613   -------------------------------\n","Train loss: 0.06178836524486542\n","Test set => Mean Squared error: 16.4917, Mean Abs error: 0.541098 \n","\n","Epoch 614   -------------------------------\n","Train loss: 0.06440819799900055\n","Test set => Mean Squared error: 16.4931, Mean Abs error: 0.541050 \n","\n","Epoch 615   -------------------------------\n","Train loss: 0.06457510590553284\n","Test set => Mean Squared error: 16.5326, Mean Abs error: 0.541735 \n","\n","Epoch 616   -------------------------------\n","Train loss: 0.0654212087392807\n","Test set => Mean Squared error: 16.5198, Mean Abs error: 0.541489 \n","\n","Epoch 617   -------------------------------\n","Train loss: 0.0656588077545166\n","Test set => Mean Squared error: 16.5297, Mean Abs error: 0.541691 \n","\n","Epoch 618   -------------------------------\n","Train loss: 0.06303413212299347\n","Test set => Mean Squared error: 16.5555, Mean Abs error: 0.542144 \n","\n","Epoch 619   -------------------------------\n","Train loss: 0.06269913911819458\n","Test set => Mean Squared error: 16.5207, Mean Abs error: 0.541578 \n","\n","Epoch 620   -------------------------------\n","Train loss: 0.06215747073292732\n","Test set => Mean Squared error: 16.5186, Mean Abs error: 0.541548 \n","\n","Epoch 621   -------------------------------\n","Train loss: 0.0627804845571518\n","Test set => Mean Squared error: 16.5575, Mean Abs error: 0.542226 \n","\n","Epoch 622   -------------------------------\n","Train loss: 0.062143709510564804\n","Test set => Mean Squared error: 16.5399, Mean Abs error: 0.541907 \n","\n","Epoch 623   -------------------------------\n","Train loss: 0.06143426150083542\n","Test set => Mean Squared error: 16.5070, Mean Abs error: 0.541315 \n","\n","Epoch 624   -------------------------------\n","Train loss: 0.0625246912240982\n","Test set => Mean Squared error: 16.5392, Mean Abs error: 0.541844 \n","\n","Epoch 625   -------------------------------\n","Train loss: 0.06119284778833389\n","Test set => Mean Squared error: 16.5625, Mean Abs error: 0.542232 \n","\n","Epoch 626   -------------------------------\n","Train loss: 0.061844900250434875\n","Test set => Mean Squared error: 16.5437, Mean Abs error: 0.541916 \n","\n","Epoch 627   -------------------------------\n","Train loss: 0.06105472147464752\n","Test set => Mean Squared error: 16.5581, Mean Abs error: 0.542162 \n","\n","Epoch 628   -------------------------------\n","Train loss: 0.06126156076788902\n","Test set => Mean Squared error: 16.5976, Mean Abs error: 0.542833 \n","\n","Epoch 629   -------------------------------\n","Train loss: 0.06094115227460861\n","Test set => Mean Squared error: 16.6002, Mean Abs error: 0.542871 \n","\n","Epoch 630   -------------------------------\n","Train loss: 0.06075109541416168\n","Test set => Mean Squared error: 16.5862, Mean Abs error: 0.542627 \n","\n","Epoch 631   -------------------------------\n","Train loss: 0.06079155579209328\n","Test set => Mean Squared error: 16.6057, Mean Abs error: 0.542948 \n","\n","Epoch 632   -------------------------------\n","Train loss: 0.06044289469718933\n","Test set => Mean Squared error: 16.6127, Mean Abs error: 0.543067 \n","\n","Epoch 633   -------------------------------\n","Train loss: 0.06046023964881897\n","Test set => Mean Squared error: 16.5962, Mean Abs error: 0.542798 \n","\n","Epoch 634   -------------------------------\n","Train loss: 0.06059381365776062\n","Test set => Mean Squared error: 16.6024, Mean Abs error: 0.542885 \n","\n","Epoch 635   -------------------------------\n","Train loss: 0.060612741857767105\n","Test set => Mean Squared error: 16.6025, Mean Abs error: 0.542887 \n","\n","Epoch 636   -------------------------------\n","Train loss: 0.06046837940812111\n","Test set => Mean Squared error: 16.5883, Mean Abs error: 0.542647 \n","\n","Epoch 637   -------------------------------\n","Train loss: 0.060473255813121796\n","Test set => Mean Squared error: 16.5971, Mean Abs error: 0.542779 \n","\n","Epoch 638   -------------------------------\n","Train loss: 0.06032238155603409\n","Test set => Mean Squared error: 16.6102, Mean Abs error: 0.543008 \n","\n","Epoch 639   -------------------------------\n","Train loss: 0.06035760045051575\n","Test set => Mean Squared error: 16.6043, Mean Abs error: 0.542931 \n","\n","Epoch 640   -------------------------------\n","Train loss: 0.06004854291677475\n","Test set => Mean Squared error: 16.6061, Mean Abs error: 0.542963 \n","\n","Epoch 641   -------------------------------\n","Train loss: 0.060080621391534805\n","Test set => Mean Squared error: 16.6128, Mean Abs error: 0.543064 \n","\n","Epoch 642   -------------------------------\n","Train loss: 0.05996399000287056\n","Test set => Mean Squared error: 16.6175, Mean Abs error: 0.543116 \n","\n","Epoch 643   -------------------------------\n","Train loss: 0.059821967035532\n","Test set => Mean Squared error: 16.6239, Mean Abs error: 0.543214 \n","\n","Epoch 644   -------------------------------\n","Train loss: 0.05980564281344414\n","Test set => Mean Squared error: 16.6319, Mean Abs error: 0.543347 \n","\n","Epoch 645   -------------------------------\n","Train loss: 0.05969814583659172\n","Test set => Mean Squared error: 16.6364, Mean Abs error: 0.543421 \n","\n","Epoch 646   -------------------------------\n","Train loss: 0.05969581380486488\n","Test set => Mean Squared error: 16.6416, Mean Abs error: 0.543512 \n","\n","Epoch 647   -------------------------------\n","Train loss: 0.059736087918281555\n","Test set => Mean Squared error: 16.6420, Mean Abs error: 0.543527 \n","\n","Epoch 648   -------------------------------\n","Train loss: 0.05969284474849701\n","Test set => Mean Squared error: 16.6466, Mean Abs error: 0.543596 \n","\n","Epoch 649   -------------------------------\n","Train loss: 0.05957195907831192\n","Test set => Mean Squared error: 16.6433, Mean Abs error: 0.543546 \n","\n","Epoch 650   -------------------------------\n","Train loss: 0.059529371559619904\n","Test set => Mean Squared error: 16.6367, Mean Abs error: 0.543440 \n","\n","Epoch 651   -------------------------------\n","Train loss: 0.05951220542192459\n","Test set => Mean Squared error: 16.6422, Mean Abs error: 0.543530 \n","\n","Epoch 652   -------------------------------\n","Train loss: 0.059401508420705795\n","Test set => Mean Squared error: 16.6472, Mean Abs error: 0.543621 \n","\n","Epoch 653   -------------------------------\n","Train loss: 0.05937708541750908\n","Test set => Mean Squared error: 16.6448, Mean Abs error: 0.543573 \n","\n","Epoch 654   -------------------------------\n","Train loss: 0.05932724103331566\n","Test set => Mean Squared error: 16.6496, Mean Abs error: 0.543644 \n","\n","Epoch 655   -------------------------------\n","Train loss: 0.059257738292217255\n","Test set => Mean Squared error: 16.6578, Mean Abs error: 0.543775 \n","\n","Epoch 656   -------------------------------\n","Train loss: 0.05929369851946831\n","Test set => Mean Squared error: 16.6579, Mean Abs error: 0.543758 \n","\n","Epoch 657   -------------------------------\n","Train loss: 0.05924675613641739\n","Test set => Mean Squared error: 16.6564, Mean Abs error: 0.543732 \n","\n","Epoch 658   -------------------------------\n","Train loss: 0.0591910257935524\n","Test set => Mean Squared error: 16.6538, Mean Abs error: 0.543693 \n","\n","Epoch 659   -------------------------------\n","Train loss: 0.05926235020160675\n","Test set => Mean Squared error: 16.6436, Mean Abs error: 0.543517 \n","\n","Epoch 660   -------------------------------\n","Train loss: 0.059212230145931244\n","Test set => Mean Squared error: 16.6407, Mean Abs error: 0.543472 \n","\n","Epoch 661   -------------------------------\n","Train loss: 0.0591103732585907\n","Test set => Mean Squared error: 16.6458, Mean Abs error: 0.543562 \n","\n","Epoch 662   -------------------------------\n","Train loss: 0.059126291424036026\n","Test set => Mean Squared error: 16.6398, Mean Abs error: 0.543457 \n","\n","Epoch 663   -------------------------------\n","Train loss: 0.05910881608724594\n","Test set => Mean Squared error: 16.6334, Mean Abs error: 0.543359 \n","\n","Epoch 664   -------------------------------\n","Train loss: 0.059102028608322144\n","Test set => Mean Squared error: 16.6378, Mean Abs error: 0.543428 \n","\n","Epoch 665   -------------------------------\n","Train loss: 0.05903296917676926\n","Test set => Mean Squared error: 16.6430, Mean Abs error: 0.543513 \n","\n","Epoch 666   -------------------------------\n","Train loss: 0.05902458727359772\n","Test set => Mean Squared error: 16.6515, Mean Abs error: 0.543659 \n","\n","Epoch 667   -------------------------------\n","Train loss: 0.059050749987363815\n","Test set => Mean Squared error: 16.6387, Mean Abs error: 0.543432 \n","\n","Epoch 668   -------------------------------\n","Train loss: 0.05918772891163826\n","Test set => Mean Squared error: 16.6497, Mean Abs error: 0.543627 \n","\n","Epoch 669   -------------------------------\n","Train loss: 0.05907474458217621\n","Test set => Mean Squared error: 16.6440, Mean Abs error: 0.543524 \n","\n","Epoch 670   -------------------------------\n","Train loss: 0.05893463268876076\n","Test set => Mean Squared error: 16.6409, Mean Abs error: 0.543459 \n","\n","Epoch 671   -------------------------------\n","Train loss: 0.05900673195719719\n","Test set => Mean Squared error: 16.6521, Mean Abs error: 0.543651 \n","\n","Epoch 672   -------------------------------\n","Train loss: 0.058992981910705566\n","Test set => Mean Squared error: 16.6472, Mean Abs error: 0.543557 \n","\n","Epoch 673   -------------------------------\n","Train loss: 0.05888216197490692\n","Test set => Mean Squared error: 16.6446, Mean Abs error: 0.543505 \n","\n","Epoch 674   -------------------------------\n","Train loss: 0.05889271944761276\n","Test set => Mean Squared error: 16.6530, Mean Abs error: 0.543652 \n","\n","Epoch 675   -------------------------------\n","Train loss: 0.058905746787786484\n","Test set => Mean Squared error: 16.6479, Mean Abs error: 0.543563 \n","\n","Epoch 676   -------------------------------\n","Train loss: 0.05880691856145859\n","Test set => Mean Squared error: 16.6502, Mean Abs error: 0.543601 \n","\n","Epoch 677   -------------------------------\n","Train loss: 0.058789391070604324\n","Test set => Mean Squared error: 16.6605, Mean Abs error: 0.543776 \n","\n","Epoch 678   -------------------------------\n","Train loss: 0.058842550963163376\n","Test set => Mean Squared error: 16.6528, Mean Abs error: 0.543640 \n","\n","Epoch 679   -------------------------------\n","Train loss: 0.05884487181901932\n","Test set => Mean Squared error: 16.6623, Mean Abs error: 0.543804 \n","\n","Epoch 680   -------------------------------\n","Train loss: 0.05879340320825577\n","Test set => Mean Squared error: 16.6682, Mean Abs error: 0.543898 \n","\n","Epoch 681   -------------------------------\n","Train loss: 0.05871903896331787\n","Test set => Mean Squared error: 16.6685, Mean Abs error: 0.543891 \n","\n","Epoch 682   -------------------------------\n","Train loss: 0.05874121934175491\n","Test set => Mean Squared error: 16.6772, Mean Abs error: 0.544046 \n","\n","Epoch 683   -------------------------------\n","Train loss: 0.05876653641462326\n","Test set => Mean Squared error: 16.6718, Mean Abs error: 0.543943 \n","\n","Epoch 684   -------------------------------\n","Train loss: 0.05868189036846161\n","Test set => Mean Squared error: 16.6784, Mean Abs error: 0.544049 \n","\n","Epoch 685   -------------------------------\n","Train loss: 0.05866372585296631\n","Test set => Mean Squared error: 16.6808, Mean Abs error: 0.544098 \n","\n","Epoch 686   -------------------------------\n","Train loss: 0.05876212939620018\n","Test set => Mean Squared error: 16.6717, Mean Abs error: 0.543929 \n","\n","Epoch 687   -------------------------------\n","Train loss: 0.058781784027814865\n","Test set => Mean Squared error: 16.6742, Mean Abs error: 0.543982 \n","\n","Epoch 688   -------------------------------\n","Train loss: 0.05871262401342392\n","Test set => Mean Squared error: 16.6822, Mean Abs error: 0.544116 \n","\n","Epoch 689   -------------------------------\n","Train loss: 0.05873174965381622\n","Test set => Mean Squared error: 16.6759, Mean Abs error: 0.544007 \n","\n","Epoch 690   -------------------------------\n","Train loss: 0.05871150642633438\n","Test set => Mean Squared error: 16.6801, Mean Abs error: 0.544083 \n","\n","Epoch 691   -------------------------------\n","Train loss: 0.0585472509264946\n","Test set => Mean Squared error: 16.6844, Mean Abs error: 0.544149 \n","\n","Epoch 692   -------------------------------\n","Train loss: 0.05858121067285538\n","Test set => Mean Squared error: 16.6829, Mean Abs error: 0.544116 \n","\n","Epoch 693   -------------------------------\n","Train loss: 0.05860495567321777\n","Test set => Mean Squared error: 16.6854, Mean Abs error: 0.544153 \n","\n","Epoch 694   -------------------------------\n","Train loss: 0.05852983146905899\n","Test set => Mean Squared error: 16.6888, Mean Abs error: 0.544209 \n","\n","Epoch 695   -------------------------------\n","Train loss: 0.058498065918684006\n","Test set => Mean Squared error: 16.6881, Mean Abs error: 0.544181 \n","\n","Epoch 696   -------------------------------\n","Train loss: 0.058479104191064835\n","Test set => Mean Squared error: 16.6895, Mean Abs error: 0.544205 \n","\n","Epoch 697   -------------------------------\n","Train loss: 0.0584237203001976\n","Test set => Mean Squared error: 16.6935, Mean Abs error: 0.544274 \n","\n","Epoch 698   -------------------------------\n","Train loss: 0.05844078212976456\n","Test set => Mean Squared error: 16.6889, Mean Abs error: 0.544182 \n","\n","Epoch 699   -------------------------------\n","Train loss: 0.0584171898663044\n","Test set => Mean Squared error: 16.6907, Mean Abs error: 0.544219 \n","\n","Epoch 700   -------------------------------\n","Train loss: 0.058351486921310425\n","Test set => Mean Squared error: 16.6893, Mean Abs error: 0.544200 \n","\n","Epoch 701   -------------------------------\n","Train loss: 0.058404341340065\n","Test set => Mean Squared error: 16.6736, Mean Abs error: 0.543923 \n","\n","Epoch 702   -------------------------------\n","Train loss: 0.05850628390908241\n","Test set => Mean Squared error: 16.6716, Mean Abs error: 0.543899 \n","\n","Epoch 703   -------------------------------\n","Train loss: 0.0583660826086998\n","Test set => Mean Squared error: 16.6690, Mean Abs error: 0.543860 \n","\n","Epoch 704   -------------------------------\n","Train loss: 0.05840078741312027\n","Test set => Mean Squared error: 16.6595, Mean Abs error: 0.543686 \n","\n","Epoch 705   -------------------------------\n","Train loss: 0.05841999873518944\n","Test set => Mean Squared error: 16.6647, Mean Abs error: 0.543777 \n","\n","Epoch 706   -------------------------------\n","Train loss: 0.05836208537220955\n","Test set => Mean Squared error: 16.6664, Mean Abs error: 0.543804 \n","\n","Epoch 707   -------------------------------\n","Train loss: 0.058327630162239075\n","Test set => Mean Squared error: 16.6585, Mean Abs error: 0.543661 \n","\n","Epoch 708   -------------------------------\n","Train loss: 0.05838469788432121\n","Test set => Mean Squared error: 16.6618, Mean Abs error: 0.543725 \n","\n","Epoch 709   -------------------------------\n","Train loss: 0.058362334966659546\n","Test set => Mean Squared error: 16.6582, Mean Abs error: 0.543655 \n","\n","Epoch 710   -------------------------------\n","Train loss: 0.058243121951818466\n","Test set => Mean Squared error: 16.6533, Mean Abs error: 0.543572 \n","\n","Epoch 711   -------------------------------\n","Train loss: 0.058279849588871\n","Test set => Mean Squared error: 16.6521, Mean Abs error: 0.543558 \n","\n","Epoch 712   -------------------------------\n","Train loss: 0.05826769396662712\n","Test set => Mean Squared error: 16.6532, Mean Abs error: 0.543565 \n","\n","Epoch 713   -------------------------------\n","Train loss: 0.05821366235613823\n","Test set => Mean Squared error: 16.6606, Mean Abs error: 0.543688 \n","\n","Epoch 714   -------------------------------\n","Train loss: 0.05818449705839157\n","Test set => Mean Squared error: 16.6649, Mean Abs error: 0.543765 \n","\n","Epoch 715   -------------------------------\n","Train loss: 0.05822063609957695\n","Test set => Mean Squared error: 16.6617, Mean Abs error: 0.543696 \n","\n","Epoch 716   -------------------------------\n","Train loss: 0.05822332575917244\n","Test set => Mean Squared error: 16.6772, Mean Abs error: 0.543957 \n","\n","Epoch 717   -------------------------------\n","Train loss: 0.058269452303647995\n","Test set => Mean Squared error: 16.6842, Mean Abs error: 0.544081 \n","\n","Epoch 718   -------------------------------\n","Train loss: 0.05837135761976242\n","Test set => Mean Squared error: 16.6809, Mean Abs error: 0.544006 \n","\n","Epoch 719   -------------------------------\n","Train loss: 0.05858846381306648\n","Test set => Mean Squared error: 16.6877, Mean Abs error: 0.544135 \n","\n","Epoch 720   -------------------------------\n","Train loss: 0.05895664915442467\n","Test set => Mean Squared error: 16.6843, Mean Abs error: 0.544051 \n","\n","Epoch 721   -------------------------------\n","Train loss: 0.05930349975824356\n","Test set => Mean Squared error: 16.6816, Mean Abs error: 0.544027 \n","\n","Epoch 722   -------------------------------\n","Train loss: 0.05985862389206886\n","Test set => Mean Squared error: 16.6841, Mean Abs error: 0.544057 \n","\n","Epoch 723   -------------------------------\n","Train loss: 0.06013346463441849\n","Test set => Mean Squared error: 16.6644, Mean Abs error: 0.543723 \n","\n","Epoch 724   -------------------------------\n","Train loss: 0.05979570746421814\n","Test set => Mean Squared error: 16.6763, Mean Abs error: 0.543929 \n","\n","Epoch 725   -------------------------------\n","Train loss: 0.05853746831417084\n","Test set => Mean Squared error: 16.6791, Mean Abs error: 0.544002 \n","\n","Epoch 726   -------------------------------\n","Train loss: 0.05884303152561188\n","Test set => Mean Squared error: 16.6346, Mean Abs error: 0.543223 \n","\n","Epoch 727   -------------------------------\n","Train loss: 0.05947069823741913\n","Test set => Mean Squared error: 16.6431, Mean Abs error: 0.543383 \n","\n","Epoch 728   -------------------------------\n","Train loss: 0.05855725705623627\n","Test set => Mean Squared error: 16.6485, Mean Abs error: 0.543480 \n","\n","Epoch 729   -------------------------------\n","Train loss: 0.0590861514210701\n","Test set => Mean Squared error: 16.6164, Mean Abs error: 0.542906 \n","\n","Epoch 730   -------------------------------\n","Train loss: 0.05883300304412842\n","Test set => Mean Squared error: 16.6178, Mean Abs error: 0.542939 \n","\n","Epoch 731   -------------------------------\n","Train loss: 0.05840739607810974\n","Test set => Mean Squared error: 16.6339, Mean Abs error: 0.543212 \n","\n","Epoch 732   -------------------------------\n","Train loss: 0.05864020809531212\n","Test set => Mean Squared error: 16.6166, Mean Abs error: 0.542896 \n","\n","Epoch 733   -------------------------------\n","Train loss: 0.05860280618071556\n","Test set => Mean Squared error: 16.6133, Mean Abs error: 0.542855 \n","\n","Epoch 734   -------------------------------\n","Train loss: 0.058517854660749435\n","Test set => Mean Squared error: 16.6138, Mean Abs error: 0.542849 \n","\n","Epoch 735   -------------------------------\n","Train loss: 0.058304183185100555\n","Test set => Mean Squared error: 16.6072, Mean Abs error: 0.542724 \n","\n","Epoch 736   -------------------------------\n","Train loss: 0.058405980467796326\n","Test set => Mean Squared error: 16.6103, Mean Abs error: 0.542802 \n","\n","Epoch 737   -------------------------------\n","Train loss: 0.058389779180288315\n","Test set => Mean Squared error: 16.6166, Mean Abs error: 0.542895 \n","\n","Epoch 738   -------------------------------\n","Train loss: 0.05832973122596741\n","Test set => Mean Squared error: 16.6234, Mean Abs error: 0.543013 \n","\n","Epoch 739   -------------------------------\n","Train loss: 0.05848459526896477\n","Test set => Mean Squared error: 16.6240, Mean Abs error: 0.543031 \n","\n","Epoch 740   -------------------------------\n","Train loss: 0.058278605341911316\n","Test set => Mean Squared error: 16.6318, Mean Abs error: 0.543157 \n","\n","Epoch 741   -------------------------------\n","Train loss: 0.05816817283630371\n","Test set => Mean Squared error: 16.6341, Mean Abs error: 0.543173 \n","\n","Epoch 742   -------------------------------\n","Train loss: 0.058211833238601685\n","Test set => Mean Squared error: 16.6284, Mean Abs error: 0.543086 \n","\n","Epoch 743   -------------------------------\n","Train loss: 0.058208584785461426\n","Test set => Mean Squared error: 16.6385, Mean Abs error: 0.543251 \n","\n","Epoch 744   -------------------------------\n","Train loss: 0.058267343789339066\n","Test set => Mean Squared error: 16.6383, Mean Abs error: 0.543219 \n","\n","Epoch 745   -------------------------------\n","Train loss: 0.05830290913581848\n","Test set => Mean Squared error: 16.6549, Mean Abs error: 0.543513 \n","\n","Epoch 746   -------------------------------\n","Train loss: 0.05814329907298088\n","Test set => Mean Squared error: 16.6611, Mean Abs error: 0.543616 \n","\n","Epoch 747   -------------------------------\n","Train loss: 0.05813683196902275\n","Test set => Mean Squared error: 16.6728, Mean Abs error: 0.543790 \n","\n","Epoch 748   -------------------------------\n","Train loss: 0.0581185445189476\n","Test set => Mean Squared error: 16.6776, Mean Abs error: 0.543879 \n","\n","Epoch 749   -------------------------------\n","Train loss: 0.05793146789073944\n","Test set => Mean Squared error: 16.6775, Mean Abs error: 0.543885 \n","\n","Epoch 750   -------------------------------\n","Train loss: 0.057907670736312866\n","Test set => Mean Squared error: 16.6754, Mean Abs error: 0.543833 \n","\n","Epoch 751   -------------------------------\n","Train loss: 0.057909127324819565\n","Test set => Mean Squared error: 16.6821, Mean Abs error: 0.543947 \n","\n","Epoch 752   -------------------------------\n","Train loss: 0.05787390470504761\n","Test set => Mean Squared error: 16.6902, Mean Abs error: 0.544087 \n","\n","Epoch 753   -------------------------------\n","Train loss: 0.05789848044514656\n","Test set => Mean Squared error: 16.6896, Mean Abs error: 0.544058 \n","\n","Epoch 754   -------------------------------\n","Train loss: 0.057942699640989304\n","Test set => Mean Squared error: 16.7078, Mean Abs error: 0.544365 \n","\n","Epoch 755   -------------------------------\n","Train loss: 0.057881198823451996\n","Test set => Mean Squared error: 16.6990, Mean Abs error: 0.544212 \n","\n","Epoch 756   -------------------------------\n","Train loss: 0.05778312683105469\n","Test set => Mean Squared error: 16.7044, Mean Abs error: 0.544309 \n","\n","Epoch 757   -------------------------------\n","Train loss: 0.057735759764909744\n","Test set => Mean Squared error: 16.7039, Mean Abs error: 0.544303 \n","\n","Epoch 758   -------------------------------\n","Train loss: 0.05769824981689453\n","Test set => Mean Squared error: 16.7029, Mean Abs error: 0.544281 \n","\n","Epoch 759   -------------------------------\n","Train loss: 0.05773147940635681\n","Test set => Mean Squared error: 16.7120, Mean Abs error: 0.544435 \n","\n","Epoch 760   -------------------------------\n","Train loss: 0.05777626112103462\n","Test set => Mean Squared error: 16.7086, Mean Abs error: 0.544371 \n","\n","Epoch 761   -------------------------------\n","Train loss: 0.057888589799404144\n","Test set => Mean Squared error: 16.7185, Mean Abs error: 0.544538 \n","\n","Epoch 762   -------------------------------\n","Train loss: 0.05811895802617073\n","Test set => Mean Squared error: 16.7055, Mean Abs error: 0.544308 \n","\n","Epoch 763   -------------------------------\n","Train loss: 0.058333318680524826\n","Test set => Mean Squared error: 16.7152, Mean Abs error: 0.544484 \n","\n","Epoch 764   -------------------------------\n","Train loss: 0.05858787149190903\n","Test set => Mean Squared error: 16.7027, Mean Abs error: 0.544262 \n","\n","Epoch 765   -------------------------------\n","Train loss: 0.058583732694387436\n","Test set => Mean Squared error: 16.7225, Mean Abs error: 0.544598 \n","\n","Epoch 766   -------------------------------\n","Train loss: 0.05840336158871651\n","Test set => Mean Squared error: 16.7064, Mean Abs error: 0.544320 \n","\n","Epoch 767   -------------------------------\n","Train loss: 0.058006949722766876\n","Test set => Mean Squared error: 16.7178, Mean Abs error: 0.544519 \n","\n","Epoch 768   -------------------------------\n","Train loss: 0.05764405429363251\n","Test set => Mean Squared error: 16.7101, Mean Abs error: 0.544380 \n","\n","Epoch 769   -------------------------------\n","Train loss: 0.05763838812708855\n","Test set => Mean Squared error: 16.7018, Mean Abs error: 0.544241 \n","\n","Epoch 770   -------------------------------\n","Train loss: 0.0578201524913311\n","Test set => Mean Squared error: 16.7052, Mean Abs error: 0.544298 \n","\n","Epoch 771   -------------------------------\n","Train loss: 0.05798264592885971\n","Test set => Mean Squared error: 16.6827, Mean Abs error: 0.543906 \n","\n","Epoch 772   -------------------------------\n","Train loss: 0.05812791734933853\n","Test set => Mean Squared error: 16.7027, Mean Abs error: 0.544263 \n","\n","Epoch 773   -------------------------------\n","Train loss: 0.058392032980918884\n","Test set => Mean Squared error: 16.6662, Mean Abs error: 0.543620 \n","\n","Epoch 774   -------------------------------\n","Train loss: 0.05875369533896446\n","Test set => Mean Squared error: 16.6901, Mean Abs error: 0.544066 \n","\n","Epoch 775   -------------------------------\n","Train loss: 0.058455951511859894\n","Test set => Mean Squared error: 16.6923, Mean Abs error: 0.544110 \n","\n","Epoch 776   -------------------------------\n","Train loss: 0.05897810310125351\n","Test set => Mean Squared error: 16.6728, Mean Abs error: 0.543766 \n","\n","Epoch 777   -------------------------------\n","Train loss: 0.05918968841433525\n","Test set => Mean Squared error: 16.7082, Mean Abs error: 0.544364 \n","\n","Epoch 778   -------------------------------\n","Train loss: 0.05900963023304939\n","Test set => Mean Squared error: 16.7118, Mean Abs error: 0.544406 \n","\n","Epoch 779   -------------------------------\n","Train loss: 0.057961106300354004\n","Test set => Mean Squared error: 16.7063, Mean Abs error: 0.544285 \n","\n","Epoch 780   -------------------------------\n","Train loss: 0.05807594209909439\n","Test set => Mean Squared error: 16.7187, Mean Abs error: 0.544491 \n","\n","Epoch 781   -------------------------------\n","Train loss: 0.057987283915281296\n","Test set => Mean Squared error: 16.7205, Mean Abs error: 0.544508 \n","\n","Epoch 782   -------------------------------\n","Train loss: 0.05846911296248436\n","Test set => Mean Squared error: 16.7286, Mean Abs error: 0.544653 \n","\n","Epoch 783   -------------------------------\n","Train loss: 0.05845886468887329\n","Test set => Mean Squared error: 16.7095, Mean Abs error: 0.544343 \n","\n","Epoch 784   -------------------------------\n","Train loss: 0.058112360537052155\n","Test set => Mean Squared error: 16.6901, Mean Abs error: 0.544019 \n","\n","Epoch 785   -------------------------------\n","Train loss: 0.0580400750041008\n","Test set => Mean Squared error: 16.6819, Mean Abs error: 0.543885 \n","\n","Epoch 786   -------------------------------\n","Train loss: 0.0577453151345253\n","Test set => Mean Squared error: 16.6754, Mean Abs error: 0.543769 \n","\n","Epoch 787   -------------------------------\n","Train loss: 0.057668112218379974\n","Test set => Mean Squared error: 16.6787, Mean Abs error: 0.543827 \n","\n","Epoch 788   -------------------------------\n","Train loss: 0.057541679590940475\n","Test set => Mean Squared error: 16.6861, Mean Abs error: 0.543939 \n","\n","Epoch 789   -------------------------------\n","Train loss: 0.05762940272688866\n","Test set => Mean Squared error: 16.6945, Mean Abs error: 0.544063 \n","\n","Epoch 790   -------------------------------\n","Train loss: 0.0576716810464859\n","Test set => Mean Squared error: 16.6965, Mean Abs error: 0.544103 \n","\n","Epoch 791   -------------------------------\n","Train loss: 0.05761411786079407\n","Test set => Mean Squared error: 16.7009, Mean Abs error: 0.544166 \n","\n","Epoch 792   -------------------------------\n","Train loss: 0.05747687816619873\n","Test set => Mean Squared error: 16.6971, Mean Abs error: 0.544081 \n","\n","Epoch 793   -------------------------------\n","Train loss: 0.05746285989880562\n","Test set => Mean Squared error: 16.6995, Mean Abs error: 0.544125 \n","\n","Epoch 794   -------------------------------\n","Train loss: 0.05743153393268585\n","Test set => Mean Squared error: 16.7016, Mean Abs error: 0.544158 \n","\n","Epoch 795   -------------------------------\n","Train loss: 0.05747612565755844\n","Test set => Mean Squared error: 16.6990, Mean Abs error: 0.544108 \n","\n","Epoch 796   -------------------------------\n","Train loss: 0.0574997179210186\n","Test set => Mean Squared error: 16.7040, Mean Abs error: 0.544200 \n","\n","Epoch 797   -------------------------------\n","Train loss: 0.05760668218135834\n","Test set => Mean Squared error: 16.6961, Mean Abs error: 0.544061 \n","\n","Epoch 798   -------------------------------\n","Train loss: 0.05754605680704117\n","Test set => Mean Squared error: 16.7033, Mean Abs error: 0.544179 \n","\n","Epoch 799   -------------------------------\n","Train loss: 0.05751703679561615\n","Test set => Mean Squared error: 16.7019, Mean Abs error: 0.544150 \n","\n","Epoch 800   -------------------------------\n","Train loss: 0.057432617992162704\n","Test set => Mean Squared error: 16.6997, Mean Abs error: 0.544097 \n","\n","Epoch 801   -------------------------------\n","Train loss: 0.057383179664611816\n","Test set => Mean Squared error: 16.7007, Mean Abs error: 0.544121 \n","\n","Epoch 802   -------------------------------\n","Train loss: 0.05731328949332237\n","Test set => Mean Squared error: 16.6971, Mean Abs error: 0.544060 \n","\n","Epoch 803   -------------------------------\n","Train loss: 0.0573275201022625\n","Test set => Mean Squared error: 16.6971, Mean Abs error: 0.544052 \n","\n","Epoch 804   -------------------------------\n","Train loss: 0.05735271796584129\n","Test set => Mean Squared error: 16.7026, Mean Abs error: 0.544151 \n","\n","Epoch 805   -------------------------------\n","Train loss: 0.05748884007334709\n","Test set => Mean Squared error: 16.6933, Mean Abs error: 0.543986 \n","\n","Epoch 806   -------------------------------\n","Train loss: 0.05762729048728943\n","Test set => Mean Squared error: 16.6881, Mean Abs error: 0.543907 \n","\n","Epoch 807   -------------------------------\n","Train loss: 0.05766801908612251\n","Test set => Mean Squared error: 16.6909, Mean Abs error: 0.543953 \n","\n","Epoch 808   -------------------------------\n","Train loss: 0.05770810693502426\n","Test set => Mean Squared error: 16.6829, Mean Abs error: 0.543811 \n","\n","Epoch 809   -------------------------------\n","Train loss: 0.057716015726327896\n","Test set => Mean Squared error: 16.6986, Mean Abs error: 0.544077 \n","\n","Epoch 810   -------------------------------\n","Train loss: 0.057611335068941116\n","Test set => Mean Squared error: 16.6930, Mean Abs error: 0.543973 \n","\n","Epoch 811   -------------------------------\n","Train loss: 0.057421665638685226\n","Test set => Mean Squared error: 16.6918, Mean Abs error: 0.543950 \n","\n","Epoch 812   -------------------------------\n","Train loss: 0.05727369338274002\n","Test set => Mean Squared error: 16.6929, Mean Abs error: 0.543971 \n","\n","Epoch 813   -------------------------------\n","Train loss: 0.05728147178888321\n","Test set => Mean Squared error: 16.6851, Mean Abs error: 0.543825 \n","\n","Epoch 814   -------------------------------\n","Train loss: 0.05735156312584877\n","Test set => Mean Squared error: 16.6934, Mean Abs error: 0.543973 \n","\n","Epoch 815   -------------------------------\n","Train loss: 0.05751757323741913\n","Test set => Mean Squared error: 16.6877, Mean Abs error: 0.543869 \n","\n","Epoch 816   -------------------------------\n","Train loss: 0.057646557688713074\n","Test set => Mean Squared error: 16.6965, Mean Abs error: 0.544010 \n","\n","Epoch 817   -------------------------------\n","Train loss: 0.05778563395142555\n","Test set => Mean Squared error: 16.6883, Mean Abs error: 0.543884 \n","\n","Epoch 818   -------------------------------\n","Train loss: 0.05772090330719948\n","Test set => Mean Squared error: 16.6894, Mean Abs error: 0.543898 \n","\n","Epoch 819   -------------------------------\n","Train loss: 0.05740329623222351\n","Test set => Mean Squared error: 16.6876, Mean Abs error: 0.543860 \n","\n","Epoch 820   -------------------------------\n","Train loss: 0.05715372413396835\n","Test set => Mean Squared error: 16.6932, Mean Abs error: 0.543955 \n","\n","Epoch 821   -------------------------------\n","Train loss: 0.057172130793333054\n","Test set => Mean Squared error: 16.6993, Mean Abs error: 0.544040 \n","\n","Epoch 822   -------------------------------\n","Train loss: 0.0572718046605587\n","Test set => Mean Squared error: 16.6973, Mean Abs error: 0.544003 \n","\n","Epoch 823   -------------------------------\n","Train loss: 0.057368673384189606\n","Test set => Mean Squared error: 16.7013, Mean Abs error: 0.544077 \n","\n","Epoch 824   -------------------------------\n","Train loss: 0.05737680941820145\n","Test set => Mean Squared error: 16.6928, Mean Abs error: 0.543927 \n","\n","Epoch 825   -------------------------------\n","Train loss: 0.057275913655757904\n","Test set => Mean Squared error: 16.6945, Mean Abs error: 0.543967 \n","\n","Epoch 826   -------------------------------\n","Train loss: 0.05714108794927597\n","Test set => Mean Squared error: 16.6911, Mean Abs error: 0.543914 \n","\n","Epoch 827   -------------------------------\n","Train loss: 0.057088546454906464\n","Test set => Mean Squared error: 16.6925, Mean Abs error: 0.543928 \n","\n","Epoch 828   -------------------------------\n","Train loss: 0.057101573795080185\n","Test set => Mean Squared error: 16.6962, Mean Abs error: 0.543990 \n","\n","Epoch 829   -------------------------------\n","Train loss: 0.05713670328259468\n","Test set => Mean Squared error: 16.6981, Mean Abs error: 0.544019 \n","\n","Epoch 830   -------------------------------\n","Train loss: 0.05719592422246933\n","Test set => Mean Squared error: 16.7035, Mean Abs error: 0.544101 \n","\n","Epoch 831   -------------------------------\n","Train loss: 0.057209454476833344\n","Test set => Mean Squared error: 16.7061, Mean Abs error: 0.544148 \n","\n","Epoch 832   -------------------------------\n","Train loss: 0.05716010183095932\n","Test set => Mean Squared error: 16.7115, Mean Abs error: 0.544238 \n","\n","Epoch 833   -------------------------------\n","Train loss: 0.057079240679740906\n","Test set => Mean Squared error: 16.7133, Mean Abs error: 0.544255 \n","\n","Epoch 834   -------------------------------\n","Train loss: 0.0570293590426445\n","Test set => Mean Squared error: 16.7169, Mean Abs error: 0.544321 \n","\n","Epoch 835   -------------------------------\n","Train loss: 0.05699333921074867\n","Test set => Mean Squared error: 16.7129, Mean Abs error: 0.544245 \n","\n","Epoch 836   -------------------------------\n","Train loss: 0.05701170489192009\n","Test set => Mean Squared error: 16.7186, Mean Abs error: 0.544341 \n","\n","Epoch 837   -------------------------------\n","Train loss: 0.05698500573635101\n","Test set => Mean Squared error: 16.7233, Mean Abs error: 0.544424 \n","\n","Epoch 838   -------------------------------\n","Train loss: 0.057009294629096985\n","Test set => Mean Squared error: 16.7184, Mean Abs error: 0.544333 \n","\n","Epoch 839   -------------------------------\n","Train loss: 0.05703531578183174\n","Test set => Mean Squared error: 16.7254, Mean Abs error: 0.544460 \n","\n","Epoch 840   -------------------------------\n","Train loss: 0.05709608644247055\n","Test set => Mean Squared error: 16.7206, Mean Abs error: 0.544362 \n","\n","Epoch 841   -------------------------------\n","Train loss: 0.05705215036869049\n","Test set => Mean Squared error: 16.7282, Mean Abs error: 0.544499 \n","\n","Epoch 842   -------------------------------\n","Train loss: 0.05705688148736954\n","Test set => Mean Squared error: 16.7278, Mean Abs error: 0.544483 \n","\n","Epoch 843   -------------------------------\n","Train loss: 0.057033658027648926\n","Test set => Mean Squared error: 16.7235, Mean Abs error: 0.544410 \n","\n","Epoch 844   -------------------------------\n","Train loss: 0.05708562210202217\n","Test set => Mean Squared error: 16.7268, Mean Abs error: 0.544473 \n","\n","Epoch 845   -------------------------------\n","Train loss: 0.057038646191358566\n","Test set => Mean Squared error: 16.7236, Mean Abs error: 0.544407 \n","\n","Epoch 846   -------------------------------\n","Train loss: 0.056935086846351624\n","Test set => Mean Squared error: 16.7189, Mean Abs error: 0.544327 \n","\n","Epoch 847   -------------------------------\n","Train loss: 0.05688415840268135\n","Test set => Mean Squared error: 16.7146, Mean Abs error: 0.544251 \n","\n","Epoch 848   -------------------------------\n","Train loss: 0.0568901002407074\n","Test set => Mean Squared error: 16.7156, Mean Abs error: 0.544261 \n","\n","Epoch 849   -------------------------------\n","Train loss: 0.05688756704330444\n","Test set => Mean Squared error: 16.7120, Mean Abs error: 0.544199 \n","\n","Epoch 850   -------------------------------\n","Train loss: 0.056898083537817\n","Test set => Mean Squared error: 16.7171, Mean Abs error: 0.544286 \n","\n","Epoch 851   -------------------------------\n","Train loss: 0.05691172182559967\n","Test set => Mean Squared error: 16.7165, Mean Abs error: 0.544261 \n","\n","Epoch 852   -------------------------------\n","Train loss: 0.056924968957901\n","Test set => Mean Squared error: 16.7244, Mean Abs error: 0.544399 \n","\n","Epoch 853   -------------------------------\n","Train loss: 0.05696532875299454\n","Test set => Mean Squared error: 16.7291, Mean Abs error: 0.544467 \n","\n","Epoch 854   -------------------------------\n","Train loss: 0.056963395327329636\n","Test set => Mean Squared error: 16.7328, Mean Abs error: 0.544526 \n","\n","Epoch 855   -------------------------------\n","Train loss: 0.05695774406194687\n","Test set => Mean Squared error: 16.7281, Mean Abs error: 0.544451 \n","\n","Epoch 856   -------------------------------\n","Train loss: 0.05691365525126457\n","Test set => Mean Squared error: 16.7279, Mean Abs error: 0.544446 \n","\n","Epoch 857   -------------------------------\n","Train loss: 0.05685244873166084\n","Test set => Mean Squared error: 16.7229, Mean Abs error: 0.544354 \n","\n","Epoch 858   -------------------------------\n","Train loss: 0.05683168023824692\n","Test set => Mean Squared error: 16.7243, Mean Abs error: 0.544392 \n","\n","Epoch 859   -------------------------------\n","Train loss: 0.056856393814086914\n","Test set => Mean Squared error: 16.7204, Mean Abs error: 0.544312 \n","\n","Epoch 860   -------------------------------\n","Train loss: 0.056854523718357086\n","Test set => Mean Squared error: 16.7244, Mean Abs error: 0.544386 \n","\n","Epoch 861   -------------------------------\n","Train loss: 0.05680941790342331\n","Test set => Mean Squared error: 16.7208, Mean Abs error: 0.544323 \n","\n","Epoch 862   -------------------------------\n","Train loss: 0.056837745010852814\n","Test set => Mean Squared error: 16.7225, Mean Abs error: 0.544345 \n","\n","Epoch 863   -------------------------------\n","Train loss: 0.05682850256562233\n","Test set => Mean Squared error: 16.7243, Mean Abs error: 0.544376 \n","\n","Epoch 864   -------------------------------\n","Train loss: 0.05680027976632118\n","Test set => Mean Squared error: 16.7257, Mean Abs error: 0.544404 \n","\n","Epoch 865   -------------------------------\n","Train loss: 0.05677870661020279\n","Test set => Mean Squared error: 16.7246, Mean Abs error: 0.544368 \n","\n","Epoch 866   -------------------------------\n","Train loss: 0.05678601562976837\n","Test set => Mean Squared error: 16.7307, Mean Abs error: 0.544483 \n","\n","Epoch 867   -------------------------------\n","Train loss: 0.056798551231622696\n","Test set => Mean Squared error: 16.7255, Mean Abs error: 0.544386 \n","\n","Epoch 868   -------------------------------\n","Train loss: 0.05680491775274277\n","Test set => Mean Squared error: 16.7282, Mean Abs error: 0.544434 \n","\n","Epoch 869   -------------------------------\n","Train loss: 0.05685947835445404\n","Test set => Mean Squared error: 16.7217, Mean Abs error: 0.544322 \n","\n","Epoch 870   -------------------------------\n","Train loss: 0.05689111351966858\n","Test set => Mean Squared error: 16.7230, Mean Abs error: 0.544345 \n","\n","Epoch 871   -------------------------------\n","Train loss: 0.056863974779844284\n","Test set => Mean Squared error: 16.7215, Mean Abs error: 0.544309 \n","\n","Epoch 872   -------------------------------\n","Train loss: 0.05685766041278839\n","Test set => Mean Squared error: 16.7256, Mean Abs error: 0.544392 \n","\n","Epoch 873   -------------------------------\n","Train loss: 0.0569697767496109\n","Test set => Mean Squared error: 16.7178, Mean Abs error: 0.544238 \n","\n","Epoch 874   -------------------------------\n","Train loss: 0.057105325162410736\n","Test set => Mean Squared error: 16.7288, Mean Abs error: 0.544443 \n","\n","Epoch 875   -------------------------------\n","Train loss: 0.05735988914966583\n","Test set => Mean Squared error: 16.7124, Mean Abs error: 0.544148 \n","\n","Epoch 876   -------------------------------\n","Train loss: 0.05761898681521416\n","Test set => Mean Squared error: 16.7230, Mean Abs error: 0.544347 \n","\n","Epoch 877   -------------------------------\n","Train loss: 0.057945117354393005\n","Test set => Mean Squared error: 16.7124, Mean Abs error: 0.544142 \n","\n","Epoch 878   -------------------------------\n","Train loss: 0.0582180954515934\n","Test set => Mean Squared error: 16.7173, Mean Abs error: 0.544248 \n","\n","Epoch 879   -------------------------------\n","Train loss: 0.05847397446632385\n","Test set => Mean Squared error: 16.7059, Mean Abs error: 0.544025 \n","\n","Epoch 880   -------------------------------\n","Train loss: 0.058172017335891724\n","Test set => Mean Squared error: 16.7118, Mean Abs error: 0.544143 \n","\n","Epoch 881   -------------------------------\n","Train loss: 0.057497862726449966\n","Test set => Mean Squared error: 16.7042, Mean Abs error: 0.543993 \n","\n","Epoch 882   -------------------------------\n","Train loss: 0.05685834959149361\n","Test set => Mean Squared error: 16.7106, Mean Abs error: 0.544099 \n","\n","Epoch 883   -------------------------------\n","Train loss: 0.05670258775353432\n","Test set => Mean Squared error: 16.7030, Mean Abs error: 0.543978 \n","\n","Epoch 884   -------------------------------\n","Train loss: 0.05697767436504364\n","Test set => Mean Squared error: 16.6880, Mean Abs error: 0.543713 \n","\n","Epoch 885   -------------------------------\n","Train loss: 0.057408593595027924\n","Test set => Mean Squared error: 16.6861, Mean Abs error: 0.543692 \n","\n","Epoch 886   -------------------------------\n","Train loss: 0.05766008421778679\n","Test set => Mean Squared error: 16.6699, Mean Abs error: 0.543419 \n","\n","Epoch 887   -------------------------------\n","Train loss: 0.05724841728806496\n","Test set => Mean Squared error: 16.6642, Mean Abs error: 0.543331 \n","\n","Epoch 888   -------------------------------\n","Train loss: 0.05685020983219147\n","Test set => Mean Squared error: 16.6657, Mean Abs error: 0.543336 \n","\n","Epoch 889   -------------------------------\n","Train loss: 0.05667940899729729\n","Test set => Mean Squared error: 16.6576, Mean Abs error: 0.543186 \n","\n","Epoch 890   -------------------------------\n","Train loss: 0.056835971772670746\n","Test set => Mean Squared error: 16.6630, Mean Abs error: 0.543299 \n","\n","Epoch 891   -------------------------------\n","Train loss: 0.05707444250583649\n","Test set => Mean Squared error: 16.6587, Mean Abs error: 0.543215 \n","\n","Epoch 892   -------------------------------\n","Train loss: 0.057173583656549454\n","Test set => Mean Squared error: 16.6673, Mean Abs error: 0.543370 \n","\n","Epoch 893   -------------------------------\n","Train loss: 0.05705147981643677\n","Test set => Mean Squared error: 16.6644, Mean Abs error: 0.543299 \n","\n","Epoch 894   -------------------------------\n","Train loss: 0.05682273209095001\n","Test set => Mean Squared error: 16.6748, Mean Abs error: 0.543472 \n","\n","Epoch 895   -------------------------------\n","Train loss: 0.056625980883836746\n","Test set => Mean Squared error: 16.6765, Mean Abs error: 0.543495 \n","\n","Epoch 896   -------------------------------\n","Train loss: 0.05659111589193344\n","Test set => Mean Squared error: 16.6902, Mean Abs error: 0.543717 \n","\n","Epoch 897   -------------------------------\n","Train loss: 0.05665235593914986\n","Test set => Mean Squared error: 16.7041, Mean Abs error: 0.543963 \n","\n","Epoch 898   -------------------------------\n","Train loss: 0.05682891234755516\n","Test set => Mean Squared error: 16.6965, Mean Abs error: 0.543809 \n","\n","Epoch 899   -------------------------------\n","Train loss: 0.05703158304095268\n","Test set => Mean Squared error: 16.7131, Mean Abs error: 0.544110 \n","\n","Epoch 900   -------------------------------\n","Train loss: 0.057220280170440674\n","Test set => Mean Squared error: 16.7041, Mean Abs error: 0.543928 \n","\n","Epoch 901   -------------------------------\n","Train loss: 0.05706365406513214\n","Test set => Mean Squared error: 16.7088, Mean Abs error: 0.544033 \n","\n","Epoch 902   -------------------------------\n","Train loss: 0.05686702951788902\n","Test set => Mean Squared error: 16.7184, Mean Abs error: 0.544184 \n","\n","Epoch 903   -------------------------------\n","Train loss: 0.05662893131375313\n","Test set => Mean Squared error: 16.7238, Mean Abs error: 0.544269 \n","\n","Epoch 904   -------------------------------\n","Train loss: 0.05658474564552307\n","Test set => Mean Squared error: 16.7197, Mean Abs error: 0.544202 \n","\n","Epoch 905   -------------------------------\n","Train loss: 0.05677841231226921\n","Test set => Mean Squared error: 16.7183, Mean Abs error: 0.544183 \n","\n","Epoch 906   -------------------------------\n","Train loss: 0.05705978348851204\n","Test set => Mean Squared error: 16.7313, Mean Abs error: 0.544419 \n","\n","Epoch 907   -------------------------------\n","Train loss: 0.05742829293012619\n","Test set => Mean Squared error: 16.7037, Mean Abs error: 0.543899 \n","\n","Epoch 908   -------------------------------\n","Train loss: 0.058118097484111786\n","Test set => Mean Squared error: 16.6894, Mean Abs error: 0.543729 \n","\n","Epoch 909   -------------------------------\n","Train loss: 0.05775174871087074\n","Test set => Mean Squared error: 16.6789, Mean Abs error: 0.543541 \n","\n","Epoch 910   -------------------------------\n","Train loss: 0.05699567869305611\n","Test set => Mean Squared error: 16.6623, Mean Abs error: 0.543235 \n","\n","Epoch 911   -------------------------------\n","Train loss: 0.05721157044172287\n","Test set => Mean Squared error: 16.6681, Mean Abs error: 0.543336 \n","\n","Epoch 912   -------------------------------\n","Train loss: 0.057190991938114166\n","Test set => Mean Squared error: 16.6798, Mean Abs error: 0.543528 \n","\n","Epoch 913   -------------------------------\n","Train loss: 0.0571206659078598\n","Test set => Mean Squared error: 16.6601, Mean Abs error: 0.543201 \n","\n","Epoch 914   -------------------------------\n","Train loss: 0.05684639886021614\n","Test set => Mean Squared error: 16.6423, Mean Abs error: 0.542915 \n","\n","Epoch 915   -------------------------------\n","Train loss: 0.05689932778477669\n","Test set => Mean Squared error: 16.6465, Mean Abs error: 0.542993 \n","\n","Epoch 916   -------------------------------\n","Train loss: 0.05686458200216293\n","Test set => Mean Squared error: 16.6354, Mean Abs error: 0.542793 \n","\n","Epoch 917   -------------------------------\n","Train loss: 0.05667616426944733\n","Test set => Mean Squared error: 16.6292, Mean Abs error: 0.542683 \n","\n","Epoch 918   -------------------------------\n","Train loss: 0.05689074471592903\n","Test set => Mean Squared error: 16.6595, Mean Abs error: 0.543173 \n","\n","Epoch 919   -------------------------------\n","Train loss: 0.05665004253387451\n","Test set => Mean Squared error: 16.6791, Mean Abs error: 0.543486 \n","\n","Epoch 920   -------------------------------\n","Train loss: 0.056671515107154846\n","Test set => Mean Squared error: 16.6777, Mean Abs error: 0.543460 \n","\n","Epoch 921   -------------------------------\n","Train loss: 0.056756362318992615\n","Test set => Mean Squared error: 16.6829, Mean Abs error: 0.543543 \n","\n","Epoch 922   -------------------------------\n","Train loss: 0.05657567083835602\n","Test set => Mean Squared error: 16.6888, Mean Abs error: 0.543633 \n","\n","Epoch 923   -------------------------------\n","Train loss: 0.056601837277412415\n","Test set => Mean Squared error: 16.6902, Mean Abs error: 0.543643 \n","\n","Epoch 924   -------------------------------\n","Train loss: 0.056762367486953735\n","Test set => Mean Squared error: 16.6807, Mean Abs error: 0.543491 \n","\n","Epoch 925   -------------------------------\n","Train loss: 0.056777358055114746\n","Test set => Mean Squared error: 16.6937, Mean Abs error: 0.543724 \n","\n","Epoch 926   -------------------------------\n","Train loss: 0.05693306028842926\n","Test set => Mean Squared error: 16.6825, Mean Abs error: 0.543500 \n","\n","Epoch 927   -------------------------------\n","Train loss: 0.057302575558423996\n","Test set => Mean Squared error: 16.6971, Mean Abs error: 0.543784 \n","\n","Epoch 928   -------------------------------\n","Train loss: 0.05795447155833244\n","Test set => Mean Squared error: 16.6854, Mean Abs error: 0.543548 \n","\n","Epoch 929   -------------------------------\n","Train loss: 0.0582895502448082\n","Test set => Mean Squared error: 16.7160, Mean Abs error: 0.544111 \n","\n","Epoch 930   -------------------------------\n","Train loss: 0.05826300010085106\n","Test set => Mean Squared error: 16.7004, Mean Abs error: 0.543819 \n","\n","Epoch 931   -------------------------------\n","Train loss: 0.05710647627711296\n","Test set => Mean Squared error: 16.7041, Mean Abs error: 0.543898 \n","\n","Epoch 932   -------------------------------\n","Train loss: 0.05659247934818268\n","Test set => Mean Squared error: 16.7164, Mean Abs error: 0.544118 \n","\n","Epoch 933   -------------------------------\n","Train loss: 0.056926220655441284\n","Test set => Mean Squared error: 16.6996, Mean Abs error: 0.543817 \n","\n","Epoch 934   -------------------------------\n","Train loss: 0.05729798600077629\n","Test set => Mean Squared error: 16.6929, Mean Abs error: 0.543740 \n","\n","Epoch 935   -------------------------------\n","Train loss: 0.057453036308288574\n","Test set => Mean Squared error: 16.6962, Mean Abs error: 0.543797 \n","\n","Epoch 936   -------------------------------\n","Train loss: 0.05743397772312164\n","Test set => Mean Squared error: 16.7067, Mean Abs error: 0.543953 \n","\n","Epoch 937   -------------------------------\n","Train loss: 0.056854721158742905\n","Test set => Mean Squared error: 16.7163, Mean Abs error: 0.544112 \n","\n","Epoch 938   -------------------------------\n","Train loss: 0.05648154765367508\n","Test set => Mean Squared error: 16.7356, Mean Abs error: 0.544423 \n","\n","Epoch 939   -------------------------------\n","Train loss: 0.056564003229141235\n","Test set => Mean Squared error: 16.7264, Mean Abs error: 0.544255 \n","\n","Epoch 940   -------------------------------\n","Train loss: 0.05696684494614601\n","Test set => Mean Squared error: 16.7164, Mean Abs error: 0.544131 \n","\n","Epoch 941   -------------------------------\n","Train loss: 0.05689593404531479\n","Test set => Mean Squared error: 16.7183, Mean Abs error: 0.544167 \n","\n","Epoch 942   -------------------------------\n","Train loss: 0.05675510689616203\n","Test set => Mean Squared error: 16.6990, Mean Abs error: 0.543817 \n","\n","Epoch 943   -------------------------------\n","Train loss: 0.05659041926264763\n","Test set => Mean Squared error: 16.6995, Mean Abs error: 0.543818 \n","\n","Epoch 944   -------------------------------\n","Train loss: 0.05653626099228859\n","Test set => Mean Squared error: 16.7001, Mean Abs error: 0.543840 \n","\n","Epoch 945   -------------------------------\n","Train loss: 0.05660483241081238\n","Test set => Mean Squared error: 16.6933, Mean Abs error: 0.543727 \n","\n","Epoch 946   -------------------------------\n","Train loss: 0.056498028337955475\n","Test set => Mean Squared error: 16.7106, Mean Abs error: 0.544025 \n","\n","Epoch 947   -------------------------------\n","Train loss: 0.05664120614528656\n","Test set => Mean Squared error: 16.7101, Mean Abs error: 0.544004 \n","\n","Epoch 948   -------------------------------\n","Train loss: 0.056401029229164124\n","Test set => Mean Squared error: 16.7160, Mean Abs error: 0.544099 \n","\n","Epoch 949   -------------------------------\n","Train loss: 0.0563865527510643\n","Test set => Mean Squared error: 16.7212, Mean Abs error: 0.544182 \n","\n","Epoch 950   -------------------------------\n","Train loss: 0.05641655623912811\n","Test set => Mean Squared error: 16.7224, Mean Abs error: 0.544190 \n","\n","Epoch 951   -------------------------------\n","Train loss: 0.056516584008932114\n","Test set => Mean Squared error: 16.7309, Mean Abs error: 0.544350 \n","\n","Epoch 952   -------------------------------\n","Train loss: 0.056517161428928375\n","Test set => Mean Squared error: 16.7237, Mean Abs error: 0.544209 \n","\n","Epoch 953   -------------------------------\n","Train loss: 0.056558240205049515\n","Test set => Mean Squared error: 16.7302, Mean Abs error: 0.544330 \n","\n","Epoch 954   -------------------------------\n","Train loss: 0.05656949803233147\n","Test set => Mean Squared error: 16.7314, Mean Abs error: 0.544321 \n","\n","Epoch 955   -------------------------------\n","Train loss: 0.05647837743163109\n","Test set => Mean Squared error: 16.7465, Mean Abs error: 0.544586 \n","\n","Epoch 956   -------------------------------\n","Train loss: 0.05646008625626564\n","Test set => Mean Squared error: 16.7432, Mean Abs error: 0.544511 \n","\n","Epoch 957   -------------------------------\n","Train loss: 0.05644284561276436\n","Test set => Mean Squared error: 16.7504, Mean Abs error: 0.544645 \n","\n","Epoch 958   -------------------------------\n","Train loss: 0.05641143396496773\n","Test set => Mean Squared error: 16.7454, Mean Abs error: 0.544542 \n","\n","Epoch 959   -------------------------------\n","Train loss: 0.05639044940471649\n","Test set => Mean Squared error: 16.7556, Mean Abs error: 0.544725 \n","\n","Epoch 960   -------------------------------\n","Train loss: 0.05636673793196678\n","Test set => Mean Squared error: 16.7612, Mean Abs error: 0.544800 \n","\n","Epoch 961   -------------------------------\n","Train loss: 0.056383345276117325\n","Test set => Mean Squared error: 16.7616, Mean Abs error: 0.544823 \n","\n","Epoch 962   -------------------------------\n","Train loss: 0.056456420570611954\n","Test set => Mean Squared error: 16.7627, Mean Abs error: 0.544822 \n","\n","Epoch 963   -------------------------------\n","Train loss: 0.05655359476804733\n","Test set => Mean Squared error: 16.7650, Mean Abs error: 0.544886 \n","\n","Epoch 964   -------------------------------\n","Train loss: 0.056699398905038834\n","Test set => Mean Squared error: 16.7575, Mean Abs error: 0.544734 \n","\n","Epoch 965   -------------------------------\n","Train loss: 0.056971192359924316\n","Test set => Mean Squared error: 16.7571, Mean Abs error: 0.544766 \n","\n","Epoch 966   -------------------------------\n","Train loss: 0.05736386775970459\n","Test set => Mean Squared error: 16.7547, Mean Abs error: 0.544700 \n","\n","Epoch 967   -------------------------------\n","Train loss: 0.0572633258998394\n","Test set => Mean Squared error: 16.7501, Mean Abs error: 0.544651 \n","\n","Epoch 968   -------------------------------\n","Train loss: 0.05699494853615761\n","Test set => Mean Squared error: 16.7345, Mean Abs error: 0.544378 \n","\n","Epoch 969   -------------------------------\n","Train loss: 0.056458473205566406\n","Test set => Mean Squared error: 16.7312, Mean Abs error: 0.544332 \n","\n","Epoch 970   -------------------------------\n","Train loss: 0.056410375982522964\n","Test set => Mean Squared error: 16.7269, Mean Abs error: 0.544279 \n","\n","Epoch 971   -------------------------------\n","Train loss: 0.05641700699925423\n","Test set => Mean Squared error: 16.7132, Mean Abs error: 0.544034 \n","\n","Epoch 972   -------------------------------\n","Train loss: 0.05675968900322914\n","Test set => Mean Squared error: 16.7196, Mean Abs error: 0.544158 \n","\n","Epoch 973   -------------------------------\n","Train loss: 0.056595638394355774\n","Test set => Mean Squared error: 16.7123, Mean Abs error: 0.544009 \n","\n","Epoch 974   -------------------------------\n","Train loss: 0.05645269900560379\n","Test set => Mean Squared error: 16.7103, Mean Abs error: 0.543995 \n","\n","Epoch 975   -------------------------------\n","Train loss: 0.05633039027452469\n","Test set => Mean Squared error: 16.7075, Mean Abs error: 0.543947 \n","\n","Epoch 976   -------------------------------\n","Train loss: 0.05623216927051544\n","Test set => Mean Squared error: 16.7021, Mean Abs error: 0.543839 \n","\n","Epoch 977   -------------------------------\n","Train loss: 0.05631285905838013\n","Test set => Mean Squared error: 16.7090, Mean Abs error: 0.543957 \n","\n","Epoch 978   -------------------------------\n","Train loss: 0.056305721402168274\n","Test set => Mean Squared error: 16.7054, Mean Abs error: 0.543890 \n","\n","Epoch 979   -------------------------------\n","Train loss: 0.0562346987426281\n","Test set => Mean Squared error: 16.7015, Mean Abs error: 0.543830 \n","\n","Epoch 980   -------------------------------\n","Train loss: 0.05618130415678024\n","Test set => Mean Squared error: 16.7060, Mean Abs error: 0.543908 \n","\n","Epoch 981   -------------------------------\n","Train loss: 0.05618336796760559\n","Test set => Mean Squared error: 16.7063, Mean Abs error: 0.543907 \n","\n","Epoch 982   -------------------------------\n","Train loss: 0.056188859045505524\n","Test set => Mean Squared error: 16.7082, Mean Abs error: 0.543952 \n","\n","Epoch 983   -------------------------------\n","Train loss: 0.056267984211444855\n","Test set => Mean Squared error: 16.7049, Mean Abs error: 0.543876 \n","\n","Epoch 984   -------------------------------\n","Train loss: 0.056274570524692535\n","Test set => Mean Squared error: 16.7078, Mean Abs error: 0.543938 \n","\n","Epoch 985   -------------------------------\n","Train loss: 0.056253962218761444\n","Test set => Mean Squared error: 16.7101, Mean Abs error: 0.543962 \n","\n","Epoch 986   -------------------------------\n","Train loss: 0.05616185441613197\n","Test set => Mean Squared error: 16.7189, Mean Abs error: 0.544120 \n","\n","Epoch 987   -------------------------------\n","Train loss: 0.05611354112625122\n","Test set => Mean Squared error: 16.7198, Mean Abs error: 0.544128 \n","\n","Epoch 988   -------------------------------\n","Train loss: 0.05608638375997543\n","Test set => Mean Squared error: 16.7206, Mean Abs error: 0.544129 \n","\n","Epoch 989   -------------------------------\n","Train loss: 0.05611071735620499\n","Test set => Mean Squared error: 16.7247, Mean Abs error: 0.544204 \n","\n","Epoch 990   -------------------------------\n","Train loss: 0.05613972991704941\n","Test set => Mean Squared error: 16.7232, Mean Abs error: 0.544166 \n","\n","Epoch 991   -------------------------------\n","Train loss: 0.05616132169961929\n","Test set => Mean Squared error: 16.7308, Mean Abs error: 0.544302 \n","\n","Epoch 992   -------------------------------\n","Train loss: 0.05616467818617821\n","Test set => Mean Squared error: 16.7295, Mean Abs error: 0.544264 \n","\n","Epoch 993   -------------------------------\n","Train loss: 0.05612105131149292\n","Test set => Mean Squared error: 16.7370, Mean Abs error: 0.544393 \n","\n","Epoch 994   -------------------------------\n","Train loss: 0.056050822138786316\n","Test set => Mean Squared error: 16.7399, Mean Abs error: 0.544436 \n","\n","Epoch 995   -------------------------------\n","Train loss: 0.05604451522231102\n","Test set => Mean Squared error: 16.7389, Mean Abs error: 0.544411 \n","\n","Epoch 996   -------------------------------\n","Train loss: 0.05604719743132591\n","Test set => Mean Squared error: 16.7410, Mean Abs error: 0.544452 \n","\n","Epoch 997   -------------------------------\n","Train loss: 0.05608648806810379\n","Test set => Mean Squared error: 16.7342, Mean Abs error: 0.544327 \n","\n","Epoch 998   -------------------------------\n","Train loss: 0.056094441562891006\n","Test set => Mean Squared error: 16.7406, Mean Abs error: 0.544444 \n","\n","Epoch 999   -------------------------------\n","Train loss: 0.05609525367617607\n","Test set => Mean Squared error: 16.7403, Mean Abs error: 0.544425 \n","\n","Epoch 1000   -------------------------------\n","Train loss: 0.056072115898132324\n","Test set => Mean Squared error: 16.7454, Mean Abs error: 0.544518 \n","\n"]}],"source":["fit(1000, model, criterion, optimizer, train_dl, test_dl)"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n","    for X, y in train_dl:\n","        prediction = model(X)\n","        labels = y"]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 4 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","# Create a 2D array\n","x = np.linspace(0,1,lenX)\n","y = np.linspace(0,1,lenY)\n","X, Y = np.meshgrid(x, y)     # Create a grid\n","Z1 = labels.view(lenX,lenY).detach()\n","Z2 = prediction.view(lenX,lenY).detach()\n","\n","# Create a 3D plot\n","fig = plt.figure()\n","ax = fig.add_subplot(121)\n","ax2 = fig.add_subplot(122)\n","\n","# Plot the surface\n","surface1 = ax.imshow(Z1)\n","surface2 = ax2.imshow(Z2)\n","\n","# Add a color bar for reference\n","fig.colorbar(surface1)\n","fig.colorbar(surface2)\n","\n","# Label axes\n","ax.set_xlabel('X-axis')\n","ax.set_ylabel('Y-axis')\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":29414,"sourceId":37484,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
