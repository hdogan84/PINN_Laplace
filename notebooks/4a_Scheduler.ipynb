{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.0036\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Loss: 0.9966\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Loss: 0.9957\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Loss: 0.9905\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Loss: 0.9892\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Loss: 0.9890\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Loss: 0.9828\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Loss: 0.9790\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Loss: 0.9751\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Loss: 0.9707\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Loss: 0.9642\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Loss: 0.9605\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Loss: 0.9531\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Loss: 0.9487\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Loss: 0.9381\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Loss: 0.9302\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Loss: 0.9282\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Loss: 0.9207\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Loss: 0.9120\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Loss: 0.9056\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Loss: 0.9027\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Loss: 0.8968\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Loss: 0.8910\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Loss: 0.8871\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 25/100, Loss: 0.8806\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 26/100, Loss: 0.8698\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 27/100, Loss: 0.8668\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 28/100, Loss: 0.8622\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 29/100, Loss: 0.8578\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 30/100, Loss: 0.8559\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 31/100, Loss: 0.8469\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 32/100, Loss: 0.8418\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 33/100, Loss: 0.8390\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 34/100, Loss: 0.8347\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 35/100, Loss: 0.8292\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 36/100, Loss: 0.8272\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 37/100, Loss: 0.8236\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 38/100, Loss: 0.8181\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 39/100, Loss: 0.8163\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 40/100, Loss: 0.8072\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 41/100, Loss: 0.8059\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 42/100, Loss: 0.8077\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 43/100, Loss: 0.8003\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 44/100, Loss: 0.7963\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 45/100, Loss: 0.7962\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 46/100, Loss: 0.7928\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 47/100, Loss: 0.7868\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 48/100, Loss: 0.7855\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 49/100, Loss: 0.7801\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 50/100, Loss: 0.7761\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 51/100, Loss: 0.7746\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 52/100, Loss: 0.7757\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 53/100, Loss: 0.7762\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 54/100, Loss: 0.7653\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 55/100, Loss: 0.7657\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 56/100, Loss: 0.7625\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 57/100, Loss: 0.7596\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 58/100, Loss: 0.7577\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 59/100, Loss: 0.7579\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 60/100, Loss: 0.7515\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 61/100, Loss: 0.7517\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 62/100, Loss: 0.7425\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 63/100, Loss: 0.7396\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 64/100, Loss: 0.7412\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 65/100, Loss: 0.7369\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 66/100, Loss: 0.7374\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 67/100, Loss: 0.7365\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 68/100, Loss: 0.7312\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 69/100, Loss: 0.7289\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 70/100, Loss: 0.7238\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 71/100, Loss: 0.7212\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 72/100, Loss: 0.7195\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 73/100, Loss: 0.7158\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 74/100, Loss: 0.7237\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 75/100, Loss: 0.7156\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 76/100, Loss: 0.7143\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 77/100, Loss: 0.7174\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 78/100, Loss: 0.7142\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 79/100, Loss: 0.7048\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 80/100, Loss: 0.7027\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 81/100, Loss: 0.7116\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 82/100, Loss: 0.7031\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 83/100, Loss: 0.7007\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 84/100, Loss: 0.6976\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 85/100, Loss: 0.6969\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 86/100, Loss: 0.6933\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 87/100, Loss: 0.6935\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 88/100, Loss: 0.6897\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 89/100, Loss: 0.6873\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 90/100, Loss: 0.6885\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 91/100, Loss: 0.6851\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 92/100, Loss: 0.6840\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 93/100, Loss: 0.6812\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 94/100, Loss: 0.6792\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 95/100, Loss: 0.6799\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 96/100, Loss: 0.6742\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 97/100, Loss: 0.6768\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 98/100, Loss: 0.6730\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 99/100, Loss: 0.6751\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 100/100, Loss: 0.6692\n",
      "Current Learning Rate: 0.001000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example dataset\n",
    "X = torch.randn(7000, 12)  # Input features\n",
    "y = torch.randn(7000, 1)   # Targets\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define your DNN model\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = DNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Scheduler to reduce learning rate on plateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Step the scheduler with the validation loss (or MSE score)\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    # Check the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6667\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Loss: 0.6616\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Loss: 0.6632\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Loss: 0.6614\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Loss: 0.6584\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Loss: 0.6638\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Loss: 0.6563\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Loss: 0.6573\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Loss: 0.6604\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Loss: 0.6489\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Loss: 0.6523\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Loss: 0.6496\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Loss: 0.6471\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Loss: 0.6497\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Loss: 0.6460\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Loss: 0.6437\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Loss: 0.6480\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Loss: 0.6424\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Loss: 0.6431\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Loss: 0.6420\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Loss: 0.6392\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Loss: 0.6372\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Loss: 0.6310\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Loss: 0.6353\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 25/100, Loss: 0.6340\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 26/100, Loss: 0.6354\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 27/100, Loss: 0.6322\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 28/100, Loss: 0.6271\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 29/100, Loss: 0.6320\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 30/100, Loss: 0.6301\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 31/100, Loss: 0.6223\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 32/100, Loss: 0.6224\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 33/100, Loss: 0.6216\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 34/100, Loss: 0.6235\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 35/100, Loss: 0.6248\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 36/100, Loss: 0.6175\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 37/100, Loss: 0.6149\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 38/100, Loss: 0.6184\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 39/100, Loss: 0.6151\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 40/100, Loss: 0.6154\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 41/100, Loss: 0.6114\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 42/100, Loss: 0.6190\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 43/100, Loss: 0.6120\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 44/100, Loss: 0.6185\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 45/100, Loss: 0.6149\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 46/100, Loss: 0.6103\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 47/100, Loss: 0.6075\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 48/100, Loss: 0.6056\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 49/100, Loss: 0.6023\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 50/100, Loss: 0.6001\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 51/100, Loss: 0.6032\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 52/100, Loss: 0.6018\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 53/100, Loss: 0.6016\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 54/100, Loss: 0.6008\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 55/100, Loss: 0.6026\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 56/100, Loss: 0.6018\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 57/100, Loss: 0.5991\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 58/100, Loss: 0.5958\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 59/100, Loss: 0.5972\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 60/100, Loss: 0.5948\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 61/100, Loss: 0.5894\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 62/100, Loss: 0.5947\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 63/100, Loss: 0.5892\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 64/100, Loss: 0.5906\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 65/100, Loss: 0.5918\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 66/100, Loss: 0.5910\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 67/100, Loss: 0.5868\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 68/100, Loss: 0.5905\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 69/100, Loss: 0.5891\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 70/100, Loss: 0.5884\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 71/100, Loss: 0.5874\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 72/100, Loss: 0.5844\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 73/100, Loss: 0.5868\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 74/100, Loss: 0.5837\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 75/100, Loss: 0.5806\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 76/100, Loss: 0.5818\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 77/100, Loss: 0.5789\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 78/100, Loss: 0.5762\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 79/100, Loss: 0.5819\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 80/100, Loss: 0.5776\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 81/100, Loss: 0.5778\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 82/100, Loss: 0.5787\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 83/100, Loss: 0.5797\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 84/100, Loss: 0.5796\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 85/100, Loss: 0.5721\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 86/100, Loss: 0.5715\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 87/100, Loss: 0.5787\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 88/100, Loss: 0.5730\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 89/100, Loss: 0.5750\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 90/100, Loss: 0.5725\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 91/100, Loss: 0.5756\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 92/100, Loss: 0.5692\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 93/100, Loss: 0.5719\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 94/100, Loss: 0.5725\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 95/100, Loss: 0.5656\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 96/100, Loss: 0.5640\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 97/100, Loss: 0.5652\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 98/100, Loss: 0.5644\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 99/100, Loss: 0.5712\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 100/100, Loss: 0.5644\n",
      "Current Learning Rate: 0.001000\n"
     ]
    }
   ],
   "source": [
    "class ReduceLROnPlateauCallback:\n",
    "    def __init__(self, optimizer, factor=0.5, patience=20, min_lr=1e-6, verbose=True):\n",
    "        self.optimizer = optimizer\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait = 0\n",
    "\n",
    "    def step(self, current_loss):\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self._reduce_lr()\n",
    "                self.wait = 0\n",
    "\n",
    "    def _reduce_lr(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            new_lr = max(param_group['lr'] * self.factor, self.min_lr)\n",
    "            if param_group['lr'] > new_lr:\n",
    "                if self.verbose:\n",
    "                    print(f\"Reducing learning rate from {param_group['lr']:.6f} to {new_lr:.6f}\")\n",
    "                param_group['lr'] = new_lr\n",
    "\n",
    "# Example usage in a training loop\n",
    "callback = ReduceLROnPlateauCallback(optimizer, factor=0.5, patience=20, verbose=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Call the callback step\n",
    "    callback.step(avg_loss)\n",
    "\n",
    "    # Check the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.5628\n",
      "Validation Loss: 0.5592\n",
      "New best model saved with min score: 0.5592\n",
      "Epoch 2/100, Loss: 0.5666\n",
      "Validation Loss: 0.5704\n",
      "Epoch 3/100, Loss: 0.5600\n",
      "Validation Loss: 0.5620\n",
      "Epoch 4/100, Loss: 0.5589\n",
      "Validation Loss: 0.5591\n",
      "New best model saved with min score: 0.5591\n",
      "Epoch 5/100, Loss: 0.5629\n",
      "Validation Loss: 0.5502\n",
      "New best model saved with min score: 0.5502\n",
      "Epoch 6/100, Loss: 0.5622\n",
      "Validation Loss: 0.5624\n",
      "Epoch 7/100, Loss: 0.5632\n",
      "Validation Loss: 0.5622\n",
      "Epoch 8/100, Loss: 0.5632\n",
      "Validation Loss: 0.5751\n",
      "Epoch 9/100, Loss: 0.5612\n",
      "Validation Loss: 0.5731\n",
      "Epoch 10/100, Loss: 0.5618\n",
      "Validation Loss: 0.5607\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class SaveBestModelCallback:\n",
    "    def __init__(self, save_path=\"best_model.pth\", mode=\"min\", verbose=True):\n",
    "        \"\"\"\n",
    "        Save the best model based on a monitored metric.\n",
    "\n",
    "        Args:\n",
    "        - save_path (str): File path to save the best model.\n",
    "        - mode (str): 'min' to minimize the monitored metric or 'max' to maximize it.\n",
    "        - verbose (bool): Print messages when a new best model is saved.\n",
    "        \"\"\"\n",
    "        self.save_path = save_path\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.best_score = float('inf') if mode == \"min\" else -float('inf')\n",
    "\n",
    "    def step(self, model, current_score):\n",
    "        \"\"\"\n",
    "        Check if the current score is the best and save the model if it is.\n",
    "\n",
    "        Args:\n",
    "        - model: The PyTorch model to save.\n",
    "        - current_score (float): The monitored metric score.\n",
    "        \"\"\"\n",
    "        is_better = (\n",
    "            current_score < self.best_score if self.mode == \"min\" else current_score > self.best_score\n",
    "        )\n",
    "        if is_better:\n",
    "            self.best_score = current_score\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "            if self.verbose:\n",
    "                print(f\"New best model saved with {self.mode} score: {current_score:.4f}\")\n",
    "\n",
    "#Example usage in a training loop\n",
    "save_best_model = SaveBestModelCallback(save_path=\"best_model.pth\", mode=\"min\", verbose=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Simulate validation loss (replace with actual validation loop if available)\n",
    "    val_loss = avg_train_loss + torch.randn(1).item() * 0.01  # Example metric\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    save_best_model.step(model, val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cpu vs gpu profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "def training_step():\n",
    "    # Dummy training loop\n",
    "    x = torch.randn(1000, 1000, device=\"cuda\")\n",
    "    y = torch.matmul(x, x)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./log\"),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    training_step()\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
